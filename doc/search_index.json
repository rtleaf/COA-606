[["index.html", "1 Biometry for the Coastal Sciences (COA 606) 1.1 Course Description and Objectives 1.2 At the conclusion of this course: 1.3 Course Materials - Online and in the Caylor Library 1.4 Course Scheduling 1.5 Course Workload Statement 1.6 Course Evaluation 1.7 Course Evaluation - Participation and Attendence 1.8 Homework policy 1.9 Grading scale 1.10 Exam Bank 1.11 Academic Support Resources 1.12 Mental Well-Being Statement 1.13 Nondiscrimination Statement 1.14 Confidentiality and Mandatory Reporting 1.15 Academic Integrity", " 1 Biometry for the Coastal Sciences (COA 606) Date: 20 September 2023 Instructor: Dr. Robert Leaf Office: GCRL Oceanography 119 Office Hours: I have an open door policy and welcome your questions and concerns. Stop in to see me anytime or make an appointment and lets meet to discuss your needs. Unfortunately, sometimes I have deadlines and travel and will not be able to accommodate “drop in” students every time but will make every reasonable effort to accomodate your schedule. Email: robert.leaf@usm.edu Course Meeting Day and Time: MW, 4:00 to 5:15 PM Repository of Readings: https://tinyurl.com/mrx7hdxe Repository of Videos and Transcripts: https://tinyurl.com/bde2ejnc Repository of HW: https://tinyurl.com/3nvwe38h In-class assignment: https://rtleaf.github.io/coa-606-in-class 1.1 Course Description and Objectives This course addresses basic approaches to experimental design, statistical analysis, and presentation of quantitative information. 1.2 At the conclusion of this course: By the end of this course, you will be able to: Define what comprises the field of statistical analysis. What is meant by random samples, random sampling, and understanding how these aspects of sampling are critical for description and inference. Explain the differences and similarities among variables, statistics, and parameters. Define a cumulative density function and a probability density function and describe several common distributional families (e.g., normal, binomial, chi-square). Understand and be familiar with the applications of: Frequency distributions Power analysis Summary statistics as a tool for describing data Means comparison techniques for testing hypotheses ANOVA comparison techniques for testing hypothesis Linear regression techniques for describing relationships between random variables. Make appropriate decisions as a part of a statistical data analysis. 1.3 Course Materials - Online and in the Caylor Library Experimental Design and Data Analysis for Biologists by G. Quinn and M. Keough (2002) Biostatistical analysis 4th edition, by Jerrold Zar, Prentice Hall (1999), ISBN-10:0131008463, ISBN-13:978-0131008465 1.4 Course Scheduling Day Activity Reading Monday, August 21, 2023 Introduction Syllabus, QK 1.1, 1.2 Wednesday, August 23, 2023 Univariate Data Zar Ch 1, Manikandan 2011, QK 1.3, 1.5 Monday, August 28, 2023 Best Practices and 4, 5, and 6 R Intro Broman and Wu 2018 Wednesday, August 30, 2023 In class assignment Monday, September 4, 2023 Labor Day Holiday Wednesday, September 6, 2023 Sampling Zar Ch 2, Banerjee and Chaudhury 2010, QK 2.1, QK 7.1, 7.2 Monday, September 11, 2023 Measures of Central Tendency Zar Ch 3 Wednesday, September 13, 2023 Measures of Dispersion Zar Ch 4 Monday, September 18, 2023 In class assignment Wednesday, September 20, 2023 The Normal Distribution Zar Ch 6, QK 2.2 Monday, September 25, 2023 In class assignment Wednesday, September 27, 2023 Exam 01 Monday, October 2, 2023 Confidence intervals and 13.NHST Zar Ch 7, QK 2.3, QK 3.1 Wednesday, October 4, 2023 One- and two-tailed Tests and 15. Statistical Power Zar Ch 8, Benjamin et al., Pernet 2015, Steidl et al. 1997, Trafimow, QK 3.2, 3.4 Monday, October 9, 2023 In class assignment Wednesday, October 11, 2023 Bivariate Relationships Zar Ch 19, Aggarwal and Ranganathan 2016, QK 5.1 Monday, October 16, 2023 In class assignment Wednesday, October 18, 2023 Exam 02 Monday, October 23, 2023 Fall Break Wednesday, October 25, 2023 Linear Regression Zar Ch 17, 18, Smith 2018, QK 5.2, 5.3 Monday, October 30, 2023 Multiple Linear Regression Zar Ch 20, QK 6.1 Wednesday, November 1, 2023 In class assignment Monday, November 6, 2023 Comparing Two Means using t-tests Wednesday, November 8, 2023 ANOVA Zar Ch 10. 11, Morley 1983, QK 8.1, 8.3 Monday, November 13, 2023 In class assignment Wednesday, November 15, 2023 Exam 03 Monday, November 20, 2023 Two-Way and Multiway Independent ANOVA Zar Ch 12, 13, 14, QK 4.3, 9.2 Wednesday, November 22, 2023 In class assignment Monday, November 27, 2023 Thanksgiving Break Wednesday, November 29, 2023 Goodness of Fit and Contingency Tables Zar 22, 23 Monday, December 4, 2023 In class assignment Monday to Thursday, December 5 to 8, 2023 Final Exam Week 1.5 Course Workload Statement The expectation of the University of Southern Mississippi is that students should spend approximately 2 to 3 hours outside of class each week for every hour in class working on reading, assignments, studying, and other work for the course. Time management is thus critical for student success. All students should assess their personal circumstances and talk with their advisors about the appropriate number of credit hours to take each term. Resources for academic support can be found at https://www.usm.edu/success. 1.6 Course Evaluation Percentage Letter Grade 93-100 A 90-92 A- 86-89 B+ 83-85 B 80-82 B- 76-79 C+ 73-75 C 70-72 C- 66-69 D+ 63-65 D 60-62 D- &lt; 60 F 1.7 Course Evaluation - Participation and Attendence Attendance is not required, nor is participation. If the students have other obligations, they do not need to let me know, and I will never question your attendance. No questions are asked about missed classes, and nothing we do in class, other than exams, are graded. 1.8 Homework policy There is no due date for the HW, you may turn it in anytime. Graded HW will be returned withing 5 working days of its receipt. Prepare your HW as an organized and clear hard copy, make it easy for me to give you full credit. However, no work will be accepted after 4 PM Monday, December 4, 2023. 1.9 Grading scale Evaluation type Number Points per item Total points HW 3 20 60 Mid-term exams 2 25 50 Final exam 1 50 50 1.10 Exam Bank The bank of exams and study material can be found here: https://tinyurl.com/3d8akhts 1.11 Academic Support Resources If a student knows or believes that they have a disability which is covered by the Americans with Disabilities Act (ADA) and makes them eligible to receive classroom accommodations, they should contact the Office for Disability Accommodations (ODA) for information regarding the registration process. Disabilities covered by the ADA may include but are not limited to ADHD, learning disabilities, psychiatric disabilities, physical disabilities, chronic health disorders, temporary illnesses or injuries and pregnancies. Students should contact ODA if they are not certain whether their documented medical condition qualifies for ODA services. Students are only required to disclose their disability to the Office for Disability Accommodations. All information submitted to ODA by the student is held with strict confidentiality. 1.12 Mental Well-Being Statement I recognize that students sometimes experience challenges that make learning difficult. If you find that life stressors such as anxiety, depression, relationship problems, difficulty concentrating, alcohol or drug problems, or other stressful experiences are interfering with your academic or personal success, consider contacting Student Counseling Services on campus at 601-266-4829. More information is also available at https://www.usm.edu/student-counseling- services. All students are eligible for free, confidential individual or group counseling services. In the event of emergency, please call 911 or contact the counselor on call at 601-606-HELP (4357). 1.13 Nondiscrimination Statement The University of Southern Mississippi offers to all persons equal access to educational, programmatic and employment opportunities without regard to age, sex, sexual orientation, disability, pregnancy, gender identity, genetic information, religion, race, color, national origin, and/or veteran status pursuant to applicable state and federal law. 1.14 Confidentiality and Mandatory Reporting As an instructor, one of my responsibilities is to help create and maintain a safe learning environment. I have a mandatory reporting responsibility related to my role as a faculty member. I am required to share information regarding sexual misconduct or information about a crime that may have occurred on the USM campus with certain University officials responsible for the investigation and remediation of sexual misconduct. The information will remain private and will only be shared with those officials necessary to resolve the matter. If you would like to speak in confidence, resources available to students include Confidential Advisors with the Shafer Center for Crisis Intervention, the Counseling Center, and Student Health Services. More information on these resources and University Policies is available at https://www.usm.edu/sexual-misconduct. 1.15 Academic Integrity All students at the University of Southern Mississippi are expected to demonstrate the highest levels of academic integrity. Forms of academic dishonesty include cheating (including copying from others work), plagiarism (representing another persons words or ideas as your own; failure to properly cite the source of your information, argument, or concepts), falsification of documents, disclosure of or use of test material or other assignment content to another student, submission of the same paper or other assignment to more than one class without the explicit approval of all faculty members involved, unauthorized academic collaboration with others, conspiracy to engage in academic misconduct. "],["univariate-data.html", "2 Univariate Data 2.1 Variables and Frequency Distributions 2.2 Categorical Variable Types 2.3 Continuous Variables", " 2 Univariate Data Univariate is a describes a type of data which consists of observations of only a single characteristic. 2.1 Variables and Frequency Distributions Zar 4th ed. (1.1, 1.3, 1.4) The properties and characteristics of an object that can assume two or more different values are called variables. These are the values that comprise our data. Examples: Heights of graduate students Foot size of toddlers at 10 days old Taxomonic diversity (number of species) in a habitat We need to understand the characteristics of variables prior to analysis because different types of measurements will require different methods of analysis. The next step after data collection is to organize the data into a meaningful form so that similarities and contrasts can be seen easily. One of the common methods for organizing univariate data is to construct frequency distributions either in a table or as a figure. The frequency distribution is an organized representation of the number of observations in each category on the scale of measurement. These allow researchers to have a glance at the entire data conveniently. Frequency distributions allow the experimenter to understand if there are observations that are high or low and also whether observations are concentrated in one area or spread out across the entire scale. 2.2 Categorical Variable Types 2.2.1 Binary variables There are only two categories, e.g. dead or alive, present or absent, positive or negative (e.g. for a disease), the value of some quantity of interest is zero or positive, or the value of some quantity of interest exceeds some threshold value. 2.2.2 Nominal variables There are more than two categories, e.g. Whether the subject is an omnivore, vegetarian, vegan, or carnivore. The subject’s taxonomic group (e.g. genera, species, phyla), 2.2.3 Ordinal variable Similar to a nominal variable but the categories are ordered. Whether people got a fail, a pass, a merit or a distinction in their exam. Intensity of infection (e.g. none, mild, moderate, severe) 2.2.4 R Code ## Provide code in R (using base R functionality) to create a data frame with three colums, ## composed of binary variables, nominal variables, and ordinal variables (e.g. “low ## income”,”middle income”,”high income”). One for each column in a data frame. ## ``` ## # Create binary variable ## # A binary variable is created and stored as a vector with values 1 and 0 ## binary_var &lt;- c(1, 0, 0, 1, 1) ## ## # Create nominal variable ## # A nominal variable is created and stored as a factor with three levels &#39;low income&#39;, &#39;middle income&#39;, and &#39;high income&#39; ## nominal_var &lt;- factor(c(&#39;low income&#39;, &#39;middle income&#39;, &#39;middle income&#39;, &#39;high income&#39;, &#39;low income&#39;), levels = c(&#39;low income&#39;, &#39;middle income&#39;, &#39;high income&#39;)) ## ## # Create ordinal variable ## # An ordinal variable is created and stored as an ordered factor with three levels &#39;Low&#39;, &#39;Medium&#39;, and &#39;High&#39; ## ordinal_var &lt;- ordered(c(&#39;Low&#39;, &#39;Medium&#39;, &#39;Low&#39;, &#39;High&#39;, &#39;Medium&#39;), levels = c(&#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;)) ## ## # Create data frame ## # A data frame is created by combining the binary, nominal, and ordinal variables using the &#39;data.frame()&#39; function ## df &lt;- data.frame(binary_var, nominal_var, ordinal_var) ## ## # View the data frame ## # The contents of the data frame are displayed in the console by calling the variable name &#39;df&#39; ## df ## ``` 2.2.5 Frequency Distributions of a Categorical Variable A tally of how frequently occurring a value is among categories. 2.2.6 R Code ## Provide code in R (using base R functionality) to plot ordinal, binary, and nominal ## variables using a barplot ## ```R ## # create example data ## ordinal_var &lt;- c(3, 2, 4, 1, 5) # create a vector of values for an ordinal variable ## binary_var &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) # create a vector of values for a binary variable ## nominal_var &lt;- c(&#39;Red&#39;, &#39;Green&#39;, &#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;) # create a vector of values for a nominal variable ## ## # create barplot for ordinal variable ## barplot(ordinal_var, names.arg = c(&#39;Item 1&#39;, &#39;Item 2&#39;, &#39;Item 3&#39;, &#39;Item 4&#39;, &#39;Item 5&#39;), # create a barplot of the ordinal variable with names for each bar ## main = &#39;Ordinal Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Value&#39;) # add a title and axis labels to the plot ## ## # create barplot for binary variable ## binary_table &lt;- table(binary_var) # create a frequency table of the binary variable ## barplot(binary_table, names.arg = c(&#39;FALSE&#39;, &#39;TRUE&#39;), # create a barplot of the frequency table with names for each bar ## main = &#39;Binary Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Count&#39;)```R ## # create barplot for nominal variable ## nominal_table &lt;- table(nominal_var) # create a frequency table of the nominal variable ## barplot(nominal_table, main = &#39;Nominal Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Count&#39;) # create a barplot of the frequency table and set a title and axis labels ## ``` ## ## Overall, this R code creates three barplots to display different types of variables - ordinal, binary, and nominal. The `barplot` function is used to create each barplot, with customization options such as `names.arg`, `main`, `xlab`, and `ylab` used as needed. The `table` function is used to create frequency tables of the binary and nominal variables to feed into the `barplot` function. 2.3 Continuous Variables Equal intervals on the variable represent equal differences in the property being measured, e.g. the difference between 6 and 8 is equivalent to the difference between 13 and 15, Density or frequency of organisms in a transect or at a sampling station, Body Mass Index or some measure of condition of an organism. 2.3.1 Frequency Distribution of a Continuous Variable A question often asked: What interval to choose? Your knowledge of the domain will guide this. As a first approach, the width of the class can be determined by dividing the range of observations by the number of classes. The following are some guidelines regarding class widths: 1.) It is advisable to have equal class widths. Unequal class widths should be used only when large gaps exist in data. 2.) The class intervals should be mutually exclusive and nonoverlapping. 3.) Open-ended classes at the lower and upper side (e.g., &lt; 10, &gt; 100) should be avoided, why? Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data Phosphorous concentration Frequency 8.1 to 8.2 2 8.2 to 8.3 6 8.3 to 8.4 8 8.4 to 8.5 11 8.5 to 8.6 17 8.6 to 8.7 17 8.7 to 8.8 24 8.8 to 8.9 18 8.9 to 9.0 13 9.0 to 9.1 10 9.1 to 9.2 4 Total frequency = 130 = n 2.3.2 Cumulative Distribution of a Continuous Variable Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data Cumulative Frequency Phosphorous concentration Frequency Starting with Low Values Starting with High Values 8.15 to 8.25 2 2 130 8.25 to 8.35 6 8 128 8.35 to 8.45 8 16 122 8.45 to 8.55 11 27 114 8.55 to 8.65 17 44 103 8.65 to 8.75 17 61 86 8.75 to 8.85 24 85 69 8.85 to 8.95 18 103 45 8.95 to 9.05 13 116 27 9.05 to 9.15 10 126 14 9.15 to 9.25 4 130 4 Total frequency = 130 = n 2.3.3 R Code ## Provide code in R (using base R functionality) to plot a frequency histogram using ## simulated data from a continuous distribution. ## ``` ## # Set seed for reproducibility ## set.seed(123) ## ## # Generate simulated data from a normal distribution ## data &lt;- rnorm(1000, mean = 10, sd = 2) ## ## # Plot frequency histogram ## hist(data, freq = TRUE, breaks = 20, xlim = c(0, 20), ## main = &#39;Frequency Histogram of Simulated Data&#39;, # Add main title to plot ## xlab = &#39;Data Values&#39;, ylab = &#39;Frequency&#39;) # Add axis labels to plot ## ``` ## In this example, we set the seed to ensure the same results every time the code is run. We then simulate 1000 data points from a normal distribution with a mean of 10 and standard deviation of 2. Finally, we use the `hist()` function with the `freq = TRUE` argument to plot a frequency histogram with 20 breaks and set the x-axis limits to 0 and 20. The main title and axis labels are also added for clarity. 2.3.4 Box and Whisker Plot of a Continuous Variable This graph, first described by Tukey, can also be used to illustrate the distribution of data. There is a vertical or horizontal rectangle (box), the ends of which correspond to the upper and lower quartiles (75th and 25th percentile, respectively). Hence the middle 50% of observations are represented by the box. The length of the box indicates the variability of the data. The line inside the box denotes the median (sometimes marked as a plus sign). The position of the median indicates whether the data are skewed or not. If the median is closer to the upper quartile, then they are negatively skewed and if it is near the lower quartile, then positively skewed. 2.3.5 R Code ## Provide code in R (using base R functionality) to create a Box and Whisker Plot of a ## Continuous Variable. ## ```R ## # Create a continuous variable data ## continuous_var &lt;- rnorm(100) # Create a variable &quot;continuous_var&quot; that stores 100 random normal numbers generated by rnorm ## ## # Create a boxplot of the continuous variable ## boxplot(continuous_var, main=&#39;Boxplot of Continuous Variable&#39;, # Create a box and whisker plot of the continuous variable, with a main title of &#39;Boxplot of Continuous Variable&#39; ## ylab=&#39;Value&#39;, col=&#39;steelblue&#39;) # Set the y-axis label to &#39;Value&#39; and the color of the boxes to &#39;steelblue&#39; ## ``` 2.3.6 Violin Plots Violin plots are an alternative to box plots that solves the issues regarding displaying the underlying distribution of the observations, as these plots show a kernel density estimate of the data. same summary statistics as box plots: the white dot represents the median. the thick gray bar in the center represents the interquartile range. the thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the interquartile range. On each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability. 2.3.7 R Code ## Provide code in R (using base R functionality) to create a violin plot in R plotting using ## the mtcars dataset using package vioplot. ## ``` ## # Load the vioplot package and the mtcars dataset ## library(vioplot) ## data(mtcars) ## ## # Create a violin plot of the mpg variable ## vioplot(mtcars$mpg, main = &#39;Violin Plot of MPG in mtcars&#39;, ylab = &#39;MPG&#39;) ## ``` ## The above code first loads the `vioplot` package and the `mtcars` dataset. Then, it creates a violin plot of the `mpg` variable using the `vioplot()` function. The `main` argument adds a title to the plot and the `ylab` argument labels the y-axis as &#39;MPG&#39;. "],["best-practices-for-data-management.html", "3 Best Practices For Data Management 3.1 Consistency and conventions 3.2 RStudio and R", " 3 Best Practices For Data Management Broman and Woo (2018) highlight the challenges and best practices for using spreadsheets. The design principles that they outline are useful for your work in general and specialized statistical programs (i.e. Python, R, and many others). Spreadsheets (Google Sheets, MS Excel) continue to to be a primary way for data storage, (some) analysis, and (some) visualization even for those using specialized statistical programs. Highly recommended is a workflow in which raw original data is never modified. Instead, it is imported and modified as a part of the workflow. Recommended workflow of data analysis (From Wickham and Grolemnund “R for Data Science”, published January 2023, 2nd edition). 3.1 Consistency and conventions Organization is critical for reproducible research and archiving. A common scenario in your scientific practice is to have multiple time-sensitive projects occurring simultaneously, organization is critical. Consistency is critical for: Recording categorical variables (a type of variable we will learn about in this class), Use a consistent fixed code for any missing values (e.g. ‘NA’ or an unfilled/empty cell). Use consistent variable (column) names. Use a consistent data layout in multiple files (same column names). This allows data to be merged in a seamless way. Use a consistent format for all dates and times (e.g. YMD, DMY). Use consistent phrases in your notes. Notes are data. So, treating these as variables that have a binary, nominal, or ordinal value will allow you to later evaluate these quantitatively. Be careful about extra spaces within cells. Again, most (all?) software will read these as character codes that must be post-processed for analysis. One variable is recorded in each cell (remember, in our scheme, a comment is a variables). Strive for a rectangular data layout. Avoid font and cell colors as annotation. Use .csv or some other file back up. Following the approaches above will allow you to better manage your projects (including work done in this class!). 3.2 RStudio and R Is a developer environment for coding, data management, and version control. Multi-pane structure Script editor Console Environment and history Files, plots, packages and help 3.2.1 R installation Open an internet browser and go to www.r-project.org. Click the “download R” link in the middle of the page under “Getting Started.” Select a CRAN location (a mirror site) and click the corresponding link. Click on the “Download R for (Mac) OS X” link at the top of the page. Click on the file containing the latest version of R under “Files.” Save the .pkg file, double-click it to open, and follow the installation instructions. Now that R is installed, you need to download and install RStudio. 3.2.2 RStudio installation Go to www.rstudio.com and click on the “Download RStudio” button. Click on “Download RStudio Desktop.” Click on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder. 3.2.3 Script Editor Allows the user to create multi-line code. It is here that you will develop your code and send it to the console. You will save your scripts as appropriately titled .r files. To create a new R script you can either go to File -&gt; New -&gt; R Script, or click on the icon with the “+” sign and select “R Script”, or simply press Ctrl+Shift+N. Make sure to save the script. 3.2.4 Console The console is where you can type commands and see output. &gt; is the R Prompt Symbol: You should see the R prompt symbol in your console. If you don’t see the prompt, you cannot execute code. + is the R Prompt Symbol meaning you have unfinished code from the previous line. This often occurs if you have an open (unmatched) parentheses or a multiline input. Pressing the ESC will return the &gt; symbol to your console. 3.2.5 Files tab The files tab shows the directory structure and allows GUI manipulation of directories. 3.2.6 Plots tab The plots tab will show all your figures and it is possible to scroll through multiple plots windows. 3.2.7 Packages tab Provides a list of installed packages and a mechanism to load new ones. Here, you can install packages here or using the command line (in the console). # In the console type: install.packages(&quot;package name in quotes&quot;) # Example: install.packages(&quot;dplyr&quot;) For some packages you will see that “dependencies” are installed when the desired package is installed. These are the packages that the installed package needs in order to run some or all of the functions. 3.2.8 Help tab This tab will be automatically selected whenever you run help code in the Console. 3.2.9 History tab The history tab keeps a record of all previous commands. This can be useful when testing and running processes. To use archived code here, select all and click on the “To Source” icon, a window on the left will open with the list of commands. 3.2.10 Environment tab Displays data, their classes, and dimensions in your workspace. "],["working-in-r.html", "4 Working in R 4.1 Arithmetic operators 4.2 Arithmetic operators practice 4.3 Missing values (NA) 4.4 Dealing with missing values 4.5 Dealing with problematic values", " 4 Working in R 4.1 Arithmetic operators Symbol Operation + addition - subtraction * multiplication / division ^ exponentiation PEMDAS applies when writing R-code # Arithmetic operators in action: 5 + 5 / 2 3 * 2^2 (3*2)^2 # Arithmetic operators using objects: z &lt;- 5 w &lt;- c(3,7,9,2) s &lt;- w[3] z + s 4.2 Arithmetic operators practice Using what you know about parenthesis and PEMDAS, in one line of code do each of the following: Number Excercise 1. Assign the variable x to be a vector containing the values 5,5,6,2 2. Assign the variable y to be a vector containing the values 3,3,1,7 3. Add x and y 4. Substract y from x 5. Assign d as y divided by x 3. Multiply z by s then add five (z and s from last practice) 4. Add 5 to z then multilply by s 5. Take z to the fifth power and then add 2 6. Divide s by three, then add 33, then take that sum to the 0.5 power 4.3 Missing values (NA) # is.na tests for missing values dat.1 &lt;- c(-1,NA,1,1,-1) dat.1 + 2 dat.1 + rep(2, length(dat.1)) 4.4 Dealing with missing values In R, missing values are represented by “NA”. Undefined values (like dividing by zero) are represented by “NaN”, not a number. Often missing values are represented with numbers: -1, 99, -9999, etc. This is obviously a problem and should be avoided. You will likely need to use indexing prior using arithmetic operations to replace these values. The “is.na” function and the “na.rm”” argument: Sometimes we do not know whether there are missing values in our data. We can use the is.na function to test for missing values: # is.na tests for missing values dat.1 &lt;- c(-1,NA,1,1,-1) is.na(dat.1) which(is.na(dat.1)) We can use the logical na.rm argument to remove missing values from our data prior to executing the funciton: dat.1 &lt;- c(-1,NA,1,1,-1) mean(dat.1) mean(dat.1, na.rm = T) 4.5 Dealing with problematic values Sometimes we code no data as -1 and that can really screw things up. R does not know that -1 means “no data”. However we can replace the -1 with NA. There are many ways to do this, but here is a one way. # In this example, -1 is coded as a missing data field. Think back to our logical arguments and subsetting exercises. dat.2 &lt;- c(2,-1,3,4,5) dat.2 == -1 dat.2[dat.2 == -1] dat.2[dat.2 == -1] &lt;- NA dat.1 &lt;- c(2,NA,3,4,5) # is.na tests for missing values is.na(dat.1) # returns the element(s) number in the vector that is NA which(is.na(dat.1)) "],["data-classes-in-r.html", "5 Data Classes in R 5.1 Numeric 5.2 Integer 5.3 Character 5.4 Logical 5.5 Factor 5.6 List 5.7 Vectors 5.8 Matrix 5.9 DataFrame", " 5 Data Classes in R There are various kinds of R-objects or data structures: Vectors Lists Matrices Arrays Factors Data Frames Let’s first understand some of the basic datatypes on which the R-objects are built like Numeric, Integer, Character, Factor, and Logical. 5.1 Numeric num &lt;- 1.2 print(num) class(num) 5.2 Integer Integer: Numbers that do not contain decimal values have a data type as an integer. However, to create an integer data type, you explicitly use as.integer() and pass the variable as an argument. int &lt;- as.integer(2.2) print(int) class(int) 5.3 Character As the name suggests, it can be a letter or a combination of letters enclosed by quotes is considered as a character data type by R. char &lt;- &quot;datacamp&quot; print(char) class(char) char &lt;- &quot;12345&quot; class(char) 5.4 Logical Logical: A variable that can have a value of True and False like a boolean is called a logical variable. log_true &lt;- TRUE print(log_true) class(log_true) log_false &lt;- FALSE class(log_false) 5.5 Factor Factor: They are a data type that is used to refer to a qualitative relationship like colors, good &amp; bad, course or movie ratings, etc. They are useful in statistical modeling. fac &lt;- factor(c(&quot;good&quot;, &quot;bad&quot;, &quot;ugly&quot;,&quot;good&quot;, &quot;bad&quot;, &quot;ugly&quot;)) print(fac) class(fac) levels(fac) nlevels(fac) class(levels(fac)) 5.6 List Unlike vectors, a list can contain elements of various data types and is often known as an ordered collection of values. It can contain vectors, functions, matrices, and even another list inside it (nested-list). lis1 &lt;- seq(1,5) # Integer Vector lis2 &lt;- factor(1:5) # Factor Vector lis3 &lt;- letters[1:5] combined_list &lt;- list(lis1, lis2, lis3) combined_list[[1]] combined_list[[2]] combined_list[[3]] combined_list[[3]][5] flat_list &lt;- unlist(combined_list) class(flat_list) 5.7 Vectors Vectors are an object which is used to store multiple information or values of the same data type. A vector can not have a combination of both integer and character. For example, if you want to store 100 students’ total marks, instead of creating 100 different variables for each student, you would create a vector of length 100, which will store all the student marks in it. marks &lt;- c(88,65,90,40,65) marks[4] marks[1] marks[6] 5.7.1 Slicing vectors marks[seq(1,4)] marks[c(1,2,4)] char_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) class(char_vector) length(char_vector) char_vector[1:3] char_vector[-3] char_num_vec &lt;- c(1,2, &quot;a&quot;) char_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) class(char_vector) length(char_vector) char_vector[1:3] char_vector[-3] vec &lt;- seq(1,1024) vec &lt;- c(1:1024) 5.8 Matrix Similar to a vector, a matrix is used to store information about the same data type. However, unlike vectors, matrices are capable of holding two-dimensional information inside it. M &lt;- matrix(vector, nrow=r, ncol=c, byrow=FALSE, dimnames=list(char_vector_rownames, char_vector_colnames)) # byrow=TRUE signifies that the matrix should be filled by rows. # byrow=FALSE indicates that the matrix should be filled by columns (the default). M &lt;- matrix(seq(1,6), nrow = 2, ncol = 3, byrow = TRUE) M &lt;- matrix(seq(1,6), nrow = 2, ncol = 3, byrow = F) 5.9 DataFrame Unlike a matrix, Data frames are a more generalized form of a matrix. It contains data in a tabular fashion. The data in the data frame can be spread across various columns, having different data types. dataset &lt;- data.frame( Person = c(&quot;Aditya&quot;, &quot;Ayush&quot;,&quot;Akshay&quot;), Age = c(26, 26, 27), Weight = c(81,85, 90), Height = c(6,5.8,6.2), Salary = c(50000, 80000, 100000)) class(dataset) nrow(dataset) ncol(dataset) df1 &lt;- rbind(dataset, dataset) df2 &lt;- cbind(dataset, dataset) head(df1,3) str(dataset) summary(dataset) "],["data-input-and-output-io.html", "6 Data input and output (IO) 6.1 Some considerations: 6.2 Reading and Writing Files 6.3 Small data 6.4 Practice 6.5 Large(r) data", " 6 Data input and output (IO) 6.1 Some considerations: Keep the names of local files downloaded from the internet or copied onto your computer unchanged. This will help you trace the provenance of the data in the future. R’s native file format .RData can be accessed using load and save. 6.2 Reading and Writing Files There are many methods to read and write files in R programming. Your command of these is critical because all scientific work begins with data, and most data is found inside files and databases. Dealing with input is probably the first step of implementing any significant project. 6.3 Small data For very small datasets, is may be preferred to enter the data by hand. c is a common function used for combine: x &lt;- c(3,7,11,19) y &lt;- c(1,1,1,1) c(x,y) y &lt;- c(x,y,5) There are a suite of functions to enter data in the console. The sequence function seq: y &lt;- seq(from = 1, to = 10) y &lt;- seq(from = 1, to = 10, by = 2.5) y &lt;- seq(from = 1, to = 10, length.out = 22) Use the function rep (repeat): y &lt;- rep(x = 5, times = 4) x.value &lt;- c(2,3) rep(x.value, times = 3) 6.4 Practice Number Excercise 1. How many different ways can you create a vector labled q containing two 3’s and four 5’s? Try some! 2. Assign a vector of four elements: 3,7,9 and 2 to w. 3. Assign the third element of w to s, where s is equal to 6. 4. What is the length of a sequence that starts at 1.1, ends at 9.2, and has increments of 0.894? 5. What is the 3rd value of the sequence you created? # Create a data frame using the &quot;data.frame&quot; function site.name &lt;- c(rep(&quot;Site.01&quot;,3),rep(&quot;Site.02&quot;,3)) density &lt;- rep(x = 2.3, times = length(site.name)) abundance &lt;- seq(from = 14.5, to = 19.8, length.out = length(site.name)) sampled. &lt;- c(F, T, F, F, T, F) y.data.frame &lt;- data.frame(site.name, density, abundance, sampled.) y.data.frame class(y.data.frame) 6.5 Large(r) data 6.5.1 Read csv Most scientific work will involve data larger than can be entered by hand. In this case we will use a suite of commands and different packages to get the data into our environment. read.csv(&quot;./Data/co2.csv&quot;) 6.5.2 Read xlsx MS Excel files are widely used install.packages(&#39;readxl&#39;) require(&#39;readxl&#39;) read_xlsx(&quot;./Data/Codes.xlsx&quot;, sheet = 1) 6.5.3 Download data from public repository Website can be found here: &quot;https://www.stats.govt.nz/large-datasets/csv-files-for-download/&quot; The data are accessed by this URL: url &lt;- &quot;https://www.stats.govt.nz/assets/Uploads/Annual-enterprise-survey/Annual-enterprise-survey-2020-financial-year-provisional/Download-data/annual-enterprise-survey-2020-financial-year-provisional-csv.csv&quot; You can see that the url links to a .csv file. This is a file in an online directory. Access the directory in a web browser: https://www.stats.govt.nz/ destfile &lt;- &quot;./Data/output.csv&quot; download.file(url, destfile) site: “bookdown::bookdown_site” output: bookdown::gitbook: lib_dir: “book_assets” bookdown::pdf_book: keep_tex: yes "],["sampling.html", "7 Sampling 7.1 Replication 7.2 Simple Random Sampling 7.3 Some Terminology of Sampling 7.4 Independence of samples 7.5 Why do we want to collect representative samples? 7.6 Sampling Design Considerations 7.7 What is Pseudoreplication? 7.8 Examples of Pseudoreplication 7.9 Sample allocation 7.10 Systematic Sampling", " 7 Sampling Statistical analyses are performed on samples that are taken from large populations (we consider most populations to be infinitely large and thus sampling is necessary). The goal is to sample some elements in a population that are representative of the whole population. This saves resources because it is not desirable or feasible to sample every element in a population. Sampling every element in a population is called a census. The most challenging aspect of a sampling program is to ensure that the sample taken from the population is a random one. 7.1 Replication Replication means having replicate observations at a spatial and temporal scale that matches the application of the experimental treatments. Replicates are essential because biological systems are inherently variable and this is particularly so for ecological systems. 7.2 Simple Random Sampling To accomplish this, the most robust approach is to use simple random sample (termed SRS). From QK “You can’t really go wrong with simple random sampling.” But… “The downside of simple random sampling is that it may be less efficient than other sampling designs, especially when there is identified heterogeneity in the population or we wish to estimate parameters at a range of spatial or temporal scales.” In SRS, each unit in the population is identified, and each unit has an equal chance of being in the sample. This is the only way to obtain a statistically valid and representative sample. Ensuring that data are collected in a random fashion allows statistics to be calculated. Non-random data collection disallows this. Only statistically valid observations can be used for analysis. Consider Exit polling vs. Twitter polls to estimate proportion voting for a candidate. Why does one provide a better estimate than another? BTW, this is the primary issue with current political polling - the quest to get a representative sample (https://fivethirtyeight.com/features/is-the-polling-industry-in-stasis-or-in-crisis/) 7.3 Some Terminology of Sampling a population is the entire collection of people or things you are interested in (all cannot be measured, or even should be); a sample is a subset of some of the units in the population; a statistic or summary statistic is a value that results from measuring all the units in the sample (e.g. mean, mode, range); For example, to find out the average age of all motor vehicles in the state in 2022: Population = all motor vehicles in the state in 2022 Sample = 300 randomly selected motor vehicles Statistic = the average age of the 300 motor vehicles in the sample Statistics derived from samples are used to estimate population parameters. In the above example, we would say that the sample statistic (sample mean) is our best estimate of the population mean. We did not perform a census, so we don’t know what the mean of the population is, but instead use our sample statistic. Another example, how to select a sample of 25 people who live in your college dorm, and ensure a random sample you might: Make a list of all the 250 people who live in the dorm. Assign each person a unique number, between 1 and 250. Then refer to a table of random numbers. Starting at any point in the table, read across or down and note every number that falls between 1 and 250. Use the numbers you have found to pull the names from the list that correspond to the 25 numbers you found. These 25 people are your sample. This is called the table of random numbers method. 7.4 Independence of samples Independence is also a critical aspect of sampling. Independence implies that sampling (measuring) of one element in the population will not impact or predict the value of another element. 7.5 Why do we want to collect representative samples? We use statistical analysis to describe (calculate) the values of sample statistics among or between populations. For example, is the mean value different between two populations or to test whether a statistic (like the mean) is different from some hypothesized value. 7.5.1 R Code ## Provide code in R (using base R functionality) to sample a simulated distribution using ## simple random sampling. ## ```R ## # Set the seed for reproducibility ## set.seed(123) ## ## # Generate a simulated population of numbers ## population &lt;- rnorm(1000, mean = 50, sd = 10) # Generating random data with 1000 samples which follows normal distribution with mean of 50 and standard deviation of 10. ## ## # Sample 100 numbers from the population using simple random sampling ## sampled_data &lt;- sample(population, size = 100, replace = FALSE) # Selecting 100 random samples, without replacement, from the generated population. This sample is stored in variable &quot;sampled_data&quot;. ## ## # View the first 5 numbers in the sampled data ## head(sampled_data) # Displays the first five numbers in the sampled_data dataset. ## ``` ## ## Explanation: ## ## - The `set.seed()` function is used to set the seed for reproducibility of the random numbers generated. ## - The `rnorm()` function is used to simulate a population of 1000 numbers following the standard normal distribution with a mean of 50 and standard deviation of 10. ## - The `sample()` function is used to randomly select 100 numbers from the population without replacement (i.e., `replace = FALSE`). The sampled data is stored in the variable `sampledata`. ## - Finally, `head()` is used to view the first 5 numbers in the sampled data. 7.6 Sampling Design Considerations Hurlburt (1984) Now that we know what constitutes a valid sample, lets look at some ways (and some ways not) to collect (and analyze) samples. Hurlburt’s paper has been instrumental in promoting an understanding of pseudoreplication and the term has become widely used. Scientists have become more aware of the need for close concordance of design, analysis, and interpretation of experiments. “No one would now dream of testing the response to a treatment by comparing two plots, one treated and the other control.” Fisher and Wishart (1930). Hurlburt evaluated the frequency of pseudoreplication as a fatal flaw of experiments (Hurlbert, 1984. Ecological Monographs 54:187‐211). The ‘fatal flaw’ implies that the conclusions of the work are not supported because the sampling was done improperly. In the paper, Hurlburt Evaluated 176 studies from 1960 to 1984. He found that 27% overall or 48% of those making statistical inferences used “pseudoreplicated” approaches. The term pseudoreplication refers to “the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent.“ The context of his paper was ecological field experiments, but pseudoreplication can occur in other contexts as well. 7.7 What is Pseudoreplication? Replication refers to having more than one experimental (or observational) unit with the same treatment. Each unit with the same treatment is called a replicate. Most models for statistical inference require true replication. True replicates are often confused with repeated measures or with pseudoreplicates. 7.8 Examples of Pseudoreplication Suppose a blood-pressure lowering drug is administered to a patient, then the patient’s blood pressure is measured twice. This is a repeated measure, not a replication. It can give information about the uncertainty in the measurement process, but not about the variability in the effect of the drug. On the other hand, if the drug were administered to two patients, and each patient’s blood pressure was measured once, we can say the treatment has been replicated, and the replication may give some information about the variability in the effect of the drug. A researcher is studying the effect on plant growth of different concentrations of CO2 in the air. He needs to grow the plants in a growth chamber so that the CO2 concentration can be controlled. He has access to only two growth chambers, but each one will hold five plants. However, since the five plants in each chamber share whatever conditions are in that chamber besides the CO2 concentration, and in fact may also influence each other, they are not independent replicates but are pseudoreplicates. The growth chambers are the experimental units; the treatments are applied to the growth chambers, not to the plant independently. Two fifth-grade math curricula are being studied. Two schools have agreed to participate in the study. One is randomly chosen to use curriculum A, the other to use curriculum B. At the end of the school year, the fifth-grade students in each school are tested and the results are used to do a statistical analysis comparing the two curricula. There is no true replication in this study; the students are pseudo-replicates. The schools are the experimental units; they, not the students, are randomly assigned to treatment. Within each school, the test results (and the learning) of the students in the experiment are not independent; they are influenced by the teacher and other school-specific factors (e.g., previous teachers and learning, socioeconomic background of the school, etc.). 7.9 Sample allocation Hurlbert’s other primary concern with proper experimental design is to ensure independence. He shows that with randomization methods do not always giving good interspersion of treatments (spatial and temporal). Hence his focus on random interspersion of samples from a population. Examples of allocation and randomization approaches. 7.10 Systematic Sampling SS is a probability sampling method where members of population selected at regular interval (k). The benefits of SS are that is imitates the randomization benefits of simple random sampling but can be easier to conduct. To employ this we will take a full list of the population and sample every \\(k^{th}\\) object. To determine k divide estimated population size by sample size. 7.10.1 Stratified Random Sampling The way in which was have selected sample units thus far has required us to know little about the population of interest in advance of selecting the sample. This approach is ideal only if the characteristic of interest is distributed homogeneously across the population. In a stratified sample, researchers divide a population into subpopulations called strata (the plural of stratum) based on specific characteristics (e.g., habitat, niche, taxanomic status, location, etc.). Every member of the population studied should be in exactly one stratum. For example, if we have information that we know to be associated with the heterogeneity in the population, we can use that ancillary information to guide alternative strategies for selecting samples that will yield estimates with higher precision that a simple random sample for the same amount of effort. The first of these designs is stratified random sampling. A stratified random sample is one obtained by dividing the population elements into mutually exclusive, non-overlapping groups of sample units called strata, then selecting a simple random sample from within each stratum (stratum is singular for strata). Stratifying involves classifying sampling units of the population into relatively homogeneous groups before (usually) selecting sample units. Strata are based on information other than the characteristic being measured that is known to or thought to vary with the characteristic of interest. Stratified Sampling Scheme. Because virtually all ecological systems are heterogeneous, stratifying is used commonly as a way to increase precision in ecological studies. Common strata in ecological studies include elevation, aspect, or other geographic features for studying plant communities and vegetation communities or soils for studying some animal communities. When choosing among several potential strata, seek strata that best minimize variation in the characteristic of interest within strata and that maximize variation among strata. How it is implemented: * Divide the entire population into non-overlapping strata * Select a simple random sample from within each strata L = number of strata \\(n_i\\) = number of sample units within stratum i n = number of sample units in the population Estimates from stratified random samples are simply the weighted average or the sum of estimates from a series of simple random samples, each generated within a unique stratum. \\(\\bar{y} = \\sum_{h=1}^{L}W_{h}\\bar{y}_h\\), where there are h = 1 to L strata, \\(W_h\\) is the proportion of total units in stratum h (often estimated from the proportion of total area in stratum h) and \\(\\bar{y}_h\\) is the sample mean for stratum h. 7.10.2 Allocating Sampling Effort among Strata Using stratified random sampling requires that we decide how to divide a fixed amount of sampling effort among the different strata; that process is called allocation. When deciding where to allocate sampling effort, the question becomes how best to allocate effort among strata so that the sampling process will provide the most efficient balance of effort, cost, and estimate precision. Uniform Allocation (equal number for all strata) Allocation Proportional to Size Allocation Proportional to Variation (developed from pilot study) "],["central-limit-theorem.html", "8 Central Limit Theorem", " 8 Central Limit Theorem The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size becomes larger regardless of the population distribution shape. The sample theory is the study of relationships existing between a population and samples drawn from population. Consider all possible samples of size n that can be drawn from the population. For each sample, we can compute statistic like mean or a standard deviation, etc that will vary from sample to sample. This way we obtain a distribution called as the sampling distribution of a statistic. If the statistic is sample mean , then the distribution is called the sampling distribution of mean. If we take many sets of samples from a population, and calculated the mean of these, we could plot the frequency distribution of the sample means. This distribution is the ‘sampling distribution’ It: 1.) The sampling distribution from a normal distribution is normally-distributed. 2.) As sample size increases (to infinity), the sampling distribution, from any distribution, will approach a normal distribution. 3.) The expected value (the mean) of the sample distribution will be the mean of the population distribution. 8.0.1 Example of the CLT (Die Rolling) A fair die can be modeled with a discrete random variable with outcome 1 through 6, each with the equal probability of 1/6. DieOutcome &lt;- sample(1:6,10000, replace= TRUE) hist(DieOutcome, col =&quot;light blue&quot;) abline(v=3.5, col = &quot;red&quot;,lty=1) We will take samples of size 10 , from the above 10000 observation of outcome of die roll, take the arithmetic mean and try to plot the mean of sample. we will do this procedure k times (in this case k= 10000 ) x10 &lt;- c() k =10000 for ( i in 1:k) { x10[i] = mean(sample(1:6,10, replace = TRUE))} hist(x10, col =&quot;pink&quot;, main=&quot;Sample size =10&quot;,xlab =&quot;Outcome of die roll&quot;) abline(v = mean(x10), col = &quot;Red&quot;) abline(v = 3.5, col = &quot;blue&quot;) By theory , we know as the sample increases, we get better bell shaped curve. As the n apporaches infinity , we get a normal distribution. Lets do this by increasing the sample size to 30, 100 and 1000. x30 &lt;- c() x100 &lt;- c() x1000 &lt;- c() k =10000 for ( i in 1:k){ x30[i] = mean(sample(1:6,30, replace = TRUE)) x100[i] = mean(sample(1:6,100, replace = TRUE)) x1000[i] = mean(sample(1:6,1000, replace = TRUE)) } par(mfrow=c(1,3)) hist(x30, col =&quot;green&quot;,main=&quot;n=30&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x30), col = &quot;blue&quot;) hist(x100, col =&quot;light blue&quot;, main=&quot;n=100&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x100), col = &quot;red&quot;) hist(x1000, col =&quot;orange&quot;,main=&quot;n=1000&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x1000), col = &quot;red&quot;) 8.0.2 Example of the CLT (Coin Flipping) Flipping a fair coin many times the probability of getting a given number of heads in a series of flips should follow a normal curve, with mean equal to half the total number of flips in each series. Here 1 represent heads and 0 tails. x &lt;- c() k =10000 for ( i in 1:k) { x[i] = mean(sample(0:1,100, replace = TRUE))} hist(x, col =&quot;light green&quot;, main=&quot;Sample size = 100&quot;,xlab =&quot;flipping coin &quot;) abline(v = mean(x), col = &quot;red&quot;) "],["measures-of-central-tendency.html", "9 Measures of Central Tendency 9.1 Population Mean 9.2 Sample Mean 9.3 Mean from the frequency distribution 9.4 Skew 9.5 Maximum Likelihood", " 9 Measures of Central Tendency Now we will start to calculate summary statistics. Zar 4th edition 3.1 to 3.5 The mode is the most frequently occurring value in the population or sample. The median is the 50th percentile in the ordered data. The mean is the most efficient, unbiased and consistent estimator of \\(\\mu\\). \\(\\mu\\) is the mean of the characteristic for the population and the one we would expect to find most frequently. Thus it is the best measure of the ‘central tendency’. The center of the distribution where we get maximum frequency. An efficient estimator is one that estimates the quantity of interest in some “best possible” manner. “unbiased” reflects the accuracy of the estimator - how close is it to the population’s value. 9.1 Population Mean \\(\\mu = \\frac{\\sum_{i = 1}^N{X_i}}{N}\\) 9.2 Sample Mean \\(\\bar{X} = \\frac{\\sum_{i = 1}^n{X_i}}{n}\\) 9.3 Mean from the frequency distribution \\(\\bar{X} = \\frac{\\sum_{i = 1}^k{f_iX_i}}{n}\\) Here, k is the number of bins. 9.3.1 R Code ## Provide code in R (using base R functionality) to report the mean, median, and mode of a ## sample. ## ```R ## # Sample data ## # Create a vector &#39;sample&#39; containing some numeric values ## sample &lt;- c(1, 3, 2, 5, 6, 3, 4, 2, 3, 2) ## ## # Mean ## # Calculate the mean of the sample vector using &#39;mean&#39; function and store the result in &#39;mean_sample&#39; variable ## mean_sample &lt;- mean(sample) ## ## # Print the mean value to the console using &#39;cat&#39; function ## cat(&#39;The mean of the sample is:&#39;, mean_sample, &#39;\\n&#39;) ## ## # Median ## # Calculate the median of the sample vector using &#39;median&#39; function and store the result in &#39;median_sample&#39; variable ## median_sample &lt;- median(sample) ## ## # Print the median value to the console using &#39;cat&#39; function ## cat(&#39;The median of the sample is:&#39;, median_sample, &#39;\\n&#39;) ## ## # Mode ## # Calculate the mode of the sample vector ## # Get the frequency of each value in the &#39;sample&#39; vector using &#39;table&#39; function ## # Sort the frequency table in decreasing order and select the first value using &#39;names&#39; function ## mode_sample &lt;- names(sort(table(sample), decreasing = TRUE))[1] ## ## # Print the mode value to the console using &#39;cat&#39; function ## cat(&#39;The mode of the sample is:&#39;, mode_sample) ## ## ``` ## ## This code will output: ## ## ``` ## The mean of the sample is: 3.2 ## The median of the sample is: 3 ## The mode of the sample is: 2 ## ``` ## ## Note that for the mode calculation, we first use `table` to get the frequency of each value in the sample, then we sort them in decreasing order and select the first value using `names`. This assumes that the mode is unique and there is at least one value that appears more than once in the sample. 9.4 Skew A left-skewed distribution has a long left tail. A right-skewed distribution has a long right tail. 9.4.1 R Code ## Provide code in R (using base R functionality) to create and plot skewed distributions ## (posititve and negative). ## ``` ## # Generate skewed data using the rlnorm() function with a seed of 123 ## set.seed(123) ## x &lt;- rlnorm(1000) ## ## # Plot skewed distribution using the hist() function with 30 bins, a blue color, and a main title ## hist(x, breaks = 30, col = &#39;steelblue&#39;, main = &#39;Positive Skewed Distribution&#39;) ## ``` ## ## ``` ## # Generate negatively skewed data using the negative value of rlnorm() with a seed of 123 ## set.seed(123) ## x &lt;- -rlnorm(1000) ## ## # Plot negatively skewed distribution using the hist() function with 30 bins, a blue color, and a main title ## hist(x, breaks = 30, col = &#39;steelblue&#39;, main = &#39;Negative Skewed Distribution&#39;) ## ``` ## ## The use of `rlnorm()` for generating skewed data is highlighted, and how negative values can create a negatively skewed distribution is demonstrated. Additionally, the `hist()` function is used to create a histogram of the generated data with specific parameters such as number of breaks, color and main title. 9.5 Maximum Likelihood The primary source for the following lecture material are primarily derived from I.J. Myung’s paper “Tutorial on maximum likelihood estimation”. Journal of Mathematical Psychology 47 (2003) 90-100. The interested reader is directed to review this paper. In this lecture we will understand an alternative to parameter estimation using least-squares estimation, maximum likelihood estimation (MLE). MLE is a preferred method of parameter estimation in statistics and is a general parameter estimation approach, in particular in non-linear modeling and/or with non-normally distributed data. Because of the prevelance of non-normally distributed data in the natural sciences (e.g. data from counting, categorical data, skewed ratio data) an examination and familiarization of this approach is useful. 9.5.1 General Exprimental Approach Because many phenomenon of interest are not directly observable, we formulate hypotheses to test and these hypothesis are stated in terms of probability using statistical models. The goal of statistical modeling is to understand underlying processes by testing the viability (e.g. quality, robustness) of the model. Our method: Specify a statistical model Collect data Evaluate how well the model fits the data by: Parameter estimation Evaluating goodness of fit 9.5.2 Two approaches to parameter estimation: LSE (least-squares estimation), for normally-distributed data MLE, a general approach for parameter estimation 9.5.3 The probability density function The goal of data analysis is to identify the population that is most likely to have generated the sample - i.e. we will estimate the parameters of the candidate model that will produce these observations. The data vector \\(y = (y_1, y_2, ..., y_m)\\) is a random sample from a population distributed in some unknown way. Populations are identified using a probability distribution and unique values of the parameters - As the parameter changes in value, different probability distributions are generated. Let \\(f(y|w)\\) denote the probability density function (PDF) that specifies the probability of observing data vector y given the parameter w. The parameter \\(w = (w_1, ..., w_k)\\) is a vector defined on a multi-dimensional parameter space. For example, if the PDF is normal, \\(w = (\\mu, \\sigma)\\) For this lecture we are primary concerned with the estimation of \\(\\mu\\). We do this by determining \\(\\bar{X}\\). Note, I present MLE because it is a very general way to determine parameter values: If the PDF is a t distribution, \\(w = (d.f.)\\) Different distributions are defined using different parameters (and different mathematical formulations), so w is distribution-specific. If we have specified a distribution that has a certain set of parameters, for example: We can use a given probability distribution and parameter set to determine the probability of obtaining a value in a population. Example: Children’s IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What is the probability of a randomly selected child having an IQ between 80 and 120? In this case the area under the curve is 0.818 or 81.8% of the integral of the distribution from \\(-\\infty\\) to \\(+\\infty\\). So, if we take 100 random draws from the population of children’s IQs, we will get values of IQ &gt; 80 and &lt; 120, 81 to 82 times… Let’s examine the statement: \\(p(80 &lt; IQ &lt; 120\\) | \\(\\mu = 100, \\sigma = 15) = 0.818\\). We are stating that the probability of randomly selecting an IQ observation the variable characteristics given the parameter values is 0.818. If we are interested in finding probabilities of students with different variable characteristics (different IQ values), then we will change the left side of the equation. For example: \\(p(IQ &lt; 65\\) | \\(\\mu = 100, \\sigma = 15)\\) or \\(p(IQ &gt; 120\\) | \\(\\mu = 100, \\sigma = 15)\\) The right side of the equation does not change because it describes the fixed shape of the distribution of the population that “creates” the observations. So when we are investigating probability of an event we are quantifying the integral of the curve for the given parameter set bounded by the left side of the equation. We change the left side to derive new probability values. We can determine the probability of obtaining \\(p(y_1, y_2, ..., y_m)\\) | \\(w)\\) if the observations are independent. Think about taking multiple random draws from the population described by the parameter set w. An analog here is thinking about coin flipping, the probability of realizing specific outcomes from multiple trials is determined by multiplication (e.g. probability of observing two “tails” flips is the product of the independent single observations). So, the notation below is Pi (product) notation. This is used in mathematics to indicate repeated multiplication. \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = p(y_1\\)| \\(w) \\times p(y_2\\)| \\(w) \\times ...p(y_n\\)| \\(w)\\) \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = \\prod_{i = 1}^{n}{p(y_i | w)}\\) 9.5.4 The likelihood function Now we will discuss the likelihood function that is derived by considering that the data are fixed and that the shape of the (parent) distribution is random. In this approach we are evaluating the inverse problem of the PDF. Specifically, given the observed data and a model of interest, we are interested in finding the unique parameter vector w, among all the possible combinations of parameters that is most likely to have produced the data. We have already observed the data and now want to define the likelihood function by reversing the roles (i.e. what is random what is fixed) of the data vector y and the parameter vector w. So, we focus on \\(L(w|y)\\). This represents the likelihood of the parameter w given the observed data y; and as such is a function of w. So, the data are fixed, and we modify the parameter vector w (in the case of IQ, \\(\\mu\\) and \\(\\sigma\\)) Assume we have some vector of observed data y, these are sampled from some population, for now, assume we take a single value. What is the likelihood that \\(\\mu = 100\\) and \\(\\sigma = 15\\) given our sampled IQ is \\(y = 120\\)? What if we have a vector of observations \\(n = 5\\), with mean values centered on 120? So, we want to find the distribution parameters that maximize the likelihood. We still think that the normal distribution is the most appropriate distribution as a candidate distribution. \\(y = (100, 110, 120, 130, 140)\\) "],["measures-of-dispersion.html", "10 Measures of Dispersion 10.1 Range 10.2 The Interquartile Range 10.3 Other quantiles 10.4 Error and Deviations from Expectations 10.5 Quantifying Error 10.6 Sum of Squared Errors 10.7 Variance 10.8 Standard Deviation 10.9 Summary of Variance Estimates 10.10 Coefficient of Variation", " 10 Measures of Dispersion 10.1 Range The smallest score subtracted from the largest score in the observation, e.g.  Number of contacts of n = 11 randomly selected social media users. 22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252 You can see that these observations are ordered (from smallest to largest) Range = 252 - 22 = 230 10.1.1 R Code ## Provide code in R (using base R functionality) to report the range, minimum, and maximum ## of a set of values. ## ```R ## # create a vector of values ## values &lt;- c(5, 2, 8, 3, 11, 6) ## ## # calculate the range ## range &lt;- max(values) - min(values) ## ## # calculate the minimum value ## minimum &lt;- min(values) ## ## # calculate the maximum value ## maximum &lt;- max(values) ## ## # print the results ## cat(&#39;Range:&#39;, range, &#39;\\n&#39;) # print the range value ## cat(&#39;Minimum:&#39;, minimum, &#39;\\n&#39;) # print the minimum value ## cat(&#39;Maximum:&#39;, maximum, &#39;\\n&#39;) # print the maximum value ## ``` ## ## The comments describe each line of code, explaining what the code does. The `cat()` function is used to print the results to the console. You can modify the values of the `values` vector to get the range, minimum, and maximum of any set of numbers. 10.2 The Interquartile Range Identify the values that split the sorted data into four equal parts. First or lower quartile (the range values of the first 25% of values in ordered sequence) Second quartile (the range values of the first 25 to 50% of values in ordered sequence) Third quartile (the range values of the first 50 to 75% of values in ordered sequence) Fourth quartile (the range values of the first 75 to 100% of values in ordered sequence) 10.2.1 R Code ## Provide code in R (using base R functionality) to report values of the interquartile range ## from a simulated set of values. ## ``` ## # Generate a simulated set of values ## set.seed(123) # set the random seed for reproducibility ## values &lt;- rnorm(100) # generate 100 normally distributed values ## ## # Calculate the interquartile range ## q25 &lt;- quantile(values, 0.25) # calculate the 25th quartile of the values ## q75 &lt;- quantile(values, 0.75) # calculate the 75th quartile of the values ## iqr &lt;- q75 - q25 # calculate the interquartile range ## ## # Report the interquartile range ## cat(&#39;The interquartile range is:&#39;, iqr) # print the interquartile range to the console ## ``` 10.3 Other quantiles In an ordered list of observations, any quantile can be determined. Let’s examine the quantiles of interest for the ‘air time’ from flights departing NYC’s airports in 2013 with destination of the Atlanta aiport “ATL” with Delta Airlines. carrier flight tailnum origin dest air_time distance hour minute DL 461 N668DN LGA ATL 116 762 6 0 DL 1743 N3739P JFK ATL 128 760 6 10 DL 575 N326NB EWR ATL 120 746 6 15 DL 1547 N6703D LGA ATL 126 762 7 0 DL 2047 N935DL LGA ATL 126 762 7 59 Let’s look at the quantiles of air time for these flights. ## 0% 5% 10% 20% 30% 40% 50% 60% 70% 80% 90% 95% 100% ## 65 98 101 104 106 109 111 114 117 120 125 129 176 10.3.1 R Code ## Provide code in R (using base R functionality) to report values of the quantiles from a ## simulated set of values. ## Sure, here are the inline comments to the provided R code: ## ## ```R ## # Simulated set of values ## set.seed(123) # Setting the seed for R&#39;s random number generator for reproducibility ## x &lt;- rnorm(100) # Generating a set of 100 random values from a normal distribution ## ## # Calling quantile function to report the quantiles ## quantile(x) # Using the `quantile()` function to report the 25th, 50th (median), and 75th quantiles in R ## ``` ## ## This will output the following: ## ``` ## 0% 25% 50% 75% 100% ## -2.345697 -0.727719 -0.013617 0.659230 2.415835 ## ``` ## ## ## ```R ## # Reporting 10th, 50th, and 90th percentiles ## quantile(x, c(0.1, 0.5, 0.9)) # Using the `quantile()` function to report the 10th, 50th (median), and 90th percentiles ## ``` ## ## This will output the following: ## ``` ## 10% 50%``` ## 90% ## -1.2305730 1.3307126 ## ``` ## ## I hope this helps clarify the use of `quantile()` function in R! Let me know if you have any further questions. 10.4 Error and Deviations from Expectations Error is the difference between a value obtained from a data collection process and the ‘true’ value for the population. We can evaluate error for a single data point or we can evaluate the total error of our sample relative to our expectation of what the population value might be. 10.5 Quantifying Error A deviation is the difference between the mean (expected) and the observed data (the outcome of the sample). The deviation of observed and expected value is called: the residual, error, or residual error. The expected value is the one that we would encounter most frequently. The expected value, in the context of the normal distribution, is the mean value. When the normal fitting models: \\(deviation = X_i - \\bar{X}\\) Should we use the Total Error as an estimate of uncertainty? We could sum \\(i^{th}\\) error terms from 1 to n. i Score Score - mean(Score) 1 -0.28 -0.795 2 -0.14 -0.655 3 1.04 0.525 4 0.65 0.135 5 0.26 -0.255 6 1.56 1.045 \\(\\Sigma(X_i - \\bar{X}) = 0\\) 10.6 Sum of Squared Errors We could add the deviations to find out the total error, but the deviations ‘cancel out’ (some are positive and others negative) Therefore, we square each deviation. If we add these squared deviations we get the sum of squared errors (SS). i Score Deviation Squared Deviation 1 -0.28 -0.795 0.632 2 -0.14 -0.655 0.429 3 1.04 0.525 0.276 3 0.65 0.135 0.018 4 0.26 -0.255 0.065 5 1.56 1.045 1.092 \\(SS = \\Sigma(X_i - \\bar{X})^2 = 2.512\\) The sum of squares is a good measure of overall variability, but is dependent on the number of scores. 10.6.1 R Code ## Provide code in R (using base R functionality) to calculated the Sum of Squared Errors ## from a simulated set of values. ## ```r ## # Simulate a set of values ## set.seed(123) # set the seed for reproducibility ## values &lt;- rnorm(10) # generate 10 random numbers from a normal distribution ## ## # Calculate the mean of the values ## mean_values &lt;- mean(values) # find the average of the generated values ## ## # Calculate the Sum of Squared Errors ## sse &lt;- sum((values - mean_values)^2) # calculate the sum of squared errors from the mean ## ## # Print the results ## cat(&#39;The Sum of Squared Errors is:&#39;, sse) # print the calculated value of Sum of Squared Errors ## ``` 10.7 Variance In statistics, when we talk about population, we mean the entire universe of possible values of a stochastic (random) variable. Population variance: \\(\\sigma^2 = \\frac{SS}{N} = \\frac{\\Sigma^n_{i=1}(X_i-\\mu)^2}{N}\\) Most of the time, we don’t sample the entire population because it is too complex or simply not feasible. Think, for instance, at a problem when you want to analyze the heights of the oak trees in a forest. You can, of course, measure every single tree of the forest and so have collected statistics about the entire forest, but this could be very expensive and would take a very long time. So, we don’t generally know \\(\\mu\\). We calculate the average variability (average variability of each sample). So, obtain a sample of, let’s say, 20 trees and try to relate sample statistics and population statistics. Sample variance: \\(s^2 = \\frac{SS}{n-1} = \\frac{\\Sigma(X_i-\\bar{X})^2}{n-1}\\) Population variance: \\(\\sigma^2 = \\frac{SS}{N} = \\frac{\\Sigma_{i=1}^N(X_i-\\mu)^2}{N}\\) Why n-1 instead of N? When we compute the difference between each value and the mean of those values (observed - expected), we don’t know the true mean of the population; all you know is the mean of your sample. In general, we do not know the population mean, the sample mean is the mean of the data and should be close to the true population mean. The value computed in the sample variance will be an underestimate of the population sample mean and cannot be larger; we have likely not sampled the full range of the values in the population. Therefore, the sample variance will be an underestimate of the population variance. To make up for this, divide by n-1 rather than N. This is called Bessel’s correction. 10.7.1 R Code ## Provide code in R (using base R functionality) to calculated the variance from a simulated ## set of values. ## ``` ## # Simulate data ## # set the seed of the random number generator to ensure reproducibility ## set.seed(123) ## # generate 100 values from a standard normal distribution and store them in the vector x ## x &lt;- rnorm(100) ## ## # Calculate variance ## # use the var() function to calculate the variance of vector x and store the result in the variable variance_x ## variance_x &lt;- var(x) ## ## # Print variance ## # print the value of variance_x to the console ## print(variance_x) ## ``` 10.8 Standard Deviation The variance has one problem: it is measured in units2 (The original units, like the numbers are squared.). This isn’t a very meaningful metric so we take the square root value. This is the sample standard deviation (sd or s): \\(sd = s = \\sqrt\\frac{\\Sigma^n_{i=1}(X_i-\\bar{X})^2}{n-1}\\) \\(\\sigma = \\sqrt\\frac{\\Sigma^N_{i=1}(X_i-\\mu)^2}{N}\\) 10.8.1 R Code ## Provide code in R (using base R functionality) to calculated the standard deviation from a ## simulated set of values. ## ```R ## # Generate a simulated set of values ## set.seed(123) # set the seed for reproducibility ## simulated_data &lt;- rnorm(n = 100, mean = 10, sd = 2) # create a vector of 100 values drawn from a normal distribution with mean 10 and standard deviation 2 ## ## # Calculate the standard deviation ## sd(simulated_data) # calculate the standard deviation of the simulated data ## ``` 10.9 Summary of Variance Estimates The sum of squares, variance, and standard deviation represent the same thing: The fit of the mean to the data, how well the mean represents the observed data The variability in the data when modeled using the mean 10.10 Coefficient of Variation The coefficient of variation (CV) is a measure of the dispersion (measured by standard deviation) of data points in a data series around the mean. The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another. \\(CV = \\frac{s}{\\bar{X}}\\) 10.10.1 R Code ## Provide code in R (using base R functionality) to calculated the coefficient of variation ## from a simulated set of values. ## ``` ## # set the seed to ensure reproducibility ## set.seed(123) ## ## # generate a simulated set of 100 values from a normal distribution with mean=20 and sd=5 ## sim_values &lt;- rnorm(100, mean=20, sd=5) ## ## # calculate the coefficient of variation by dividing the standard deviation of the values by the mean of the values ## cv &lt;- sd(sim_values) / mean(sim_values) ## ## # print the result ## cv ## ``` ## ## This code generates a simulated set of 100 values based on a normal distribution with a mean of 20 and a standard deviation of 5, calculates the coefficient of variation for the generated values, and prints the result. By setting the seed, we ensure that the same set of values will be generated each time the code is run. You can adjust the parameters of the `rnorm()` function to generate different sets of values based on your needs. "],["the-normal-distribution.html", "11 The Normal Distribution 11.1 The Normal Probability Density Function 11.2 Z-scores 11.3 Properties of Z-scores 11.4 Areas under the Normal Curve for different quantile values 11.5 Checking for normality", " 11 The Normal Distribution The normal distribution is probably the most common distribution in all of probability and statistics. 11.1 The Normal Probability Density Function The probability density function for the normal distribution is defined as: \\(y_i = \\frac{1}{\\sigma\\sqrt2\\pi}e^{-(X_i-\\mu)^2/2\\sigma^2}\\) We can think of the model in this way (mathematical approach): \\(f(X) = \\frac{1}{\\sigma\\sqrt2\\pi}e^{-(X_i-\\mu)^2/2\\sigma^2}\\) Where the parameters (the symbols ) represent the mean, \\(\\mu\\) and the standard deviation, \\(\\sigma\\). What are some of the general characteristics of this model? Can you describe its shape? What are the parameters of the model? These are the quantities we will estimate in the fitting process. What are the variables used in the model? These are the observations. Below, two distributions are plotted from the ‘Standard Normal Distribution’, in this formulation: \\({\\sigma=1}\\) and \\({\\mu=0}\\). The normal distribution is an example of a continuous univariate probability distribution with infinite support. By infinite support, we mean that we can calculate values of the probability density function for all outcomes between \\(-\\infty\\) and \\(+\\infty\\). For clarification, the density value on the y-axis is not the resulting probability of obtaining the sampled value. Well, how do we get the probability from a probability density function? We need to integrate the density function given the value of the parameters. So from our example distribution with mean = 0 and standard deviation = 1, we can find the probability that an observed value will bebetween 0 and 1 by finding the area shown in the image below. \\(\\int_0^1f(x;\\mu,\\sigma)dx = P(0 &lt; X &lt; 1)\\) We can read this as “the integral of the probability density function between 0 and 1 (on the left-hand side) is equal to the probability that the outcome of the random variable is between zero and 1 (on the right-hand side)”. We can cover all possible values if we evaluate the density from \\(-\\infty\\) to \\(+\\infty\\). Therefore the following has to be true for the function to be a probability density function: \\(\\int_{-\\infty}^{\\infty}f(x)dx = 1\\). One last thing here: The probability of the random variable being equal to a specific outcome is 0, because the integral over x values of x to x is equal to zero. The definition of the definite integral: \\(\\int_{a}^{b}f(x)dx = \\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{f(x_i)\\Delta{x}}\\), where \\(x_i = a + i\\Delta{x}\\) and \\(\\Delta{x} = \\frac{b-a}{n}\\). If \\(a - b = 0\\), then \\(\\Delta{x} = 0\\). So the integral is zero: \\(\\int_{a}^{b}f(x)dx = \\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{0}\\), \\(\\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{0} = \\underset{\\rm n \\rightarrow\\infty}{lim}{0}\\), \\(\\underset{\\rm n \\rightarrow\\infty}{lim}{0} = 0\\). 11.2 Z-scores Z-scores are a way to center and scale the observations to understand how many standard deviations each value is away from the mean. Z-score values are measures of the ‘distance’ in standard deviations of a single value. \\(Z_i = \\frac{X_i-\\mu}{\\sigma}\\) Standardizing a score with respect to the other scores in the group. Expresses a score in terms of how many standard deviations it is away from the mean. Converts a distribution to a z-score distribution. Z-scores have mean = 0 and standard deviation = 1. 11.2.1 Centering with the mean 11.2.2 Scaling by the standard deviation 11.3 Properties of Z-scores A Z-scores, measure that quantifies how many standard deviations an observation is away from the mean of a distribution. Because of the scaling and centering we can quickly (and easily) convert any distribution of normally distributed random variables into Z-scores using the standard normal distribution (\\(\\mu = 0\\) and \\(\\sigma = 1\\)). The quantile value of: 1.96 is the maximum 2.5% of the standard normal distribution. -1.96 is the minimum 2.5% of the standard normal distribution. Thus, 95% of z-scores lie between \\(-1.96 \\le x_i \\le 1.96\\). 99% of z-scores lie between \\(-2.58 \\le x_i \\le 2.58\\). 99.9% of them lie between \\(-3.29 \\le x_i \\le 3.29\\). Let’s look at a Z Table to reinforce our understanding (Table B2 in Zar 4th edition). 11.3.1 R Code ## Provide code in R (using base R functionality) to illustrate and show the calculations for ## the Z-score. ## ```R ## # Sample data ## my_data &lt;- c(10, 12, 15, 18, 20) ## ## # Mean and Standard deviation ## my_mean &lt;- mean(my_data) # calculate mean of my_data ## my_sd &lt;- sd(my_data) # calculate standard deviation of my_data ## ## # Calculate the Z-score for each data point ## my_z_scores &lt;- (my_data - my_mean) / my_sd # calculate z-score of each data point ## ## # Print the Z-scores ## print(my_z_scores) # print calculated Z-scores ## ``` 11.4 Areas under the Normal Curve for different quantile values 11.4.1 R Code ## Provide code in R (using base R functionality) to determine the integral (area under the ## curve) from normal distributions using the function pnorm. ## ```R ## # area under the standard normal curve from -Inf to x ## pnorm(x) ## ## # area under the standard normal curve from a to b ## pnorm(b) - pnorm(a) ## ## # To find the area under a normal curve with a given mean &#39;m&#39; and standard deviation &#39;s&#39;, we first standardize the values using &#39;z = (x - m) / s&#39;, then use the &#39;pnorm()&#39; function: ## ## # area under a normal curve with mean m and std dev s from -Inf to x ## pnorm(x, mean = m, sd = s) ## ## # area under a normal curve with mean m and std dev s from a to b ## pnorm(b, mean = m, sd = s) - pnorm(a, mean = m, sd = s) ## ``` ## ## The inline comments provide brief explanations of what each line of code does. 11.4.2 R Code ## Provide code in R (using base R functionality) to determine the quantile for a given area ## under the curve from a normal distribution using the qnorm function. ## ```R ## area &lt;- 0.95 # Define the desired area under the curve as 0.95 ## quantile &lt;- qnorm(area) # Calculate the quantile using qnorm function ## quantile # Print the result ## ``` ## ## In this code, we have first defined the desired area under the curve as `0.95`. We have then used the `qnorm()` function to calculate the corresponding quantile value and stored it in the `quantile` variable. Finally, we have printed the result using the `print()` function. ## ## Note that the `qnorm()` function returns the quantile value corresponding to the desired area in a standard normal distribution (i.e., with a mean of 0 and standard deviation of 1). If you want to find the quantile value for a normal distribution with a different mean and/or standard deviation, you can use the `qnorm()` function with appropriate arguments (see the documentation of `qnorm()` for details). 11.5 Checking for normality Because many statistical tests we will be considering in this course are useful only if data are normally distributed we will need a way to assess if this is true. Are the observations normally distributed? 11.5.1 Qualitative Measures 11.5.2 QQ plot The QQ plot (Quantile Quantile) plot is a scatter plot that compares two sets of data. Here we will compare the observations (real-world data) to a theoretical data set that we would expect to see if the data came from a normal distribution with the sample \\(\\bar{X}\\) and \\(s\\), the reference data. If the distribution of the data is the same, the result will be a straight line. Each data value of the data is plotted along this reference line using the scale parameter. So, the question is, do these data come from a population of normally distributed values. First, rank your data and assign a probability to each value of the original data (empirical probability). obs rank.obs probability theoretical.quantiles observed.quantiles -2.215 1 0.005 -2.576 -2.587 -1.989 2 0.015 -2.170 -2.336 -1.805 3 0.025 -1.960 -2.131 -1.524 4 0.035 -1.812 -1.818 -1.471 5 0.045 -1.695 -1.759 -1.377 6 0.055 -1.598 -1.654 We build on the rank order of the data points to calculate the corresponding probabilty values. \\(p = \\frac{rank-\\frac{1}{2}}{n}\\). We then determine the theoretical quantiles, the theoretical standard normal quantiles for the calculated probability values. The quantiles from the observed data are Z scores calculated from the observed data \\(X_i\\), \\(\\bar{X}\\), and \\(s\\). Now lets look at the simulated data drawn from a non-normal distribution: Question, do these data come from a normal distribution? 11.5.3 R Code ## Provide code in R (using base R functionality) to make a quantile-quantile plot to perform ## a qualitative test of nomality from a simulated set of values. ## ```R ## # Generate a simulated set of data ## set.seed(123) # Set the seed for reproducibility ## sim_data &lt;- rnorm(1000) # Generate 1000 normally distributed random numbers ## ## # Create a Q-Q plot of the simulated data ## qqnorm(sim_data) # Plot the simulated data on the Q-Q plot ## qqline(sim_data) # Add a reference line to the Q-Q plot ## ``` ## ## This code snippet generates a simulated set of data consisting of 1000 normally distributed random numbers using the `rnorm()` function. We set the seed to ensure that the same set of random numbers is generated every time the code is run. ## ## We then create a Q-Q plot of the simulated data using the `qqnorm()` function. The `qqline()` function is used to add a reference line to the plot. ## ## The Q-Q plot allows us to compare the simulated data to a normal distribution; if the points on the plot fall roughly along the reference line, then the simulated data is assumed to be normally distributed. "],["confidence-interval.html", "12 Confidence Interval 12.1 Example using Z-score 12.2 Confidence Intervals of the Mean from samples - using the t-distribution 12.3 Contrast standard deviation and standard error: 12.4 Calculate Confidence Interval", " 12 Confidence Interval Confidence intervals are statements about probability. For example, a confidence interval of a parameter with a 95% confidence level means that you are confident that 95 out of 100 times the estimate of the parameter will fall between the upper and lower values specified by the confidence interval. How do we determine confidence intervals? The first step is to determine a confidence level. The confidence level is 1 - \\(\\alpha\\). We will never know the magnitude of the true error of our esimated value. Instead, what we can estimate is the combined effect of sampling error and process error (the precision of \\(\\mu\\)). so, we attach a probability that the error is a certain size. Now, we want to solve for the statistic. \\(\\bar{X} = \\mu \\pm Error\\) Thought experiment: If we take an infinite number of samples from the population, we will get an estimate of the error term, \\(\\sigma\\). We cannot take an infinite number of samples (for obvious reasons). So, we need to estimate the value of the error that occurs from estimating the mean. This is called the standard error of the mean \\(S_{\\overline{X}}\\). 12.1 Example using Z-score Let’s first assume we know the population level parameters, \\(\\mu\\) and \\(\\sigma\\). We can use the Z-score. What is the Z value (bounds of negative and positive) in which 95% of the values are under the curve? Let’s look at a Z Table \\(\\mu = \\bar{X} \\pm 1.96\\sigma\\) (95% Certainty) \\(\\mu = \\bar{X} \\pm ? \\sigma\\) (99% Certainty) The above presumes that we have knowledge of the population parameters. We do not sample populations and infinite number of times. 12.2 Confidence Intervals of the Mean from samples - using the t-distribution In this example we will use the t-distribution. The t-distribution, also known as Student’s t-distribution, is a probability distribution that is widely used in statistical inference and hypothesis testing when the sample size is small or when the population standard deviation is unknown. Here are the key characteristics of the t-distribution: Shape: The t-distribution is symmetric and bell-shaped, similar to the normal distribution. However, it has heavier tails, which means it has more probability in the tails compared to the normal distribution. Degrees of freedom: The shape of the t-distribution is determined by its degrees of freedom (df). The degrees of freedom represent the sample size minus one (df = n - 1), where n is the number of observations in the sample. As the degrees of freedom increase, the t-distribution approaches the shape of the standard normal distribution. When the sample size is large (typically considered as n &gt; 30), the t-distribution is nearly identical to the standard normal distribution. Mean and variance: The mean of the t-distribution is 0, just like the standard normal distribution. However, the variance of the t-distribution depends on the degrees of freedom. For a t-distribution with df degrees of freedom, the variance is df / (df - 2) if df &gt; 2. If the degrees of freedom are less than or equal to 2, the variance is undefined. Tails: The t-distribution has thicker tails compared to the normal distribution. This means that extreme values are more likely to occur in the tails of the distribution, making it more robust to outliers and deviations from normality. Overall, the t-distribution is a valuable statistical tool when dealing with small sample sizes or uncertain population parameters, providing a way to make inferences and draw conclusions from limited data. Let’s look at a t Table ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. To determine the sampling interval from populations with unknown \\(\\sigma\\) we will use our best estimate of \\(\\sigma\\), which is \\(s\\) (sd). \\(SE = \\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) 12.3 Contrast standard deviation and standard error: Standard error and standard deviation are two related but distinct measures used in statistics. Standard Deviation: Standard deviation (SD) is a measure of the dispersion or variability of a set of data points. It quantifies how spread out the values are from the mean. A larger standard deviation indicates greater variability, while a smaller standard deviation indicates less variability. It is calculated as the square root of the variance. Standard Error: Standard error (SE) is a measure of the variability of a sample statistic. It quantifies how much the sample statistic (e.g., mean, proportion, regression coefficient) varies from sample to sample. The standard error provides an estimate of the precision or accuracy of the sample statistic as an estimate of the population parameter. In other words, it represents the average amount that the sample statistic differs from the true population parameter. The key difference between standard deviation and standard error lies in the populations they describe: Standard deviation characterizes the variability within a single sample or the entire population. It tells us how the individual data points are dispersed around the mean. For example, if you have a sample of test scores, the standard deviation would indicate how much the scores deviate from the mean score. Standard error, on the other hand, pertains to the sampling distribution of a statistic. It reflects the variability in the estimates of a parameter across different samples. For example, if you take multiple random samples from a population and calculate the mean of each sample, the standard error would measure the spread or variability of those sample means. In summary, the standard deviation quantifies the variability within a dataset, while the standard error quantifies the variability of a sample statistic (such as the mean) across different samples. The standard error provides information about the precision or reliability of the sample statistic as an estimate of the population parameter. 12.4 Calculate Confidence Interval Here, because we have incomplete knowledge of the population, we will use the t distribution to model the population variability. The distribution is flatter (has larger tails). What does this mean to our estimate of the confidence interval? To model the confidence interval we assume that the univariate distribution is t distributed. \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (95% Certainty, with n = 10?) \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (99% Certainty, with n = 10?) \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (99% Certainty, with n = 50?) Here is some code to document the calculation. # Sample data data &lt;- c(5.1, 4.9, 4.7, 4.8, 5.0, 4.9, 5.1, 4.8, 5.0, 4.9) # Confidence level confidence_level &lt;- 0.95 # Sample mean sample_mean &lt;- mean(data) # Sample standard deviation sample_sd &lt;- sd(data) # Sample size sample_size &lt;- length(data) # Degrees of freedom df &lt;- sample_size - 1 # Critical value for the t-distribution critical_value &lt;- qt((1 - confidence_level) / 2, df) # Margin of error margin_of_error &lt;- critical_value * (sample_sd / sqrt(sample_size)) # Confidence interval confidence_interval &lt;- c(sample_mean - margin_of_error, sample_mean + margin_of_error) # Print the results cat(&quot;Sample Mean:&quot;, sample_mean, &quot;\\n&quot;) cat(&quot;Sample Standard Deviation:&quot;, sample_sd, &quot;\\n&quot;) cat(&quot;Sample Size:&quot;, sample_size, &quot;\\n&quot;) cat(&quot;Confidence Level:&quot;, confidence_level, &quot;\\n&quot;) cat(&quot;Degrees of Freedom:&quot;, df, &quot;\\n&quot;) cat(&quot;Critical Value:&quot;, critical_value, &quot;\\n&quot;) cat(&quot;Margin of Error:&quot;, margin_of_error, &quot;\\n&quot;) cat(&quot;Confidence Interval:&quot;, confidence_interval, &quot;\\n&quot;) "],["null-hypothesis-significance-testing-nhst.html", "13 Null Hypothesis Significance Testing (NHST) 13.1 State testable hypothesis 13.2 Example of testing significance using Z-scores. 13.3 Example of testing significance to detect normality.", " 13 Null Hypothesis Significance Testing (NHST) Zar 6.4 NHST is a method of statistical inference by which an experimental factor is tested against a hypothesis of no effect (the null hypothesis) or no relationship based on a given observation. The first step in doing any statistical test is to state a testable hypothesis: \\(H_0\\) Null hypothesis \\(H_A\\) Alternative hypothesis Declare \\(\\alpha\\) level (this determines your quantile value) This is the level of significance that is a reference point to determine whether the results that you have is different from the null-hypothesis of no effect. Fisher recommended using \\(\\alpha = 0.05\\) to judge whether an effect is significant or not. If you recall, this alpha level is roughly two standard deviations away from the mean for the normal distribution. From Fisher: ‘The value for which p = 0.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not’. How small the level of significance is, is left to researchers (domain knowledge). The experimental procedure: Collect Data State falsifiable null hypothesis Conduct statistical test Compare the test statistic to the critical value (determined by \\(\\alpha\\)). This provides some measure of objectivity. State the resulting probability (the p-value), less than or greater than \\(\\alpha\\). The p-value is the probability of obtaining test results at least as extreme as the results actually observed if the null hypothesis is true. 13.1 State testable hypothesis These are a set of mutually exclusive and exhaustive outcomes The test statistic will support one or the other outcomes \\(H_0: \\mu = 0\\), \\(H_A:\\mu \\ne 0\\) \\(H_0: \\mu = 3.5 cm\\), \\(H_A:\\mu \\ne 3.5 cm\\) \\(H_0: \\mu = 10.5 kg\\), \\(H_A:\\mu \\ne 10.5 kg\\) 13.2 Example of testing significance using Z-scores. Let’s examine a simple statistical test. We will test the hypothesis: Is the mean fuel consumption of a population of buses equal to 20 mpg? What is the null hypothesis? We need information about the population (remember we are using Z-score so we know the population-level parameters \\(\\mu\\) and \\(\\sigma\\)). Determine the associated probability that the mean is 20 mpg given: \\(\\sigma\\) = 0.3, \\(\\mu\\) = 19.1 \\(Z_{calc} = \\frac{\\overline{X} - \\mu}{\\sigma}\\) What is the probability that we would get \\(Z_{calc}\\)? Let’s compare this to the test value of \\(Z\\), \\(Z_{test}\\). \\(Z_{test}\\) is the quantile of \\(Z\\) that results from a given \\(\\alpha\\). Given our declared \\(\\alpha\\), how does the resulting probability compare? Remember, \\(\\alpha\\) is defined prior to statistical testing If our \\(Z_{calc}\\) &gt; \\(Z_{test}\\), then we are in a ‘region of rejection’, i.e. rejection of the null hypothesis. Now we have a way to objectively reject or accept the null hypothesis. 13.2.1 R Code ## Provide code in R (using base R functionality) to determine how many standard deviations ## away from the mean a given data point is and if this is statistically significant. ## ``` ## # create data vector ## data &lt;- c(5, 7, 9, 11, 13, 15) ## ## # calculate mean and standard deviation of data ## mean_data &lt;- mean(data) # calculates the mean of the data vector ## sd_data &lt;- sd(data) # calculates the standard deviation of the data vector ## ## # calculate the number of standard deviations a given data point is away from the mean ## given_data_point &lt;- 12 # defines the value to calculate the z-score for ## num_sd_away &lt;- (given_data_point - mean_data) / sd_data # calculates the z-score for the given data point ## ## # determine if the number of standard deviations is statistically significant ## if (abs(num_sd_away) &gt;= 1.96) { # checks if the absolute value of z-score is greater than or equal to 1.96 ## cat(&#39;The given data point is statistically significant.\\n&#39;) # if true, prints that the given data point is statistically significant ## } else { ## cat(&#39;The given data point is not statistically significant.\\n&#39;) # if false, prints that the given data point is not statistically significant ## } ## ``` 13.3 Example of testing significance to detect normality. There are a variety of quantitative ways to assess goodness of fit (GOF) of normality. These are statistical tests and we will evaulate them using models and data. 13.3.1 Kolmogorov-Smirnoff Test This is a commonly used statistical test to see if your data is normally distributed. The Kolmogorov-Smirnov Goodness of Fit Test (K-S test) compares the distribution of observations (your univariate data) with those from a specified candidate distribution to understand if they come from the same distribution (in this case normal). We will test to see if our observations (data) are close to the expectations (those values we would expect if the data came from a normal distribution). In this test (and all statistical tests) we will determine if the difference in observed and expected values indicate that the distribution is similar or different. If the difference between observation and expectation is small, then we would likely say that the data are distributed normally. If the difference in observed and expected values is large then we say that the data must come from some other distribution, and the data are not normally distributed. The hypotheses for the test are: Null hypothesis (\\(H_0\\)): the data comes from the specified distribution. Alternate Hypothesis (\\(H_A\\)): at least one value does not match the specified distribution. \\(H_0: P = P_0\\) \\(H_A: P \\ne P_0\\) Here P is the sample density and \\(P_0\\) is a specified distribution. So, we need to evaluate two distributions, the one resulting from our sampling (s, sampled cumulative distribution \\(F_S(x)\\) and the ones we would expect from a theoretical cumulative distribution, \\(F_T(x)\\). The difference between the two distribution functions is evaluated by the test statistic D, the greatest vertical distance between \\(F_S(x)\\) and \\(F_T(x)\\). \\(D = \\underset{\\rm x}{sup}|F_S(x) - F_T(x)|\\), where \\(\\underset{\\rm x}{sup}\\) is the supremum of the set of distances. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all x values. The K-S test statistic measures the largest distance between the EDF \\(F_{data}\\) and the theoretical function \\(F_{data}\\), measured in a vertical direction (Kolmogorov as cited in Stephens 1992). The question is whether these data are normally distributed: First we will create a CDF (Cumulative Distribution Function) for the sample data (n = 30 observations), \\(F_S(x)\\)), Observations Freq. (Relative Freq) Cumulative Frequency, F_S(x) -2.2 1 (1/30) 0.033 (1/30) -2 1 (1/30) 0.067 (2/30) -1.5 1 (1/30) 0.1 (3/30) -0.8 2 (2/30) 0.167 (5/30) Now, we find the associated Z score for each value of the empirically derived Cumulative Frequency (\\(F_S(x)\\)): Observations Frequency F_S(x) Z.score.obs -2.2 1 0.033 -2.205 -2.0 1 0.067 -2.007 -1.5 1 0.100 -1.511 -0.8 2 0.167 -0.816 Now, we find the associated probability for each Z score from the empirical cumulative distribution, this is \\(F_T(x)\\). We get these from the Z table in Zar. P(Z = -2.205)? Observations Frequency F_S(x) Z.score.obs F_T(X) -2.2 1 0.033 -2.205 0.014 -2.0 1 0.067 -2.007 0.022 -1.5 1 0.100 -1.511 0.064 -0.8 2 0.167 -0.816 0.203 Finally, we will calculate the test statistic D. This if you recall is the maximum difference in the absolute value of \\(F_S(x) and F_T(x)\\). Remember: \\(D = \\underset{\\rm x}{sup}|F_S(x) - F_T(x)|\\). Observations Frequency F_S(x) Z.score.obs F_T(X) D -2.2 1 0.033 -2.205 0.014 0.019 -2.0 1 0.067 -2.007 0.022 0.045 -1.5 1 0.100 -1.511 0.064 0.036 -0.8 2 0.167 -0.816 0.203 0.036 -0.6 2 0.233 -0.618 0.263 0.030 -0.5 1 0.267 -0.519 0.295 0.028 -0.3 1 0.300 -0.320 0.367 0.067 -0.2 1 0.333 -0.221 0.404 0.071 -0.1 1 0.367 -0.122 0.443 0.076 0.0 2 0.433 -0.023 0.482 0.049 0.1 1 0.467 0.077 0.521 0.054 0.2 1 0.500 0.176 0.560 0.060 0.3 1 0.533 0.275 0.599 0.066 0.4 2 0.600 0.374 0.636 0.036 0.5 1 0.633 0.473 0.672 0.039 0.6 3 0.733 0.573 0.707 0.026 0.7 1 0.767 0.672 0.740 0.027 0.8 2 0.833 0.771 0.771 0.062 0.9 2 0.900 0.870 0.800 0.100 1.1 1 0.933 1.069 0.850 0.083 1.5 1 0.967 1.466 0.924 0.043 1.6 1 1.000 1.565 0.937 0.063 The open question then, how do we use the calculated \\(D\\) value, \\(D = 0.100\\) for evaluating our \\(n = 30\\) samples? Use the lookup table in Zar 4th edition, Appendix Table B.9 or Critical Values of D This will give us the ‘critical value’ for the test. The test value is \\(D = 0.100\\), the critical value is \\(\\alpha = 0.05\\) and \\(n = 30\\) is 0.24170. Remember, we are testing the Null hypothesis (\\(H_0\\)) that the data comes from the specified distribution. If our test value of \\(D\\) exceed the critical value we would reject the null hypothesis. However, it does not. \\(D = 0.100\\) \\(0.100 &lt; D_{critical, n = 30, \\alpha = 0.05}\\). 13.3.2 R Code ## Provide code in R (using base R functionality) to calculate the Kolmogorov-Smirnoff test ## of nomality from a simulated set of values. ## ``` ## # Generate a simulated set of values ## x &lt;- rnorm(100, mean = 5, sd = 2) # Generate 100 random numbers from a normal distribution with mean 5 and standard deviation 2. Store the values in x. ## ## # Perform the Kolmogorov-Smirnov test for normality ## ks.test(x, &#39;pnorm&#39;, mean(x), sd(x)) # Use the ks.test function to perform the Kolmogorov-Smirnov test for normality. The comparison cumulative distribution function used is pnorm, since we are testing for normality. The mean and standard deviation of x are passed as arguments to the function. ## ``` 13.3.3 Shapiro Wilk test This is one we will use extensively. The Shapiro-Wilk test is a statistical test used to assess the normality of a dataset. It was developed by Samuel Sanford Shapiro and Martin Wilk in 1965. The test determines whether a given sample comes from a population that follows a normal distribution. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. The alternative hypothesis is that the data does not follow a normal distribution. The test calculates a test statistic, W, which is based on the observed and expected values of the ordered sample values. The test statistic is then compared to critical values from the Shapiro-Wilk table or determined using software. 13.3.4 R Code ## Provide code in R (using base R functionality) to test that a set of data is normally ## distributed using the Shapiro-Wilk test ## ``` R ## # Generate some sample data ## data &lt;- rnorm(100) # Using the rnorm() function to generate a random sample of 100 normally distributed data points and assigning it to a variable named &quot;data&quot; ## ## # Test normality using the Shapiro-Wilk test ## shapiro.test(data) # Using the shapiro.test() function to perform the Shapiro-Wilk test on the data to test the null hypothesis that the data is normally distributed ## ``` "],["one--and-two-tailed-tests.html", "14 One- and Two-Tailed Tests", " 14 One- and Two-Tailed Tests In a two-tailed test, the alternative hypothesis is typically expressed as “not equal to” or “different from” the null hypothesis. It allows for the possibility of the observed data being significantly greater or significantly smaller than what would be expected under the null hypothesis. Two-tailed tests are often used when the research question or hypothesis is not specific about the direction of the effect or when there is a genuine interest in detecting differences in both directions. In some cases we care about the direction of the difference (is the value less than or greater than some value). In statistical hypothesis testing (NHST), a one-tailed test refers to a type of hypothesis test that focuses on the possibility of a significant difference or relationship in only one direction, either greater than or less than the null hypothesis. Use one-tailed test In general, one-tailed hypotheses about a mean are: \\(H_0:\\mu\\ge\\mu_0\\) and \\(H_A:\\mu&lt;\\mu_0\\) In which case, H0 is rejected if the test statistic is in the left-hand tail of the distribution or: \\(H_0:\\mu\\le\\mu_0\\) and \\(H_A:\\mu&gt;\\mu_0\\) Contrast the region of rejection for these. "],["statistical-power.html", "15 Statistical Power 15.1 Type-1 and Type-2 Errors 15.2 Type 1 (\\(\\alpha\\)) Error 15.3 Type 2 (\\(\\beta\\)) Error 15.4 Tables of Error Types 15.5 What Influences Statistical Power?", " 15 Statistical Power Statistical power is: The probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result. Zar defines statistical power as: “The probability that the statistical test will correctly reject the null hypothesis”. The power of a hypothesis test is between 0 and 1; If the power is close to 1, the hypothesis test is very good at detecting a false null hypothesis. 15.1 Type-1 and Type-2 Errors Sometimes we: Reject the null hypothesis when it is true. Accept the alternative hypothesis when it is false. These are error that we have made and occure even when following the correct statistical procedure and doing all of the calculations correctly. How do these errors arise? 15.2 Type 1 (\\(\\alpha\\)) Error Type 1 error or alpha error - probability of rejecting \\(H_0\\) when it is true. Type 1 error rate is equal to \\(\\alpha\\). Type 1 error: “rejecting the null hypothesis when it is true.” We rejected the null hypothesis but did so erroneously.The samples available to us indicated that a difference existed when there was in fact no difference and the null hypothesis should have been accepted. Type 1 error is termed ‘\\(\\alpha\\) error’ because it is equal to \\(\\alpha\\) Now we have some criteria to choose alpha. So if your \\(\\alpha\\), or critical value is 0.10 we have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it. 15.3 Type 2 (\\(\\beta\\)) Error Type 2 error: “accepting the null hypothesis when it is false.” Type 2 error or ‘\\(\\beta\\) error’ is equal to \\(\\beta\\). 15.4 Tables of Error Types If H0 is true If H0 is false If H0 is rejected Type I error No error If H0 is not rejected No error Type II error Two Types of Errors in Hypothesis Testing If H0 is true If H0 is false If H0 is rejected \\(\\alpha\\) \\(1-\\beta\\) (“power”) No error If H0 is not rejected No error \\(1-\\alpha\\) \\(\\beta\\) Long-term Probabilities of Outcomes in Hypothesis Testing Beta is commonly set at 0.2, but may be set by the researchers to be smaller. Consequently, power may be as low as 0.8, but may be higher. Powers lower than 0.8, while not impossible, would typically be considered too low for most areas of research. 15.5 What Influences Statistical Power? These are listed in Zar, we can take them step by step to learn why power is influenced. Significance level (or alpha) Sample Size - Power depends on sample size. Other things being equal, larger sample size yields higher power. Variance - Power also depends on variance: smaller variance yields higher power. Experimental Design - Power can sometimes be increased by adopting a different experimental design that has lower error variance. For example, stratified sampling can reduce error variance and hence increase power. Magnitude of the effect of the variable. "],["bivariate-relationships.html", "16 Bivariate Relationships 16.1 Direction of Relationships 16.2 Limitations of Covariance 16.3 The Correlation Coefficient 16.4 Non-parametric Correlation", " 16 Bivariate Relationships We often have information on two numeric characteristics for each member of a group and are interested in finding the degree of association between these characteristics. For instance, an obstetrician may decide to look up the records of women who delivered in her hospital in the previous year to find out whether there is a relationship between their family incomes and the birth weights of their babies. The relationship here means whether the two variables fluctuate together, i.e., does thebirth weight increase (or decrease) as the income increases. Parametric approaches Pearson’s correlation coefficient Nonparametric approaches Spearman’s rho Kendall’s tau 16.1 Direction of Relationships 16.1.1 Positive Relationship 16.1.2 Negative Relationship 16.1.3 No Relationship 16.1.4 Quantifying the magnitude of correlation As one variable increases, does the other increase, decrease or not change at all (stay the same)? This can be done by understood by calculating the covariance. We look at how much each score deviates from their respective mean values. If both variables deviate from the mean in a similar way, they are likely to be related (or ‘covary’). Here is the results (bivariate) of an experiment aimed at understanding the efficacy of advertising: Participant number (i) 1 2 3 4 5 Mean SD Adverts Watched (X) 5 4 4 6 8 5.4 1.67 Packets Bought (Y) 8 9 10 13 15 11 2.92 Residual error values: Remember residuals are the value of the observation - expectation. Participant number (i) 1 2 3 4 5 Mean SD Adverts Watched 5 4 4 6 8 5.4 1.67 Packets Bought 8 9 10 13 15 11 2.92 Advertiser Residual -0.4 -1.4 -1.4 0.6 2.6 Packets residual -3 -2 -1 2 4 16.1.5 Covariance and a re-examination of variance Remember the variance tells us how much scores deviate from the mean for a single variable. It is closely linked to the sum of squares, indeed we use the sum of squares to calculate the variance. Covariance is similar - it tells is by how much scores on two variables differ from their respective means. \\(s^2=\\frac{\\Sigma(X_i - \\bar{X})}{n-1}^2\\) \\(s^2=\\frac{\\Sigma(X_i - \\bar{X})(X_i - \\bar{X})}{n-1}\\) 16.1.6 Here we are examining the ‘covariance’ so the calculation changes a bit. Calculate the error (residual value) between the mean and each subject’s score for the first variable (X). Calculate the error (residual value) between the mean and their score for the second variable (y). Multiply the error values (the residual values). Add these values and you get the cross product deviations. The covariance is the mean (average) of the cross-product deviations: \\(cov(X,Y)=\\frac{\\Sigma(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\\) From our example, and plugging in the values: \\(cov(X,Y)=\\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\\) \\(cov(X,Y)=\\frac{1.2+2.8+1.4+1.2+10.4}{4}\\) \\(cov(X,Y)=\\frac{17}{4}\\) \\(cov(X,Y)=4.25\\) 16.2 Limitations of Covariance The magnitude of the covariance is dependent on the units of measurement. e.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is changed… To address this issue we can standarize the covariance value by standardization: Divide by the standard deviations of both variables. The standardized version of covariance is known as the correlation coefficient. It is unaffected by units of measurement. 16.3 The Correlation Coefficient \\(r=\\frac{cov_{X,Y}}{s_Xs_Y}\\) \\(r=\\frac{\\Sigma(X_i - \\bar{X})(Y_i - \\bar{Y})}{(n-1)s_Xs_Y}\\) \\(r=\\frac{cov_{XY}}{s_Xs_Y}\\) \\(r=\\frac{4.25}{1.67 * 2.92}\\) \\(r=0.87\\) Termed Pearson-product moment correlation coefficient It ranges between -1 and +1 A value of zero, indicates that there is no relationship It is a testable hypothesis Testing \\(H_0: \\rho=0\\) versus \\(H_A: \\rho\\ne0\\) The standard error of the correlation coefficient is calculated as: \\(S_r=\\sqrt\\frac{1-r^2}{n-2}\\) It is a testable hypothesis r = 0.870 n = 12 (new data set, with more samples) We will calculate the critical value: \\(t=\\frac{r}{S_r}= \\frac{0.870}{0.156}= 5.58\\) t0.05(2),10 =2.228 Testing \\(H_0: \\rho=0\\) versus \\(H_A: \\rho\\ne0\\) Coefficient of determination, r^2 By squaring the value of r you get the proportion of variance in one variable shared by the other. Square of correlation coefficient (\\(r^2\\)), known as coefficient of determination, represents the proportion of variation in one variable that is accounted for by the variation in the other variable. For example, if height and weight of a group of persons have a correlation coefficient of (\\(\\rho = 0.80\\)), one can estimate that 64% (0.80 × 0.80 = 0.64) of variation in their weights is accounted for by the variation in their heights. 16.4 Non-parametric Correlation The Spearman’s Rank Correlation Coefficient is used to discover the strength of a link between two sets of data. This worked example is taken from: https://geographyfieldwork.com/SpearmansRank.htm. I found it to be a very elegant and real world example. There is also the case of tied ranks in this example, which is useful. Hypothesis: We might expect to find that the price of a bottle of water decreases as distance from the Contemporary Art Museum (tourist area) increases. Higher property rents close to the museum should be reflected in higher prices in the shops. The hypothesis might be written like this: The price of a convenience item decreases as distance from the Contemporary Art Museum increases. Note in the column “Rank Price” there is no rank 3 or 4. In this case of the tied ranks Store Distance (m) Rank distance Price Rank price Difference between ranks (d) d x d 1 50 10 1.80 2.0 8.0 64.00 2 175 9 1.20 3.5 5.5 30.25 3 270 8 2.00 1.0 7.0 49.00 4 375 7 1.00 6.0 1.0 1.00 5 425 6 1.00 6.0 0.0 0.00 6 580 5 1.20 3.5 1.5 2.25 7 710 4 0.80 9.0 -5.0 25.00 8 790 3 0.60 10.0 -7.0 49.00 9 890 2 1.00 6.0 -4.0 16.00 10 980 1 0.85 8.0 -7.0 49.00 Next, we will calculate the coefficient (\\(R_s\\)) using the formula below. The answer will always be between 1.0 (a perfect positive correlation) and -1.0 (a perfect negative correlation). \\(R_s = 1 - \\frac{6\\Sigma{d^2}}{n^3-n}\\). Now to put all these values into the formula. Find the value of all the \\(d^2\\) values by adding up all the values in the difference² column. In our example this is 285.5. Multiplying this by 6 gives 1713. Now for the bottom line of the equation. The value n is the number of sites at which you took measurements. This, in our example is 10. Substituting these values into n³ - n we get 1000 - 10 We now have the formula: \\(R_s = 1 - \\frac{1713}{990}\\), which gives a value for R: 1 - 1.73 = -0.73 The closer (\\(R_s\\)) is to +1 or -1, the stronger the likely correlation. A perfect positive correlation is +1 and a perfect negative correlation is -1. The Rs value of -0.73 suggests a fairly strong negative relationship. Now, let’s determine if this calculated value is signifcantly different from zero. We will use the Student’s t-distribution. Similar to Pearson’s coefficient, the t-value is found by: \\(t = R_s\\sqrt{\\frac{n - 2}{1 - {R_s}^2}}\\) \\(t_{calc} = -0.73\\sqrt{\\frac{8}{1 - ({-0.73})^2}}\\) \\(t_{critical} = t_{\\alpha = 0.05, df = 9} = 2.262\\), this is Table B.3 in Zar 4th edition (Critical Values of the t distribution). So, we would reject the null hypothesis, "],["linear-regression.html", "17 Linear Regression 17.1 What is Regression? 17.2 Assumptions of Simple Linear Regression 17.3 Model for Linear Relationship is a two-parameter model 17.4 Intercepts and Gradients 17.5 The Method of Least Squares 17.6 Sum of Squares Components 17.7 Total SS (SST, SST) 17.8 Residual SS or Error SS (SSR) 17.9 Model SS or Regression SS (SSM) 17.10 The SS components are related and useful for understanding the model 17.11 Variance Ratio Test 17.12 Test \\(\\beta_1 = 0\\) 17.13 Worked Example 17.14 Regression: An Example 17.15 Output of a Simple Regression 17.16 Using the Model 17.17 Analysis of the slope coeffient 17.18 An R example 17.19 Prediction and confidence intervals 17.20 An R example - Confidence Intervals 17.21 An R example - Prediction Intervals", " 17 Linear Regression Analysis with a one independent (X) and dependent variable (Y) is termed “Simple” linear regression. 17.1 What is Regression? A way of predicting the value of one variable from another. It is a model of the relationship between two variables. The modeled relationship is linear. Therefore, we describe the relationship using the equation for a straight line. To do the fitting and the hypothesis testing, we will evaluate the following quantities: Total sum of squares (TSS or \\(SS_T\\)) Model sum of squares (Regression SS, \\(SS_M\\)) Residual sum of squares (RSS or \\(SS_R\\)) We will use these quantities to understand two things: Is \\(\\beta_1 = 0\\) (using the confidence intervals)? We would like to know if X is a good predictor of Y and this is determined by evaluating the null hypothesis, \\(\\beta_1 = 0\\). Is the model significant (using an F-test)? Specifically does the fit of model fit the data better than does the null model (more on this below). 17.2 Assumptions of Simple Linear Regression For each value of X, Y are randomly sampled and independent. For any value of X in the population there exists a normal distribution of Y values There is homogeneity of variance in the population. i.e. the variance of the normal distribution of Y values in population are equal for all of values of X. X is measured without error The exptected value of Y at the \\(i^{th}\\) value of X is \\(E(Y_i)=\\beta_0 + \\beta_1X_i\\) 17.3 Model for Linear Relationship is a two-parameter model The first parameter is \\(\\beta_1\\) Gradient (slope) of the regression line. This indicates the direction of the relationship. The second parameter is \\(\\beta_0\\) Intercept (value of Y when X = 0). Location where the regression line crosses the Y-axis (ordinate). 17.4 Intercepts and Gradients 17.5 The Method of Least Squares This is the approach we will use for parameter estimation and inference in this course. This figure shows a scatterplot of some data with a line representing the best fit model. x &lt;- c(2,3,4,6,7,8,10,11,14,15,17,18,20,21,23) y &lt;- c(5,10,7,11,20,13,15,30,27,37,35,30,32,35,40) plot(x, y, xlim = c(0,25), ylim = c(0,45), pch = 1, xlab = &quot;Size of Spider&quot;, ylab = &quot;Anxiety&quot;) segments(x0 = 0, y0 = 3, x1 = 23, y1 = 41) This figure shows the residual (observed - expected) value of the best fit model. Our task is to minimize the \\(SS\\) value. Let’s look at a some cases for alternative candidate model parameters: Our task is to minimize the \\(SS\\) value. 17.6 Sum of Squares Components Once the best fit model is derived, we can make inference about the quality of the model (relative to the null model) and the hypothesis \\(\\beta_1 = 0\\). Let’s take a close look at the three sum of squares components. 17.7 Total SS (SST, SST) Total variability (variability between scores and the mean) in the observations. TSS is the sum of the squared residuals when the most basic model is applied to the data. How good is the mean as a model to the observed data? Lets consider the mean of Y to be the null model: SST uses the differences between the observed data and the mean value of Y (the null model) \\(TSS =\\sum(Y_i - \\bar{Y})^2\\) 17.8 Residual SS or Error SS (SSR) SSR is the error that is derived between the regression model and the actual data). Difference between the observation and the model Represents the degree of inaccuracy when fitting the model to the data. Residual/error variability (variability between the regression model and the actual data). \\(RSS =\\sum(Y_i - \\hat{Y})^2\\) 17.9 Model SS or Regression SS (SSM) SSM is the difference between the prediction from the regression model and the mean (null model). This is the improvement we get from fitting the model to the data relative to the null model. \\(SS_M=\\sum(\\hat{Y}_i - \\bar{Y})^2\\) 17.10 The SS components are related and useful for understanding the model The total variabilty is partitioned between the model and the residual: \\(SS_T = SS_M + SS_R\\) We calculate the sum of squares components to evaluate the amount of systematic variance (regression/model) and the amount of unsystematic (residual) variance. If we have a good model, the total variance in the observations will be described to a great extent by the model. The proportion of improvement due to the model. \\(R^2 = SS_M/SS_T\\), percentage of variation explained by the model. 17.11 Variance Ratio Test We will evalute the model using an F test - “termed variance ratio test”. We will be evaluating the ratio of SSM and SSR. We will use the SSM and SSR to calculate quantities “mean squares” This will be done by dividing the SSM and SSR by their respective degrees of freedom (df). These terms can be expressed as averages, divided by df terms. df for SSM (The number of parameters in the model - 1) df for SSR (number of obs - number of parameters in the model) These quantities are called mean squares. Mean Squares Model; MSM = SSM/dfM Mean Squares Residual: MSR = SSR/dfR F-test is termed the “variance ratio test” \\(F=\\frac{MS_M}{MS_R}\\) 17.12 Test \\(\\beta_1 = 0\\) To test whether \\(\\beta_1 = 0\\) we will use the t-distribution. After determining the model parameters values we can estimate the standard error of the slope paramter. The approach is to determine the confidence interval of the slope (\\(\\beta_1\\)) for a given value of \\(\\alpha\\). Look at the following equation, what values will we need? \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{(\\sum{{Y_i - \\hat{Y_i}}})^2}{df}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{(\\sum{{Y_i - \\hat{Y_i}}})^2}{(n - 2)}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) Degrees of freedom: For simple linear regression (one independent and one dependent variable), the degrees of freedom (df) is equal to, n - 2. Our calculated test statistic is: \\(t = \\frac{\\beta_1}{SE_{\\beta_1}}\\) We will compare this to the critical value: \\(t_{\\alpha, 2-tailed, df}\\) 17.13 Worked Example Age (days) (X) Wing Length (cm) (Y) 3.0 1.4 4.0 1.5 5.0 2.2 6.0 2.4 8.0 3.1 9.0 3.2 10.0 3.2 11.0 3.9 12.0 4.1 14.0 4.7 15.0 4.5 16.0 5.2 17.0 5.0 We find that the TSS is = 19.65 We find that the Model SS = 19.13 Source of Variation Sum of Squares (SS) df Mean Squares (MS) Total \\(\\sum(Y_i-\\bar{Y})^2\\) Model (or Regression) \\(\\sum(\\hat{Y}_i-\\bar{Y})^2\\) The number of parameters in the model - 1 SSM/dfM Residual (or Error) \\(\\sum(Y_i-\\hat{Y_i})^2\\) The number of obs - the number of parameters in the model SSR/dfR 17.13.1 Summary of the Calculations for Model Fit We are falsifying the null hypothesis: \\(H_0\\): There is no statistically significant relationship between variables x and y. Source of Variance SS df MS Total 19.65 12 Model (or Regression) 19.13 1 19.13 Residual (or Error) 0.52 11 0.047 \\(F=\\frac{19.132214}{0.047701}=401.1\\) \\(F_{0.05(1),1.11} = 4.84\\) Therefore, reject H0 P &lt; 0.0005 17.14 Regression: An Example A record company executive is interested in predicting record sales from advertising. Data are: 200 different album releases Outcome variable: Sales (CDs and downloads) in the week after release Predictor variable: The amount (in units of $1,000) spent promoting the record before release. 17.15 Output of a Simple Regression In R: Coefficients: Estimate Std. Error t value Pr(&gt; (intercept) 134 7.53 17.799 &lt; \\(2\\times10^{-16}\\) *** Adverts 0.096 0.00963 9.979 &lt; \\(2\\times10^{-16}\\) *** Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 65.99 on 198 degrees of freedom Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 F-statistic: 99.59 on 1 and 198 df, p-value: &lt; 2.2e-16 17.16 Using the Model Expected Record Salesi = 134.14 + (0.096 x Advertising Budgeti) E(Record sales if Budget = 100) = 134.14 + (0.096 x 100) E(Record sales if Budget = 100) = 143.75 17.17 Analysis of the slope coeffient Using the fact that \\(\\beta_1\\) is approximately normally distributed in large samples, we can test the hypothesis that \\(\\beta_1 = 0\\). From our reading, we learned that the calculated value of the t-statistic is: \\(t = \\frac{\\beta_{observed} - {\\beta_{expected}}}{{SE_{\\beta}}}\\). For the hypothesis testing, we: Compute the standard error of \\(\\beta_1\\), \\(SE_{\\beta_1}\\): \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{\\sum{{Y_i - \\hat{Y_i}}})^2}{{n - 2}}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) Compute the t-statistic, in this case, we are comparing our observed value of \\(\\beta_1\\) to the expected value, if the null hypothesis is true. So, our expected value is \\(\\beta_1 = 0\\). And we divide this difference by \\(SE_{\\beta_1}\\). \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\). \\(t = \\frac{\\hat{\\beta_1}}{SE_{\\beta_1}}\\). This is our computed t value. We calculate the critical value of t, which is derived using the degrees of freedom. For simple linear regression (one independent and one dependent variable), the degrees of freedom (df) is equal to, n - 2. To determine the 95% CI of \\(\\beta_1\\), we can rearrange the above equation: \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\). \\(t SE_{\\beta_1} = \\hat{\\beta_1}\\). \\(\\hat{\\beta_1} = t_{\\alpha, df} SE_{\\beta_1}\\). If \\(\\alpha = 0.05\\), then we would get the 95% confidence interval of \\(\\beta_1\\). 17.18 An R example data(mtcars) plot(mtcars$mpg ~ mtcars$hp) model. &lt;- lm(mtcars$mpg ~ mtcars$hp) summary(model.)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886054 1.6339210 18.421246 6.642736e-18 ## mtcars$hp -0.06822828 0.0101193 -6.742389 1.787835e-07 The second column of the coefficients’ summary, reports \\(SE_{\\beta_1}\\) and \\(SE_{\\beta_0}\\). In the third column t value, we find the t statistics suitable for tests of the separate hypotheses: \\(\\beta_o = 0\\) and \\(\\beta_1 = 0\\). Furthermore, the output provides us with p-values corresponding to both tests against the two-sided hypothesis in the fourth column of the table. Given these results: \\(\\beta_1 = -0.06822828\\) \\(SE_{\\beta_1} = 0.0101193\\) \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\) \\(t = \\frac{ -0.068}{0.0101}\\). \\(t = -6.742\\). 95% CI of \\(\\beta_1\\): \\(t_{\\alpha=0.05, df = (n - k -1)}\\) \\(t_{\\alpha=0.05, df = (32 - 2 -1)}\\) \\(t_{\\alpha=0.05, df = (29)}\\) \\(t_{\\alpha=0.05, df = (29)} = -1.699\\) 95% CI of \\(\\beta_1\\) = \\(t_{\\alpha=0.05, df = 29}SE_{\\beta_1}\\) } \\(\\beta_1 = -0.068 \\pm 0.017\\) 17.19 Prediction and confidence intervals Sometimes (often) we will want to use the model to predict values. For example, suppose we fit a simple linear regression model using hours studied as a predictor variable and exam score as the response variable. Using this model, we might predict that a student who studies for 6 hours will receive an exam score of 91. However, because there is uncertainty around this prediction, we might create a prediction interval that says there is a 95% chance that a student who studies for 6 hours will receive an exam score between 85 and 97. This range of values is known as a 95% prediction interval and it’s often more useful to us than just knowing the predicted value. The predicted value \\(\\hat{Y_i}\\) is called a point and is useful. We would like to report the variability around this value.. Two type of intervals available: 1.) Confidence interval A confidence interval captures the uncertainty around the mean predicted values. 2.) Prediction interval The prediction interval predicts in what range a future individual observation will fall. A prediction interval will always be wider than a confidence interval for the same value. Confidence interval: \\(\\hat{y} = s_yt_{(\\alpha, df = n-2)}\\sqrt{\\frac{1}{n}+\\frac{(X^*-\\bar{X})^2}{(n-1)s^2_X}}\\) Here, \\(s_y= \\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}\\) \\(s_y\\) is called residual standard error in R regression output. Prediction intervals are similar in their calculation \\(\\hat{y} = s_yt_{(\\alpha, df = n-2)}\\sqrt{1+\\frac{1}{n}+\\frac{(X^*-\\bar{X})^2}{(n-1)s^2_X}}\\) Here, \\(s_y= \\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}\\) The formula is very similar, except the variability is higher since there is an added 1 in the formula. 17.20 An R example - Confidence Intervals data(mtcars) mt &lt;- mtcars[order(mtcars$hp),] plot(mt$mpg ~ mt$hp) model. &lt;- lm(mt$mpg ~ mt$hp) new_hp &lt;- data.frame(hp = mt$hp) pred.vals &lt;- predict(model., newdata = new_hp, interval = &quot;confidence&quot;) lines(cbind(new_hp, pred.vals)[,c(1,2)]) lines(cbind(new_hp, pred.vals)[,c(1,3)], col = &quot;red&quot;) lines(cbind(new_hp, pred.vals)[,c(1,4)], col = &quot;red&quot;) 17.21 An R example - Prediction Intervals data(mtcars) mt &lt;- mtcars[order(mtcars$hp),] plot(mt$mpg ~ mt$hp) model. &lt;- lm(mt$mpg ~ mt$hp) new_hp &lt;- data.frame(hp = mt$hp) pred.vals &lt;- predict(model., newdata = new_hp, interval = &quot;prediction&quot;) lines(cbind(new_hp, pred.vals)[,c(1,2)]) lines(cbind(new_hp, pred.vals)[,c(1,3)], col = &quot;red&quot;) lines(cbind(new_hp, pred.vals)[,c(1,4)], col = &quot;red&quot;) "],["multiple-linear-regression.html", "18 Multiple Linear Regression 18.1 Multiple Predictors 18.2 Album Sales Model 18.3 Multiple Predictors Model 18.4 Regression “Plane” 18.5 Multiple Linear Regression 18.6 Model Interpretation 18.7 Test of Null Hypothesis 18.8 Significance Testing 18.9 Multicollinearity 18.10 Determination of Predictors to Include? 18.11 Model Design Considerations 1", " 18 Multiple Linear Regression Model is fit by minimizing the sum of squared differences between the line and the actual data points - method of least squares. We still use our base equation: \\(outcome=model+error\\) But this time the model it is a bit more complicated. When we add predictors, we add parameters (\\(\\beta\\) terms). So each predictor variable has its own coefficient (parameter) and the outcome is predicted from a combination of all variables multiplied by their respective coefficients (\\(\\beta\\) terms). 18.1 Multiple Predictors Y is the outcome variable, \\(\\beta_1\\) is the coefficient of X1,\\(\\beta_n\\) is the coefficient of \\(X_n\\). We seek to find combinations of \\(\\beta\\) values (parameters) that minimize the sum of squares error: \\(outcome_i=model_i+error_i\\) \\(error_i = outcome_i - model_i\\) As you know, we quantify the error the model fitting using \\(SS\\). \\(Y_i=(b_0 + b_1X_{1i}+b_2X_{2i}+\\ldots+b_nX_{ni})+\\varepsilon_i\\) \\(\\varepsilon_i=Y_i-(b_0 + b_1X_{1i}+b_2X_{2i}+\\ldots+b_nX_{ni})\\) 18.2 Album Sales Model From our previous example above of record sales data, we know that advertising accounts for 33% of the variation in album sales. Remember: \\(R^2=\\frac{SS_M}{SS_T}\\) \\(R^2=0.3313\\) Therefore a large proportion of variation remains unexplained. Lets bring a new predictor variable into the mix to see if we can increase the amount of residual variation described by the model: How many times the song was played on the radio during the week prior to its release. 18.3 Multiple Predictors Model We will incorporate a new “airplay” variable. So we have a model with three parameters and two slope coefficients. Because there are two predictors, so we can view the model in two dimensions: \\(E(Album \\:Sales_i)=\\beta_0+\\beta_1X_{advertising \\: budget_i}+\\beta_2X_{airplay_i}\\) 18.4 Regression “Plane” 18.5 Multiple Linear Regression Cities Y X1 X2 X3 X4 X5 X6 Pheonix 10 70.3 213 582 6.0 7.05 36 Little Rock 13 61.0 91 132 8.2 48.52 100 San Francisco 12 56.7 453 716 8.7 20.66 67 Denver 17 51.9 454 515 9.0 12.95 86 Hartford 56 49.1 412 158 9.0 43.37 127 Wilmington 36 54.0 80 80 9.0 40.25 114 Washington 29 57.3 434 757 9.3 38.89 111 Jacksonville 14 68.4 136 529 8.8 54.57 116 Miami 10 75.5 207 335 9.0 59.80 128 Atlanta 24 61.5 368 497 9.1 48.34 115 Chicago 110 50.6 3344 3369 10.4 34.44 122 Indianapolis 28 52.3 361 746 9.7 38.74 121 Des Moines 17 49.0 104 201 11.2 30.85 103 Wichita 8 56.6 125 277 12.7 30.58 82 Louisville 30 55.6 291 593 8.3 43.11 123 New Orleans 9 68.3 204 361 8.4 56.77 113 Baltimore 47 55.0 625 905 9.6 41.31 111 Detroit 35 49.9 1064 1513 10.1 30.96 129 Minneapolis- St. Paul 29 43.5 699 744 10.6 25.94 137 Kansas City 14 54.5 381 507 10.0 37.00 99 St. Louis 56 55.9 775 622 9.5 35.89 105 Omaha 14 51.5 181 347 10.9 30.18 98 Alburquerque 11 56.8 46 244 8.9 7.77 58 Albany 46 47.6 44 116 8.8 33.36 135 Buffalo 11 47.1 391 463 12.4 36.11 166 Cincinnati 23 54.0 462 453 7.1 39.04 132 Table of air pollution in 41 U.S. cities associatied with six environmental variables 18.6 Model Interpretation We regress SO2 content in the air on average temperature X1 and the number of manufacturing enterprises, X2 \\(\\hat{Y} = 77.231 + 1.0480X_1 + 0.02431X_2\\) A one unit change in \\(X_2\\) results in a 0.02431 increase in Y. A one unit change in \\(X_1\\) results in a 1.048 increase in Y. 18.7 Test of Null Hypothesis We can analyze the null hypothesis that all of the regression coefficients are equal to zero using an ANOVA analysis. This approach is analogous to that used in simple linear regression. 18.8 Significance Testing In general, a significant F value will be associated with the rejection of the null hypothesis for some regression coefficients. 18.9 Multicollinearity Sometimes it is possible to have a significant F without significant regression coefficients. Multicollinearity occurs when there is multicollinearity among variables. This happens when two variables are highly correlated. Result in untrustworthy coefficients - serves to increase the variance of the estimate of the \\(\\beta\\) values. Example: inclusion of one predictor results in R2 = 0.80. When you add a highly correlated variable, the variance it accounts for is already described by the first variable - it does not account for unique variance. So we only get a slight increase in our R value. We can deal with this by evaluating the variables prior to inclusion into the model. 18.10 Determination of Predictors to Include? One of the most widespread ways is to use “stepwise” methods, you specify a direction either forward or backward. Forward selection: Initial model with only the constant b0 is made Add single predictor that best predicts the outcome by selecting the one with the greatest correlation with the outcome If fit is improved, then the predictor is retained Repeat Backward selection: As above but remove coefficients one at a time. 18.11 Model Design Considerations 1 Predictor variables must be either continuous or categorical. Predictors should have non-zero variance. Predictors should not be highly correlated (avoid multicollinearity). It is desirable to employ a regression that is parsimonious - as few as necessary but as many as needed. "],["comparing-two-means-using-t-tests.html", "19 Comparing Two Means using t-tests 19.1 Small differences in mean samples values 19.2 Large differences in mean samples values 19.3 Example 19.4 We will evaluate these data using a linear model 19.5 Picture Group 19.6 Real Spider Group 19.7 Output from a Regression 19.8 The Independent t-test 19.9 Lets use the data from Zar as a worked example. 19.10 Determine test value 19.11 Determine critical value 19.12 When Assumptions are Broken", " 19 Comparing Two Means using t-tests A common situation is the comparison of the means of two populations. An example of a t-test is to evaluate the mean blood clotting times between populations that were given two drugs. \\(H_0: \\mu_B = \\mu_G\\), where \\(\\mu\\) is the mean blood clotting time (in minutes) If the samples came from two normally distributed populations and the variances of the populations are equal ,we can use a two sample t-test. These conform to the assumptions of parametric statistics. Two samples of data are collected and the sample means calculated. The calculated means might differ by either a little or a lot. 19.1 Small differences in mean samples values We expect their means to be equal or very close. In this case, it is possible (even likely) for the population means to differ by chance alone (due to sampling error). We wwould expect that large differences in the mean values would occur very infrequently. \\(H_0: \\mu_B = \\mu_G\\) Alternatively, we can pose this as the difference in the mean: \\(H_0: \\mu_B - \\mu_G = 0\\). We are interested in understanding the magnitude of difference between the sample means. We use the standard error as a gauge of the variability between sample means. 19.2 Large differences in mean samples values If the difference between the samples we have collected is large, then we can infer one of two things: 1.) There is no effect of the two blood clotting medicine and by chance we calculated two sample means that are atypical of the population from which they came. 2.) The two samples come from different populations (populations with different mean values). In this scenario, the difference between samples represents a genuine difference between the samples (and we falsify the null hypothesis). So, this is the null model we are testing; did the samples come from the same population or did the samples come from the different populations? If the observed difference between the sample means is large: The more confident we become that the second explanation is correct (i.e. that the null hypothesis should be rejected). 19.3 Example Is arachnophobia (fear of spiders) specific to real spiders or is a picture enough? 24 arachnophobic individuals Experimental Manipulation: n = 12 randomly chosen participants were exposed to a real spider. n = 12 randomly chosen participants were exposed to a picture of the same spider. We monitor the anxiety produced. 19.4 We will evaluate these data using a linear model Consider an experiment where ‘Groups’ were exposed to a “Picture of a Spider” and an “Actual Spider” The response variable is the level of Anxiety (A) \\(\\hat{A_i}=b_0+b_1G_i\\) The independent variable G has only two values “Group 1” and “Group 2” ie. The “real” and “picture” groups. ## Group Anxiety G ## 1 Picture 20.67124 0 ## 2 Picture 36.79956 0 ## 3 Picture 38.36280 0 ## 4 Picture 26.76544 0 ## 5 Picture 36.46704 0 ## 6 Picture 15.36294 0 ## 7 Picture 20.63903 0 ## 8 Picture 23.93972 0 ## 9 Picture 40.35177 0 ## 10 Picture 41.79260 0 ## 11 Picture 31.16043 0 ## 12 Picture 28.42174 0 ## 13 Real Spider 23.68506 1 ## 14 Real Spider 59.23398 1 ## 15 Real Spider 35.84786 1 ## 16 Real Spider 47.14871 1 ## 17 Real Spider 44.51467 1 ## 18 Real Spider 52.42887 1 ## 19 Real Spider 61.94535 1 ## 20 Real Spider 52.43209 1 ## 21 Real Spider 39.05075 1 ## 22 Real Spider 57.72527 1 ## 23 Real Spider 48.60876 1 ## 24 Real Spider 41.13388 1 19.5 Picture Group We can code the variable, G, as a ‘dummy’ variable. It will take on the values one or zero. We can assign one group to have a value of zero, and one group to have a value of one. In the case of the ‘picture’ group, we will assign a value of G to be zero. \\(G_{picture}=0\\) \\(\\hat{A_i}=b_0+b_1G_i\\) The expected anxiety of group “picture” is the mean of anxiety of group “picture”. \\(\\bar{A}_{Picture}=b_0+b_1G_{picture}\\) \\(\\bar{A}_{Picture}=b_0+b_1\\times0\\) \\(\\bar{A}_{Picture}=b_0\\) 19.6 Real Spider Group Again, we can code the variable, G, as a ‘dummy’ variable. It can take on the values one or zero. We have assigned a value of G to be zero for the “picture” group. So, for the “real” group, we will assign the dummy variable G to equal one. \\(G_{real}=1\\) \\(\\hat{A_{G=1}}=b_0+b_1G_i\\) \\(\\hat{A_i}=30.06+b_1\\times1\\) \\(\\hat{A_i}=30.06+b_1\\) \\(b_1 = \\hat{A_i}-30.06\\) \\(b_1\\) = Difference between means \\(b_1=\\bar{A}_{Real}-\\bar{A}_{Picture}\\) Our task is to understand if this \\(\\beta_1\\) value is significantly different from zero. 19.7 Output from a Regression Let’s see how this is analyzed in the linear regression approach using R: ## ## Call: ## lm(formula = df.$Anxiety ~ df.$Group) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.2945 -6.5733 0.6342 7.1292 14.9657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.061 2.864 10.496 4.97e-10 *** ## df.$GroupReal Spider 16.918 4.050 4.177 0.000392 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.921 on 22 degrees of freedom ## Multiple R-squared: 0.4423, Adjusted R-squared: 0.4169 ## F-statistic: 17.45 on 1 and 22 DF, p-value: 0.0003915 ## Analysis of Variance Table ## ## Response: df.$Anxiety ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## df.$Group 1 1717.4 1717.40 17.448 0.0003915 *** ## Residuals 22 2165.5 98.43 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 19.8 The Independent t-test I am using the variable Y for the observations - this is a departure from Zar’s presentation but the mechanics are identical. \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{\\sqrt{\\frac{S^2_p}{n_1}+\\frac{S^2_p}{n_2}}}\\) The numerator is the difference between sample means. The denominator is the standard error of the difference between the sample means. This quantity is a measure of the variability of the data within the two samples. \\(S^2_p=\\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}\\) \\(S^2_p=\\frac{SS_1+SS_2}{v_1+v_2}\\) Here v1 and v2 are the degrees of freedom, v1 = n1 - 1 and v2 = n2 -1 The test value is compared to the critical value at a given \\(\\alpha\\) \\(t_{\\alpha,2,df}\\) Need to set \\(\\alpha\\) value. One or two-tailed test? v1 = n1 - 1 and v2 = n2 - 1 19.9 Lets use the data from Zar as a worked example. \\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1\\ne\\mu_2\\) \\(H_0: \\mu_1 - \\mu_2 = 0\\) \\(H_A: \\mu_1-\\mu_2\\ne0\\) Given drug B Given drug G 8.8 9.9 8.4 9.0 7.9 11.1 8.7 9.6 9.1 8.7 9.6 10.4 9.5 —— \\(n_1=6\\) \\(n_2=7\\) \\(\\nu_1=5\\) \\(\\nu_2=6\\) \\(\\bar{Y}_1=\\) 8.75 min \\(\\bar{Y}_2=\\) 9.74 min SS1 = 1.6950 min2  SS2 = 4.0171 min2 \\(S^2_p=\\frac{SS_1+SS_2}{v_1+v_2}=\\frac{1.6950+4.0171}{5+6}=\\frac{5.7121}{11}=0.5193 \\: \\mbox{min}^2\\) \\(s_{\\bar{Y_1}-\\bar{Y}_2}=\\sqrt{\\frac{S^2_p}{n_1}+\\frac{S^2_p}{n_2}}=\\sqrt{\\frac{0.5193}{6}+\\frac{0.5193}{7}}=\\sqrt{0.0866+0.0742}\\) \\(=\\sqrt{0.1608}=0.40\\mbox{min}\\) \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}\\) 19.10 Determine test value \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}=\\frac{8.75-9.74}{0.40}=\\frac{-0.99}{0.40}=-2.475\\) \\(|t|=|\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}|=2.475\\) 19.11 Determine critical value \\(t_{0.05(2),v}=t_{0.05(2),11}=2.201\\) Therefore, reject H0 \\(P(|t|\\ge2.475)&lt;0.05\\) \\(\\: \\:\\) \\(P=0.031\\) 19.12 When Assumptions are Broken Non-parameteric t-test Mann-Whitney “U” Test +Do not require estimation of \\(\\mu\\) and \\(\\sigma\\). +No assumptions about distributions. Convert data to RANKS of data. Two sample rank test Rank from highest to lowest, the greatest value in either group is given a one, second given a two.. \\(U=n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\\) n1 and n2 are the number of observation sin samples 1 and 2. R1 is the sum of the ranks in sample 1 H0: Male and female students are the same height. HA: Male and female students are not the same height. \\(\\alpha=0.05\\) Height of males Height of females Ranks of male heights Ranks of female heights 193 cm 178 cm 1 6 188 173 2 8 185 168 3 10 183 165 4 11 180 163 5 12 175 7 170 9 — \\(n_1=7\\) \\(n_2=5\\) \\(R_1=31\\) \\(R_2=47\\) \\(U=n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\\) \\((7)(5)+\\frac{(7)(8)}{2}-31\\) \\(35+28-31\\) \\(32\\) \\(U^1=n_1n_2-U\\) \\((7)(5)-32\\) \\(U_{0.05(2),5,7}=30\\) Because 32&gt;30, H0 is rejected. Therefore, we conclude that height is different for male and female students. "],["analysis-of-variance-anova.html", "20 Analysis of Variance (ANOVA) 20.1 When and why to use ANOVA 20.2 What Does ANOVA Tell Us? 20.3 ANOVA as Regression 20.4 Placebo Group 20.5 Low Dose Group 20.6 High Dose Group 20.7 Output from Regression 20.8 Theory of ANOVA 20.9 ANOVA Worked Example 20.10 The Data 20.11 Step 1: Calculate SST 20.12 Degrees of Freedom 20.13 Model Sum of Squares (SSM) 20.14 Step 2: Calculate SSM 20.15 Model Degrees of Freedom 20.16 Residual Sum of Squares (SSR) 20.17 Step 3: Calculate SSR 20.18 Residual Degrees of Freedom 20.19 Double Check 20.20 Step 4: Calculate the Mean Squared Error 20.21 Step 5: Calculate the F-Ratio 20.22 Step 6: Construct a Summary Table 20.23 Multiple-Comparison Tests 20.24 Tukey Test", " 20 Analysis of Variance (ANOVA) In this module we will understand the basic principles of ANOVA. We will be analyzing one-way ANOVA, also called ‘one-factor’ ANOVA or ‘single factor Analysis of Variance’. We will be analyzing the impact to the mean value among k groups, where \\(k &gt;2\\). ANOVA is a parametric test. We will stick with Zar’s terminology (p 178, 4th edition). 20.1 When and why to use ANOVA When we want to compare means we can use a t-test but this test has limitations, we can only test the equality of two mean values. Often we would like to compare means from three or more groups. The ANOVA is an extension of regression (and hence is a general linear model). 20.1.1 Why Not Use Lots of t-tests? Inflates the Type I error rate \\(1 - (1- \\alpha)^n\\) 20.2 What Does ANOVA Tell Us? Null hypothesis: Like a t-test, one way ANOVA tests the null hypothesis that the means are the same among groups. ANOVA is an omnibus test: It test for an overall test for evaluating the equality of means among groups. It tells us that the group means are different. It doesn’t tell us which mean(s) differ. 20.3 ANOVA as Regression Let us assume that we test three different medicine levels and want to see if it impacts (results in contrast, i.e. significant differences) the illness level in pigs. We are going to test the effect of the three groups (k = 3), in this case medicine type. The three groups are ‘placebo’, ‘high’, and ‘low’. The analysis is termed a one-factor test or one-way ANOVA (only a single thing, the medicine, is different). Pigs are assigned, at random, to each of the three groups. These are the replicates of each group. \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) Here \\(high\\) and \\(low\\) are dummy variables, just like G in the t-test. Group Dummy variable 1 (High) Dummy variable 2 (Low) Placebo 0 0 Low dose medicine 0 1 High dose medicine 1 0 20.4 Placebo Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 0)+(b_2\\times 0)\\) \\(\\mbox{illness}_i=b_0\\) \\(\\bar{Y}_{placebo}=b_0\\) 20.5 Low Dose Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 1)+(b_2\\times 0)\\) \\(\\mbox{illness}_i=b_0+b_1\\) \\(\\bar{Y}_{low}=\\bar{Y}_{placebo}+b_1\\) \\(b_1=\\bar{Y}_{low}-\\bar{Y}_{placebo}\\) 20.6 High Dose Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 0)+(b_2\\times 1)\\) \\(\\mbox{illness}_i=b_0+b_2\\) \\(\\bar{Y}_{high}= \\bar{Y}_{placebo}+b_2\\) \\(b_2=\\bar{Y}_{high}-\\bar{Y}_{placebo}\\) 20.7 Output from Regression Coefficients: Estimate Std. Error t value Pr(&gt; (intercept) 2.2000 0.6272 3.508 0.00432 ** dummy 1 2.8000 0.8869 3.157 0.00827 ** dummy 2 1.0000 0.8869 1.127 0.28158 Signif. Codes:  0 ‘’ 0.001 ’’ 0.01 ’’ 0.05’.’ 0.1 ’’ 1 residual standard error:  1.402 on 12 degrees of freedom. Multiple R-squared:  0.4604, Adjusted R-squared:  0.3704 F-statistic:  5.119 on 2 and 12 df, p-value:  0.02469 20.8 Theory of ANOVA We use SS components: First, we calculate how much variability there is between scores Total sum of squares (SST). We then calculate how much of this variability can be explained by the model we fit to the data How much variability is due to the experimental manipulation? This is evaluated using the model sum of squares (SSM) How much variability is due to individual differences in performance, residual sum of squares (SSR). We compare the amount of variability explained by the model (experiment), to the error in the model (individual differences) This ratio is called the F-ratio. If the model explains a lot more variability than it can’t explain, then the experimental manipulation has had a significant effect on the outcome. 20.9 ANOVA Worked Example Testing the effects of medicine on illness using three groups: Placebo (sugar pill) Low dose medicine High dose medicine The outcome/dependent variable (DV) was an objective measure of illness. 20.10 The Data Placebo Low Dose High Dose 3 5 7 2 2 4 1 4 5 1 2 3 4 3 6 \\(\\bar{Y}\\) 2.20 3.20 5.00 s 1.30 1.30 1.58 s2 1.70 1.70 2.50 Grand Mean = 3.467, Grand s = 1.767, Grand s2 = 3.124 20.11 Step 1: Calculate SST SST uses the differences between the observed data and the mean value of Y. Where the mean is the grand mean Placebo Low Dose High Dose 3 5 7 2 2 4 1 4 5 1 2 3 4 3 6 \\(\\bar{Y}\\) 2.20 3.20 5.00 s 1.30 1.30 1.58 s2 1.70 1.70 2.50 Grand Mean = 3.467, Grand s = 1.767, Grand s2 = 3.124 SST = sum((observed - Grand mean)2) \\(SS_T=\\sum(obs_i - \\bar{X}_{grand})\\) 20.12 Degrees of Freedom Degrees of freedom (df) are the number of values that are free to vary. In general, the df are one less than the number of values used to calculate the SS. dfTotal = N - 1 20.13 Model Sum of Squares (SSM) Difference between the model estimate and the mean (or “Grand Mean”) 20.14 Step 2: Calculate SSM \\(SS_M=\\sum n_i(\\bar{x}_i - \\bar{x}_{grand})^2\\) \\(SS_M=5(2.2-3.467)^2+5(3.2-3.467)^2+5(5.0-3.467)^2\\) \\(SS_M=5(-1.267)^2+5(-0.267)^2+5(1.533)^2\\) \\(SS_M=8.025+0.355+11.755\\) \\(SS_M=20.135\\) 20.15 Model Degrees of Freedom How many values did we use to calculate SSM? We used the 3 means. \\(df_M=k-1=3-1=2\\) 20.16 Residual Sum of Squares (SSR) 20.17 Step 3: Calculate SSR \\(SS_R=\\mbox{sum}([x_i-\\bar{x}_i]^2)\\) \\(SS_R=S^2_{group1}(n_1-1)+S^2_{group2}(n_2-1)+S^2_{group3}(n_3-1)\\) \\(SS_R=1.70(5-1)+1.70(5-1)+2.5(5-1)\\) \\(SS_R=(1.70\\times4)+(1.70\\times4)+(2.50\\times4)\\) \\(SS_R=6.8+6.8+10\\) \\(SS_R=23.60\\) 20.18 Residual Degrees of Freedom How many values did we use to calculate SSR? We used the 5 scores for each of the SS for each group. \\(df_R=df_{group1}+df_{group2}+df_{group3}\\) \\(df_R=(n_1-1)+(n_2-1)+(n_3-1)\\) \\(df_R=(5-1)+(5-1)+(5-1)\\) \\(df_R=12\\) 20.19 Double Check \\(SS_T=SS_R+SS_M\\) \\(43.74=20.14+23.60\\) \\(df_T=df_R+df_M\\) \\(14=2+12\\) 20.20 Step 4: Calculate the Mean Squared Error \\(MS_M=\\frac{SS_M}{df_M}=\\frac{20.135}{2}=10.067\\) \\(MS_R=\\frac{SS_R}{df_R}=\\frac{23.60}{12}=1.967\\) 20.21 Step 5: Calculate the F-Ratio \\(F=\\frac{MS_M}{MS_R}\\) \\(F=\\frac{MS_M}{MS_R}=\\frac{10.067}{1.967}=5.12\\) 20.22 Step 6: Construct a Summary Table Source SS df MS F Model 20.14 2 10.067 5.12* Residual 23.60 12 1.967 Total 43.74 14 20.23 Multiple-Comparison Tests The ANOVA that you examined is used to test the hypothesis that there is no difference in the sample means among k treatment levels However we cannot conclude, after doing the test, which of the mean values are different from one-another. 20.24 Tukey Test Tukey test - balanced, orthogonal designs Step 1: is to arrange and number all five sample means in order of increasing magnitude Calculate the pairwise difference in sample means. We use a t-test “analog” to calculate a q-statistic S2 is the error mean square by ANOVA computation n is the of data in each of groups B and A Remember this is a completely balanced design. \\(SE=\\sqrt{\\frac{s^2}{n}}\\) \\(q=\\frac{\\bar{Y}_B-\\bar{Y}_A}{SE}\\) Start with the largest mean, vs. the smallest mean. Then when the first largest mean has been compared with increasingly large second means, use the second largest mean. If the null hypothesis is accepted between two means then all other means within that range cannot be different. "],["two-way-independent-anova.html", "21 Two-Way Independent ANOVA 21.1 What is Two-Way Independent ANOVA? 21.2 Example of two-way ANOVA 21.3 Hypotheses Being Tested 21.4 Multiway Factorial ANOVA Hypotheses Being Tested 21.5 Hypotheses Being Tested 21.6 Benefit of Factorial Designs 21.7 A worked example of factorial design 21.8 Hypotheses Being Tested 21.9 Remeber the terminology 21.10 Step 1: Calculate SST 21.11 Step 2: Calculate SSM 21.12 Step 2a: Calculate SSA 21.13 Step 2b: Calculate SSB 21.14 Step 2c: Calculate SS(AxB) 21.15 Step 3: Calculate SSR 21.16 Degrees of Freedom and Mean Square Calculations 21.17 Interpreting Factorial ANOVA 21.18 Interpretation: Main Effect pH 21.19 Interpretation: Main Effect Sun 21.20 Interpretation: Interaction Effects 21.21 Factorial ANOVA as Regression 21.22 Interaction Effects 21.23 Post-hoc Tests 21.24 Non-Parametric Tests", " 21 Two-Way Independent ANOVA Like one-way ANOVA this is a comparison of means test. In this module we will examine multifactor ANOVA. Multi-factorial ANOVA as a linear model Hypotheses being tested Interaction effects Post-hoc tests Non-parametric versions 21.1 What is Two-Way Independent ANOVA? Two independent variables Two-way (n = 2) factors Simultaneous analysis of two factors and measurement of mean response. Terminology: One factor termed A and one factor termed B. a is the number of levels in A. b is the number of levels in B. 21.2 Example of two-way ANOVA Researchers have sought to examine the effects of various types of music on agitation levels in patients in early and middle stages of Alzheimer’s disease. Patients were selected based on their form of Alzheimer’s disease. Three forms of music were tested: easy listening, Mozart, and piano interludes. The response variable agitation level was scored. Here is the design: Group Piano Interlude Mozart Easy Listening Early Stage ALzheimer’s 21 9 29 24 12 26 22 10 30 18 5 24 20 9 26 Middle Stage Alzheimer’s 22 14 15 20 18 18 25 11 20 18 9 13 20 13 19 21.3 Hypotheses Being Tested Three factor ANOVA: HO: Mean agitation is the same for each music type. HO : Mean agitation is the same for each Alzheimer’s stage. HO : Mean agitation is the same for all levels of music, independent of Alzheimer stage. 21.4 Multiway Factorial ANOVA Hypotheses Being Tested Popcorn Oil Amt. Batch Yeild Plain Little Large 8.2 Gourmet Little Large 8.6 Plain Lots Large 10.4 Gourmet Lots Large 9.2 Plain Little Small 9.9 Gourmet Little Small 12.1 Plain Lots Small 10.6 Gourmet Lots Small 18.0 Plain Little Large 8.8 Gourmet Little Large 8.2 Plain Lots Large 8.8 Gourmet Lots Large 9.8 Plain Little Small 10.1 Gourmet Little Small 15.9 Plain Lots Small 7.4 Gourmet Lots Small 16.0 21.5 Hypotheses Being Tested Three factor ANOVA: HO: Yield is the same in all three Batch sizes HO : Yield is the same in all three Oil amounts HO : Yield is the same in all three Popcorn types HO : The mean yield is the same for all levels of batch, independent of oil amount (Batch X Oil) HO : The mean yield is the same for all levels of Oil amount, independent of popcorn type (Oil X Type) HO : The mean yield is the same for all levels of batch, independent of popcorn type (Batch X Type) HO : Differences in mean Yield among the batch, oil amount, and popcorn type are independent of the other factors + (Batch X Type X Oil) Df Sum Sq Mean Sq F value Pr(&gt;F) popcorn type 1 3.062 3.062 0.1731 0.6883 oil amount 1 0.062 0.062 0.0035 0.9541 batch size 1 52.562 52.562 2.9717 0.1230 popcorn type:oil amount 1 27.562 27.562 1.5583 0.2472 popcorn type:batch size 1 14.062 14.062 0.7951 0.3986 oil amount:batch size 1 0.063 0.063 0.0035 0.9541 popcorn type:oil amount:batch size 1 1.563 1.563 0.0883 0.7739 Residuals 8 141.500 17.687 21.6 Benefit of Factorial Designs 1.) The experimental design is an efficient design to test multiple hypotheses simultaneously. 2.) We can look at how variables interact, this process is called ‘interaction’. Show how the effects that one level of one group (or factor) might depend on the effects of level in another group. Are often more interesting than main effects. 21.7 A worked example of factorial design Testing the effects of the pH of soil and amount of sunlight exposure (full or partial) on mean plant height. We will test: 21.8 Hypotheses Being Tested HO: Mean plant height is the same in all sunlight treatments. HO : Mean plant height is the same in all pH treatments. HO : There is no significant interaction of ph and sunlight. 21.9 Remeber the terminology Factor A (pH), levels = 6, 7, 8 Factor B (Sun), levels = full, partial The response is the height of the plant at the end of the experiment. The experiment has two “levels” for the factor “B” (b = 2) and three “levels” for the factor “A” (a = 3). Thus, there are \\(ab = 3\\times2=6\\) unique combinations of sun and pH With each combination. We have r = 8 observations for each combination. r is called the number of replicates, in each cell. The total number of replicates (n) is: \\(N = abr = 3\\times2\\times8 = 48\\). The amounts \\(Y_{i,j,k}\\) is the amount of plant height for each replicate (k = 1 to 8) with sun i, (i = 1, 2) at pH j, (j = 1, 2, 3). Treatment and level-specific data for the plant height (cm): pH (Factor A, a levels) 6 6 7 7 8 8 Sun (Factor B, b levels) Full Partial Full Partial Full Partial 65 50 70 45 55 30 70 55 65 60 65 30 60 80 60 85 70 30 60 65 70 65 55 55 60 70 65 70 55 35 55 75 60 70 60 20 60 75 60 80 50 45 55 65 50 60 50 40 Total 485 535 500 535 460 285 Mean 60.625 66.875 62.50 66.875 57.50 35.625 Variance 24.55 106.70 42.86 156.70 50.00 117.41 21.10 Step 1: Calculate SST \\(Grand \\: Mean = \\bar{X} = 58.33\\) \\(SS_T=\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{l=1}^{n}({X_{i,j,l} - \\bar{X}})^2\\) \\(SS_T=8966.66\\) 21.11 Step 2: Calculate SSM pH (Factor A, a levels) 6 6 7 7 8 8 Sun (Factor B, b levels) Full Partial Full Partial Full Partial Mean 60.625 66.875 62.50 66.875 57.50 35.625 Here we evaluate main effects and interaction effects. \\(SS_M=n\\sum_{i=1}^{a}\\sum_{j = 1}^{b}({\\bar{X}_{ij}}-\\bar{X})^2\\) \\(\\mbox{SS}_\\mbox{M}=8(60.625-58.33)^2+8(66.875-58.33)^2+8(62.5-58.33)^2+8(66.875-58.33)^2+8(57.5-58.33)^2+8(35.625-58.33)^2\\) \\(\\mbox{SS}_\\mbox{M}=8(2.295)^2+8(8.545)^2+8(4.17)^2+8(8.545)^2+8(-0.83)^2+8(-22.705)^2\\) \\(\\mbox{SS}_\\mbox{M}=42.1362+584.1362+139.1112+584.1362+5.5112+4124.1362\\) \\(\\mbox{SS}_\\mbox{M}=5479.17\\) 21.12 Step 2a: Calculate SSA Mean “Full Sun”: 60.21 cm SSA 65 70 55 70 65 65 60 60 70 60 70 55 60 65 55 55 60 60 60 60 50 55 50 50 Mean “Partial Sun”: 56.46 cm SSA 50 45 30 55 60 30 80 85 30 65 65 55 70 70 35 75 70 20 75 80 45 65 60 40 \\(SS_{A} = {rb}\\sum_{i=1}^a(\\overline{X}_{i} - \\overline{X})^2\\) \\(\\mbox{SS}_\\mbox{Sun}=24(60.21-58.33)^2+24(56.46-58.33)^2\\) \\(\\mbox{SS}_\\mbox{Sun}=24(1.88)^2+24(-1.87)^2\\) \\(\\mbox{SS}_\\mbox{Sun}=84.8256+83.9256\\) \\(\\mbox{SS}_\\mbox{Sun}=168.75\\) 21.13 Step 2b: Calculate SSB Factor B is pH. pH = 6 Mean \\(\\mbox{pH}_6\\) = 63.75 SSB 65 50 70 55 60 80 60 65 60 70 55 75 60 75 55 65 pH = 7 Mean \\(\\mbox{pH}_7\\) = 64.68 SSB 70 45 65 60 60 85 70 65 65 70 60 70 60 80 50 60 pH = 8 Mean \\(\\mbox{pH}_8\\) = 46.56 SSB 55 30 65 30 70 30 55 55 55 35 60 20 50 45 50 40 \\(SS_B = ra\\sum_{j=1}^b(\\overline{X}_{j} - \\overline{X})^2\\) \\(\\mbox{SS}_\\mbox{pH}=16(63.75-58.33)^2+16(64.68-58.33)^2+16(46.56-58.33)^2\\) \\(\\mbox{SS}_\\mbox{pH}=16(5.42)^2+16(6.3575)^2+16(-11.7675)^2\\) \\(\\mbox{SS}_\\mbox{pH}=470.0224+646.6849+2215.5849\\) \\(\\mbox{SS}_\\mbox{pH}=3332.292\\) 21.14 Step 2c: Calculate SS(AxB) \\(\\mbox{SS}_\\mbox{M}=\\mbox{SS}_{\\mbox{A}{\\times}\\mbox{B}} + \\mbox{SS}_\\mbox{A}+\\mbox{SS}_\\mbox{B}\\) \\(\\mbox{SS}_{\\mbox{A}{\\times}\\mbox{B}}=\\mbox{SS}_\\mbox{M}-\\mbox{SS}_\\mbox{A}-\\mbox{SS}_\\mbox{B}\\) \\(\\mbox{SS}_{\\mbox{Sun}{\\times}\\mbox{pH}}=\\mbox{SS}_\\mbox{M}-\\mbox{SS}_\\mbox{Sun}-\\mbox{SS}_\\mbox{pH}\\) \\(=5479.167-168.75-3332.292\\) \\(=1978.125\\) 21.15 Step 3: Calculate SSR The residual sum of squares is calculated in the same way as for one-way ANOVA . Represents individual differences in performance or the variance that can’t be explained by factors that were systematically manipulated. We saw in one-way ANOVA that the value is calculated by taking the squared error between each data point and its corresponding group mean. So, we use the individual variances of each group and multiply them by one less than the number of people within the group (n). We have the individual group variances: there were eight people in each group (therefore, n = 8). \\(\\mbox{SS}_\\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group \\: n}(n_n-1)\\) \\(\\mbox{SS}_\\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group4}(n_4-1)+s^2_{group5}(n_5-1)+s^2_{group6}(n_6-1)\\) \\(\\mbox{SS}_\\mbox{R}=(24.55\\times7)+(106.7\\times7)+(42.86\\times7)+(156.7\\times7)+(50\\times7)+(117.41\\times7)\\) \\(\\mbox{SS}_\\mbox{R}=171.85+746.9+300+1096.9+350+821.87\\) \\(\\mbox{SS}_\\mbox{R}=3487.52\\) 21.16 Degrees of Freedom and Mean Square Calculations Source SS df MS Factor A \\(SS_A\\) a-1 \\(\\frac{SS_A}{a-1}\\) Factor B \\(SS_B\\) b-1 \\(\\frac{SS_B}{b-1}\\) Factor A x B \\(SS_{AB}\\) (a-1)(b-1) \\(\\frac{SS_{AB}}{(a-1)(b-1)}\\) Error SSE N-ab \\(\\frac{SSE}{N-ab}\\) Total SST N-1 21.17 Interpreting Factorial ANOVA Response Plant Growth Sum Sq Df F value Pr(&gt;F) Sunlight 169 1 2.0323 0.1614 pH 3332 2 20.0654 7.649e-07 Sunlight:pH 1978 2 11.9113 7.987e-05 residuals 3488 42 21.18 Interpretation: Main Effect pH There was a significant main effect of the amount of pH in the soil on the height of the plant, F(2, 42) = 20.07, p &lt; 0.001 21.19 Interpretation: Main Effect Sun There was a non-significant main effect of sunlight exposure on the plant height, F(1, 42) = 2.03, p = 0.161. 21.20 Interpretation: Interaction Effects There was a significant interaction between the amount of pH in the soil and the amount of sunlight exposure, on the height of the plant, F(2, 42) = 11.91, p &lt; .001. Non-parallel lines indicate such an interaction: For low pH full and partial sunlight, scores do not change much. At a high pH, partial sunlight scores plummet but full sunlight scores remain fairly high. So, the interaction is caused by a difference between sunlight exposure in the height of plants. 21.21 Factorial ANOVA as Regression pH 6 6 7 7 8 8 Sunlight Full Partial Full Partial Full Partial 65 50 70 45 55 30 70 55 65 60 65 30 60 80 60 85 70 30 60 65 70 65 55 55 60 70 65 70 55 35 55 75 60 70 60 20 60 75 60 80 50 45 55 65 50 60 50 40 Total 485 535 500 535 460 285 Mean 60.625 66.875 62.50 66.875 57.50 35.625 Variance 24.55 106.70 42.86 156.70 50.00 117.41 \\(outcome_i = (\\mbox{model})+\\mbox{error}_i\\) \\(plant height_i=(b_0+b_{1}\\mbox{Sunlight}_i+b_2\\mbox{pH}_i)+\\varepsilon_i\\) \\(plant height_i=(b_0+b_1A_i+b_2B_i+b_3AB_i)+\\varepsilon_i\\) \\(plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\\varepsilon_i\\) How do we code the interaction term? Multiply the variables A x B Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\\varepsilon_i\\) \\(\\bar{Y}_{partial,6}=b_0+(b_1\\times0)+(b_2\\times0)+(b_3\\times0)\\) \\(b_0=\\bar{Y}_{partial,6}\\) \\(b_0=66.875\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{Full,6}=b_0+(b_1\\times1)+(b_2\\times0)+(b_3\\times0)\\) \\(\\bar{Y}_{Full,6}=b_0+b_1\\) \\(\\bar{Y}_{Full,6}=\\bar{Y}_{partial,6}+b_1\\) \\(b_1=\\bar{Y}_{Full,6}-\\bar{Y}_{partial,6}\\) \\(b_1=60.625-66.875\\) \\(b_1=-6.25\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{partial,4 \\: pH}=b_0+(b_1\\times0)+(b_2\\times1)+(b_3\\times0)\\) \\(\\bar{Y}_{partial,4 \\: pH}=b_0+b_2\\) \\(\\bar{Y}_{partial,4 \\: pH}=\\bar{Y}_{partial,6}+b_2\\) \\(b_2=\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6}\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{Full,4 \\: pH}=b_0+(b_1\\times1)+(b_2\\times1)+(b_3\\times1)\\) \\(\\bar{Y}_{Full,4 \\: pH}=b_0+b_1+b_2+b_3\\) \\(\\bar{Y}_{Full,4 \\: pH}=\\bar{Y}_{partial,6}+(\\bar{Y}_{Full,6}-\\bar{Y}_{partial,6})+(\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6})+b_3\\) \\(\\bar{Y}_{Full,4 \\: pH}=\\bar{Y}_{Full,6}+\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6}+b_3\\) \\(b_3=\\bar{Y}_{partial,6}-\\bar{Y}_{Full,6}+\\bar{Y}_{Full,4 \\: pH}-\\bar{Y}_{partial,4 \\: pH}\\) \\(b_3=66.875-60.625+57.500-35.625\\) \\(b_3=28.125\\) 21.22 Interaction Effects Experiment: we are interested in oxygen consumption of two species of limpets in different concentration of seawater. Factor A is the species of limpet (levels, a) Factor B is the concentration of SW as a function of maximum salinity - 100, 75, and 50 % (levels, b) Completed anova Source of variation df SS MS Species 1 16.6380 16.638 ns salinities 2 10.3566 5.178 ns Sp X Sal 2 194.8907 97.445 ** Error 42 401.5213 9.560 Total 47 623.4066 When the two factors are identified as A and B, the interaction is identified as the A X B interaction. Variability not accounted for by A and B alone. Interaction: The effect of one factor in the presence of a particular level of another factor. There is an interaction between two factors if the effect of one factor depends on the levels of the second factor. Species Species Seawater Concentration A. scabra A. digitalis Mean 100% 10.56 7.43 9.00 75% 7.89 7.34 10.11 50% 12.17 12.33 9.76 Mean 10.21 9.03 9.62 The response to salinity differs between the two species At 75% salinity A. scabra consumes the least oxygen and A. digitalis consumes the most. Therefore a simple statement about the species response to salinity is not clear; all we can really say is: The pattern of response to changes in salinity differed in the two species. The difference among levels of one factor is not constant at all levels of the second factor “It is generally not useful to speak of an individual factor effect - even if its F is significant - if there is a significant interaction effect” - Zar 21.23 Post-hoc Tests Tukey test - balanced, orthogonal designs Step one: is to arrange and number all five sample means in order of increasing magnitude Calculate the pairwise difference in sample means. We use a t-test “analog” to calculate a q-statistic Scheffe’s test Examine multiple contrasts: ideas is to compare combinations of samples to each other instead of the comparison among individual k levels. Compare the mean outflow volume of four different rivers: 5 vs 1,2,3,4 \\(H_0:\\mu_2/3+\\mu_4/3+\\mu_3/3-\\mu_5=0\\) \\(H_0:(\\mu_2+\\mu_4+\\mu_3)/3=\\mu_5\\) \\(c_2=\\frac{1}{3}, \\: c_4=\\frac{1}{3}, \\: c_3=\\frac{1}{3}, \\: and \\: c_5=-1\\) Alternatives multiple contrasts: \\(H_0:(\\mu_1+\\mu_5)/2-(\\mu_2+\\mu_4+\\mu_3)/3=0\\) \\(H_0:\\mu_1-(\\mu_2+\\mu_4+\\mu_3)/3\\) Test Statistic: \\(s=\\frac{|\\sum c_i\\bar{Y}_i|}{SE}\\) Where \\(SE=\\sqrt{s^2(\\sum \\frac{c^2_i}{n_i})}\\) and the critical value of the test is \\(S_{\\alpha}=\\sqrt{(k-1)F_{\\alpha(1),k-1,N-k}}\\) 21.24 Non-Parametric Tests Violations of the assumptions We assume equality of variance - ANOVA is a robust test. Robust to unbalanced design. How to deal with outliers: use in analysis if they are valid data. Test of normality: Shapiro Wilks Test of equality of variance: Bartletts test. Nonparametric analysis of variance. If k &gt; 2 Kruskal-Wallis test - analysis of variance by rank Power increases with sample size. If k = 2 the Kruskal-Wallis is equivalent to the Mann-Whitney test. \\(H=\\frac{12}{N(N+1)}\\sum ^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\) If there are tied ranks H needs to be corrected using a correction factor C. \\(C=1-\\frac{\\sum t}{N^3-N}\\) \\(H_c=\\frac{H}{C}\\) \\(\\sum t=\\sum (t^3_i-t_i)\\) ti is the number of tied ranks. A limnologist obtained eight containers of water from each of four ponds. The pH of each water sample was measured. The data are arranged in ascending order within each pond. (One of the containers from pond 3 was lost, so n3 = 7, instead of 8; but the test procedure does not require equal numbers of data in each group.) The rank of each datum is shown parenthetically. H0: pH is the same in all four ponds. HA: pH is not the same in all four ponds. Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(H=\\frac{12}{N(N+1)}\\sum ^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\) \\(=\\frac{12}{32(32)}[\\frac{55^2}{8}+\\frac{132.5^2}{8}+\\frac{145^2}{7}+\\frac{163.5^2}{8}]-3(32)\\) \\(=11.876\\) Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(\\sum t=\\sum (t^3_i-t_i)\\) \\(\\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)\\) \\(\\sum t=168\\) Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(\\sum t=\\sum (t^3_i-t_i)\\) \\(\\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)\\) \\(\\sum t=168\\) \\(C=1-\\frac{\\sum t}{N^3-N}=1-\\frac{168}{31^3-31}=1-\\frac{168}{29760}=0.9944\\) \\(H_c=\\frac{H}{C}=\\frac{11.876}{0.9944}=11.943\\) \\(\\nu=k-1=3\\) \\(F=\\frac{(N-k)H_c}{(k-1)(N-1-H_c)}=\\frac{(31-4)(11.943)}{(4-1)(31-1-11.943)}=5.95\\) \\(F_{0.05(1),3,26}=2.98\\) Reject H0 "],["maximum-likelihood-1.html", "22 Maximum Likelihood 22.1 General Exprimental Approach 22.2 Two approaches to parameter estimation: 22.3 The probability density function 22.4 The likelihood function", " 22 Maximum Likelihood The primary source for the following lecture material are primarily derived from I.J. Myung’s paper “Tutorial on maximum likelihood estimation”. Journal of Mathematical Psychology 47 (2003) 90-100. The interested reader is directed to review this paper. In this lecture we will understand an alternative to parameter estimation using least-squares estimation, maximum likelihood estimation (MLE). MLE is a preferred method of parameter estimation in statistics and is a general parameter estimation approach, in particular in non-linear modeling and/or with non-normally distributed data. Because of the prevelance of non-normally distributed data in the natural sciences (e.g. data from counting, categorical data, skewed ratio data) an examination and familiarization of this approach is useful. 22.1 General Exprimental Approach Because many phenomenon of interest are not directly observable, we formulate hypotheses to test and these hypothesis are stated in terms of probability using statistical models. The goal of statistical modeling is to understand underlying processes by testing the viability (e.g. quality, robustness) of the model. Our method: Specify a statistical model Collect data Evaluate how well the model fits the data by: Parameter estimation Evaluating goodness of fit 22.2 Two approaches to parameter estimation: LSE (least-squares estimation), for normally-distributed data MLE, a general approach for parameter estimation 22.3 The probability density function The goal of data analysis is to identify the population that is most likely to have generated the sample - i.e. we will estimate the parameters of the candidate model that will produce these observations. The data vector \\(y = (y_1, y_2, ..., y_m)\\) is a random sample from a population distributed in some unknown way. Populations are identified using a probability distribution and unique values of the parameters - As the parameter changes in value, different probability distributions are generated. Let \\(f(y|w)\\) denote the probability density function (PDF) that specifies the probability of observing data vector y given the parameter w. The parameter \\(w = (w_1, ..., w_k)\\) is a vector defined on a multi-dimensional parameter space. For example, if the PDF is normal, \\(w = (\\mu, \\sigma)\\) If the PDF is a t distribution, \\(w = (d.f.)\\) Different distributions are defined using different parameters (and different mathematical formulations), so w is distribution-specific. If we have specified a distribution that has a certain set of parameters, for example: We can use a given probability distribution and parameter set to determine the probability of obtaining a value in a population. Example: Children’s IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What is the probability of a randomly selected child having an IQ between 80 and 120? In this case the area under the curve is 0.818 or 81.8% of the integral of the distribution from \\(-\\infty\\) to \\(+\\infty\\). So, if we take 100 random draws from the population of children’s IQs, we will get values of IQ &gt; 80 and &lt; 120, 81 to 82 times… Let’s examine the statement: \\(p(80 &lt; IQ &lt; 120\\) | \\(\\mu = 100, \\sigma = 15) = 0.818\\). We are stating that the probability of randomly selecting an IQ observation the variable characteristics given the parameter values is 0.818. If we are interested in finding probabilities of students with different variable characteristics (different IQ values), then we will change the left side of the equation. For example: \\(p(IQ &lt; 65\\) | \\(\\mu = 100, \\sigma = 15)\\) or \\(p(IQ &gt; 120\\) | \\(\\mu = 100, \\sigma = 15)\\) The right side of the equation does not change because it describes the fixed shape of the distribution of the population that “creates” the observations. So when we are investigating probability of an event we are quantifying the integral of the curve for the given parameter set bounded by the left side of the equation. We change the left side to derive new probability values. We can determine the probability of obtaining \\(p(y_1, y_2, ..., y_m)\\) | \\(w)\\) if the observations are independent. Think about taking multiple random draws from the population described by the parameter set w. An analog here is thinking about coin flipping, the probability of realizing specific outcomes from multiple trials is determined by multiplication (e.g. probability of observing two “tails” flips is the product of the independent single observations). So, the notation below is Pi (product) notation. This is used in mathematics to indicate repeated multiplication. \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = p(y_1\\)| \\(w) \\times p(y_2\\)| \\(w) \\times ...p(y_n\\)| \\(w)\\) \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = \\prod_{i = 1}^{n}{p(y_i | w)}\\) 22.4 The likelihood function Now we will discuss the likelihood function that is derived by considering that the data are fixed and that the shape of the (parent) distribution is random. In this approach we are evaluating the inverse problem of the PDF. Specifically, given the observed data and a model of interest, we are interested in finding the unique parameter vector w, among all the possible combinations of parameters that is most likely to have produced the data. We have already observed the data and now want to define the likelihood function by reversing the roles (i.e. what is random what is fixed) of the data vector y and the parameter vector w. So, we focus on \\(L(w|y)\\). This represents the likelihood of the parameter w given the observed data y; and as such is a function of w. So, the data are fixed, and we modify the parameter vector w (in the case of IQ, \\(\\mu\\) and \\(\\sigma\\)) Assume we have some vector of observed data y, these are sampled from some population, for now, assume we take a single value. What is the likelihood that \\(\\mu = 100\\) and \\(\\sigma = 15\\) given our sampled IQ is \\(y = 120\\)? What if we have a vector of observations \\(n = 5\\), with mean values centered on 120? So, we want to find the distribution parameters that maximize the likelihood. We still think that the normal distribution is the most appropriate distribution as a candidate distribution. \\(y = (100, 110, 120, 130, 140)\\) "]]
