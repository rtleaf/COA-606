---
site: "bookdown::bookdown_site"
output:
  bookdown::gitbook:
    lib_dir: "book_assets"
  bookdown::pdf_book:
    keep_tex: yes
---

# The Normal Distribution
+ The normal distribution is probably the most common distribution in all of probability and statistics. 

## The Normal Probability Density Function

The probability density function for the normal distribution is defined as:

$y_i = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2}$

We can think of the model in this way (mathematical approach):

$f(X) = \frac{1}{\sigma\sqrt2\pi}e^{-(X_i-\mu)^2/2\sigma^2}$

Where the parameters (the symbols ) represent the mean, $\mu$ and the standard deviation, $\sigma$.

+ What are some of the general characteristics of this model? *Can you describe its shape?*
+ What are the parameters of the model? *These are the quantities we will estimate in the fitting process.*
+ What are the variables used in the model? *These are the observations.*

```{r,message=FALSE, echo=FALSE}
# Figures 30 and 31 (went out of order)
par(mfrow = c(1,2))
par(mar = rep(1,4))
xlim. <- c(-3.5,5.5)
plot(x = seq(xlim.[1],xlim.[2], length.out = 100), y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), 
     xlab = "", ylab = "Y", type = "l", yaxt = 'n', xlim = xlim.,
     main = "Different Means, Identical SD")
lines(x = seq(xlim.[1],xlim.[2], length.out = 100) + 1, y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), lwd = 2)
lines(x = seq(xlim.[1],xlim.[2], length.out = 100) + 2, y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100)), lwd = 3)


xlim. <- c(-4,4)

plot(x = seq(xlim.[1],xlim.[2], length.out = 100), 
     y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), mean = 0, sd = 0.5), 
     xlab = "", ylab = NA, type = "l", yaxt = 'n', xlim = xlim.,
     main = "Same Mean, Different SD")

lines(x = seq(xlim.[1],xlim.[2], length.out = 100), 
      y = dnorm(seq(xlim.[1],xlim.[2],length.out = 100), mean = 0, sd = 1.5), 
      col = 'darkgray', lwd = 2)

labels <- c(expression(paste(mu,"-3",sigma)), expression(paste(mu,"-2",sigma)))

```

Below, two distributions are plotted from the 'Standard Normal Distribution', in this formulation:

${\sigma=1}$ and ${\mu=0}$.

```{r, results='hide',message=FALSE, echo=FALSE}
set.seed(3000)
xseq<-seq(-4,4,.01)
densities<-dnorm(xseq, 0,1)
cumulative<-pnorm(xseq, 0, 1)
randomdeviates<-rnorm(1000,0,1)
par(mfrow=c(1,2), mar=c(3,4,4,2))
plot(xseq, densities,xlab="", ylab="Density", type="l",lwd=2, cex=2, main="PDF of Standard Normal", cex.axis=.8)
plot(xseq, cumulative, xlab="", ylab="Cumulative Probability",type="l",lwd=2, cex=2, main="CDF of Standard Normal", cex.axis=.8)
```

The normal distribution is an example of a continuous univariate probability distribution with infinite support. 

By infinite support, we mean that we can calculate values of the probability density function for all outcomes between $-\infty$ and $+\infty$.

For clarification, the density value on the y-axis is not the resulting probability of obtaining the sampled value.

Well, how do we get the probability from a probability density function?

We need to integrate the density function given the value of the parameters. 

So from our example distribution with mean = 0 and standard deviation = 1, we can find the probability that an observed value will bebetween 0 and 1 by finding the area shown in the image below.

```{r, results='hide',message=FALSE, echo=FALSE}
set.seed(3000)
xseq<-seq(-4,4,.01)
densities<-dnorm(xseq, 0,1)
cumulative<-pnorm(xseq, 0, 1)
randomdeviates<-rnorm(1000,0,1)
plot(xseq, densities,xlab="", ylab="Density", type="l",lwd=2, cex=2, main="PDF of Standard Normal", cex.axis=.8)
abline(v = c(0,1), lwd = 2)
```

<center>
$\int_0^1f(x;\mu,\sigma)dx = P(0 < X < 1)$
</center>

We can read this as “the integral of the probability density function between 0 and 1 (on the left-hand side) is equal to the probability that the outcome of the random variable is between zero and 1 (on the right-hand side)”.

We can cover all possible values if we evaluate the density from $-\infty$ to $+\infty$. 

Therefore the following has to be true for the function to be a probability density function:

<center>
$\int_{-\infty}^{\infty}f(x)dx = 1$.
</center>

One last thing here: The probability of the random variable being equal to a specific outcome is 0, because the integral over x values of x to x is equal to zero.

The definition of the definite integral: 

$\int_{a}^{b}f(x)dx = \underset{\rm n \rightarrow\infty}{lim}\sum_{i = 1}^n{f(x_i)\Delta{x}}$,

where $x_i = a + i\Delta{x}$ and $\Delta{x} = \frac{b-a}{n}$.

If $a - b = 0$, then $\Delta{x} = 0$.

So the integral is zero:

$\int_{a}^{b}f(x)dx = \underset{\rm n \rightarrow\infty}{lim}\sum_{i = 1}^n{0}$,

$\underset{\rm n \rightarrow\infty}{lim}\sum_{i = 1}^n{0} = \underset{\rm n \rightarrow\infty}{lim}{0}$,

$\underset{\rm n \rightarrow\infty}{lim}{0} = 0$.

```{r, eval=FALSE, echo=FALSE}
## Taking samples from a Normal Distribution

Normal distribution sampling theorem:
  
  + Sampling distribution is normal when the population distribution is normal.
+ Sample mean = population mean
+ Sample sd = population *s*
  
  ## Central limit theorem
  
  + The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size becomes larger regardless of the population distribution shape.

```

## *Z*-scores

*Z*-scores are a way to center and scale the observations to understand how many standard deviations each value is away from the mean.

*Z*-score values are measures of the 'distance' in standard deviations of a single value.

$Z_i = \frac{X_i-\mu}{\sigma}$

+ Standardizing a score with respect to the other scores in the group.
+ Expresses a score in terms of how many standard deviations it is away from the mean.
+ Converts a distribution to a z-score distribution.
+ Z-scores have mean = 0 and standard deviation = 1.

### Centering with the mean
```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
par(mfrow = c(1,2))
par(mar = rep(3,4))
set.seed(1)

plot(x = seq(-10,10, length.out = 100), 
     y = dnorm(seq(-10,10, length.out = 100), mean = 3, sd = 2), 
     ylab = "", type = "l", yaxt = 'n', lwd = 2,xlab = "",xlim = c(-10,10),
     main = "Normal Distribution with \n mean = 3, sd = 2")
samp <- c(-5.25, 0.5, 4.75)
abline(v = samp)
abline(v = 3, col = "red",lwd = 2, lty = 2)
text(x = samp, y = 0.1, labels = round(samp,2), pos = 3, col = 'blue',cex = 1)

plot(x = seq(-10,10,  length.out = 100), 
     y = dnorm(seq(-10,10,  length.out = 100), mean = 0, sd = 2), 
     ylab = "", type = "l", yaxt = 'n', lwd = 2,xlab = "",xlim = c(-10,10),
     main = "Normal Distribution with \n mean = 0, sd = 2")
samp <- samp - 3
abline(v = samp)
abline(v = 0, col = "red",lwd = 2, lty = 2)
text(x = samp, y = 0.1, labels = round(samp,2), pos = 3, col = 'blue',cex = 1)

```

### Scaling by the standard deviation
```{r, message=FALSE, echo=FALSE, warning=FALSE}
library(knitr)
library(kableExtra)
par(mfrow = c(1,2))
par(mar = rep(3,4))
set.seed(1)

plot(x = seq(-10,10,  length.out = 100), 
     y = dnorm(seq(-10,10,  length.out = 100), mean = 0, sd = 2), 
     ylab = "", type = "l", yaxt = 'n', lwd = 2,xlab = "",xlim = c(-10,10),
     main = "Normal Distribution with \n mean = 0, sd = 2")
samp <- samp
abline(v = samp)
text(x = samp, y = 0.1, labels = round(samp,2), pos = 3, col = 'blue',cex = 1)

plot(x = seq(-10,10,  length.out = 100), 
     y = dnorm(seq(-10,10,  length.out = 100), mean = 0, sd = 1), 
     ylab = "", type = "l", yaxt = 'n', lwd = 2,xlab = "",xlim = c(-5,5),
     main = "Normal Distribution with \n mean = 0, sd = 1")
samp = samp/2
abline(v = samp)
text(x = samp, y = 0.1, labels = round(samp,2), pos = 3, col = 'blue',cex = 1)

```

## Properties of *Z*-scores

A *Z*-scores, measure that quantifies how many standard deviations an observation is away from the mean of a distribution. Because of the scaling and centering we can quickly (and easily) convert any distribution of normally distributed random variables into *Z*-scores using the standard normal distribution ($\mu = 0$ and $\sigma = 1$).

The quantile value of:

+ 1.96 is the maximum 2.5% of the standard normal distribution.

+ -1.96 is the minimum 2.5% of the standard normal distribution.

Thus, 95% of z-scores lie between $-1.96 \le x_i \le 1.96$.

+ 99% of z-scores lie between $-2.58 \le x_i \le 2.58$.

+ 99.9% of them lie between $-3.29 \le x_i \le 3.29$. 

+ Let's look at a [Z Table](http://www.z-table.com/) to reinforce our understanding (Table B2 in Zar 4th edition).

### R Code
```{r,message=FALSE, echo=FALSE, warning=FALSE}
gsub('(.{1,90})(\\s|$)', '\\1\n', paste(prefix., input.content[19])) %>% cat()
```

```{r,message=FALSE, echo=FALSE, warning=FALSE}
responses.gpt[19] %>% cat()
```

## Areas under the Normal Curve for different quantile values 
```{r, results='hide',message=FALSE, echo=FALSE}
par(mfrow = c(1,1)) # This tells R to put 1 row, 1 columns
plot(x = seq(-4,4, length.out = 100), 
     y = dnorm(seq(-4,4,length.out = 100)), ylab = "", type = "l", yaxt = 'n', xlab = "Quantile", lwd = 2)
abline(v = c(-1.96, 1.96), lwd = 2, col = "green")
abline(v = c(-2.58, 2.58), lwd = 2, col = "blue")
abline(v = c(-3.29, 3.29), lwd = 2, col = "red")

#x axis labels and percentages need to be added 
```

### R Code
```{r,message=FALSE, echo=FALSE, warning=FALSE}
gsub('(.{1,90})(\\s|$)', '\\1\n', paste(prefix., input.content[17])) %>% cat()
```

```{r,message=FALSE, echo=FALSE, warning=FALSE}
responses.gpt[17] %>% cat()
```

### R Code
```{r,message=FALSE, echo=FALSE, warning=FALSE}
gsub('(.{1,90})(\\s|$)', '\\1\n', paste(prefix., input.content[18])) %>% cat()
```

```{r,message=FALSE, echo=FALSE, warning=FALSE}
responses.gpt[18] %>% cat()
```

## Checking for normality

Because many statistical tests we will be considering in this course are useful only if data are normally distributed we will need a way to assess if this is true.

Are the observations normally distributed?

### Qualitative Measures

### QQ plot

The QQ plot (Quantile Quantile) plot is a scatter plot that compares two sets of data. 

Here we will compare the observations (real-world data) to a theoretical data set that we would expect to see if the data came from a normal distribution with the sample $\bar{X}$ and $s$, the reference data.

If the distribution of the data is the same, the result will be a straight line. Each data value of the data is plotted along this reference line using the scale parameter.

So, the question is, do these data come from a population of normally distributed values.

```{r,message=FALSE, echo=FALSE}

require(kableExtra)
set.seed(1)

obs <- round(rnorm(100),3) %>% sort()
hist(obs,20)
```

First, rank your data and assign a probability to each value of the original data (empirical probability).

```{r,message=FALSE, echo=FALSE}

require(kableExtra)
set.seed(1)

obs <- round(rnorm(100),3) %>% sort()
rank.y <- order(obs)
probability <- (rank.y - 0.5)/length(obs)

df <- data.frame(obs = obs, 
                 rank.obs = rank.y, 
                 probability = probability,
                 theoretical.quantiles = round(qnorm(probability),3),
                 observed.quantiles = round((obs - mean(obs))/(sd(obs)),3))

df %>% head %>% kable() %>%
  kable_styling(full_width = F)

```

We build on the rank order of the data points to calculate the corresponding probabilty values. 

$p = \frac{rank-\frac{1}{2}}{n}$.

We then determine the theoretical quantiles, the theoretical standard normal quantiles for the calculated probability values.

The quantiles from the observed data are *Z* scores calculated from the observed data $X_i$, $\bar{X}$, and $s$.

```{r,message=FALSE, echo=FALSE}

require(kableExtra)
set.seed(1)

obs <- round(rnorm(100),3) %>% sort()
rank.y <- order(obs)
probability <- (rank.y - 0.5)/length(obs)

df <- data.frame(obs = obs, 
                 rank.obs = rank.y, 
                 probability = probability,
                 theoretical.quantiles = round(qnorm(probability),3),
                 observed.quantiles = round((obs - mean(obs))/(sd(obs)),3))



plot(df$observed.quantiles, df$theoretical.quantiles,
     xlab = "Observed quantiles", ylab = "Theoretical quantiles")

abline(a = 0, b = 1, col = 'red', lwd = 2)

```

Now lets look at the simulated data drawn from a non-normal distribution:

Question, do these data come from a normal distribution?

```{r,message=FALSE, echo=FALSE}
require(kableExtra)
set.seed(1)

par(mfrow = c(2,2))
par(mar = c(3,3,1,1))
obs <- round(c(rnorm(50),rnorm(50,mean = 2, sd = 0.3)),3) %>% sort()
rank.y <- order(obs)
probability <- (rank.y - 0.5)/length(obs)

hist(obs,20)

df <- data.frame(obs = obs, 
                 rank.obs = rank.y, 
                 probability = probability,
                 theoretical.quantiles = round(qnorm(probability),3),
                 observed.quantiles = round((obs - mean(obs))/(sd(obs)),3))

plot(df$observed.quantiles, df$theoretical.quantiles,
     xlab = "Observed quantiles", ylab = "Theoretical quantiles")
abline(a = 0, b = 1, col = 'red', lwd = 2)


# df %>% head %>% kable() %>%
#   kable_styling(full_width = F)%>%
#   row_spec(5, bold = T, color = "white", background = "red")

plot(df$rank.obs, (df$theoretical.quantiles - df$observed.quantiles), xlab = "Rank",ylab = "Obs - Exp")
abline(h = 0)

```

### R Code
```{r,message=FALSE, echo=FALSE, warning=FALSE}
gsub('(.{1,90})(\\s|$)', '\\1\n', paste(prefix., input.content[16])) %>% cat()
```

```{r,message=FALSE, echo=FALSE, warning=FALSE}
responses.gpt[16] %>% cat()
```

## Maximum Likelihood 

+ The primary source for the following lecture material are primarily derived from I.J. Myung's paper "Tutorial on maximum likelihood estimation". Journal of Mathematical Psychology 47 (2003) 90-100. The interested reader is directed to review this paper.

+ In this lecture we will understand an alternative to parameter estimation using least-squares estimation, maximum likelihood estimation (MLE).

+ MLE is a preferred method of parameter estimation in statistics and is a *general* parameter estimation approach, in particular in non-linear modeling and/or with non-normally distributed data. Because of the prevelance of non-normally distributed data in the natural sciences (e.g. data from counting, categorical data, skewed ratio data) an examination and familiarization of this approach is useful.

### General Exprimental Approach

+ Because many phenomenon of interest are not directly observable, we formulate hypotheses to test and these hypothesis are stated in terms of probability using statistical models.

+ The goal of statistical modeling is to understand underlying processes by testing the viability (e.g. quality, robustness) of the model.

Our method: 

+ Specify a statistical model
+ Collect data
+ Evaluate how well the model fits the data by:
1. Parameter estimation

2. Evaluating goodness of fit

### Two approaches to parameter estimation:

1) LSE (least-squares estimation), for normally-distributed data

2) MLE, a general approach for parameter estimation

### The probability density function

The goal of data analysis is to identify the population that is most likely to have generated the sample - i.e. we will estimate the parameters of the candidate model that will produce these observations.

The data vector $y = (y_1, y_2, ..., y_m)$ is a random sample from a population distributed in some unknown way. 

Populations are identified using a probability distribution and unique values of the parameters - As the parameter changes in value, different probability distributions are generated.

Let $f(y|w)$ denote the *probability density function* (PDF) that specifies the probability of observing data vector *y* given the parameter *w*. 

The parameter $w = (w_1, ..., w_k)$ is a vector defined on a multi-dimensional parameter space.

+ For example, if the PDF is normal, $w = (\mu, \sigma)$

+ If the PDF is a *t* distribution, $w = (d.f.)$

+ Different distributions are defined using different parameters (and different mathematical formulations), so *w* is distribution-specific.

If we have specified a distribution that has a certain set of parameters, for example:

```{r echo= F}
par(mfrow = c(2,2))
q.val <- seq(-4,4,0.1)
plot(q.val, dnorm(q.val), type = 'l', lwd = T, main = "Normal distribution",xlab="",ylab="f(x)")

curve(dweibull(x, scale=2.5, shape=1.5),from=0, to=15, main="Weibull distribution",xlab="",ylab="f(x)")

plot(seq(0,25,.1), dlnorm(seq(0,25,.1), 1, 0.6),type="l",xlab="",ylab="f(x)", main="Lognormal distribution")

plot(seq(-3,3,.1), dt(seq(-3,3,.1), 30),type="l",xlab="",ylab="f(x)", main="t distribution")

```

We can use a given probability distribution and parameter set to determine the probability of obtaining a value in a population.

Example:

Children's IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What is the probability of a randomly selected child having an IQ between 80 and 120?

```{r echo= F}
mean=100; sd=15
lb=80; ub=120

x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)

plot(x, hx, type="n", xlab="IQ Values", ylab="",
  main="Normal Distribution", axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red")

area <- pnorm(ub, mean, sd) - pnorm(lb, mean, sd)
result <- paste("P(",lb,"< IQ <",ub," | mu = 100, sd = 15)")
mtext(result,3)
axis(1, at=seq(40, 160, 20), pos=0)

```

In this case the area under the curve is 0.818 or 81.8% of the integral of the distribution from $-\infty$ to $+\infty$. So, if we take 100 random draws from the population of children's IQs, we will get values of IQ > 80 and < 120, 81 to 82 times...

Let's examine the statement:

$p(80 < IQ < 120$ | $\mu = 100, \sigma = 15) = 0.818$.

We are stating that the *probability* of randomly selecting an IQ observation the variable characteristics *given* the parameter values *is* 0.818.

If we are interested in finding probabilities of students with different variable characteristics (different IQ values), then we will change the *left* side of the equation.

For example:

$p(IQ < 65$ | $\mu = 100, \sigma = 15)$

or 

$p(IQ > 120$ | $\mu = 100, \sigma = 15)$

The right side of the equation does not change because it describes the fixed shape of the distribution of the population that "creates" the observations. So when we are investigating probability of an event we are quantifying the integral of the curve for the given parameter set bounded by the *left* side of the equation. We change the left side to derive new probability values.


```{r echo= F}
par(mfrow = c(1,2))
mean=100; sd=15
lb=0; ub=65

x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)

plot(x, hx, type="n", xlab="IQ Values", ylab="",
  main="", axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red")

area <- pnorm(ub, mean, sd) - pnorm(lb, mean, sd)
result <- paste("P(","IQ <",ub," | mu = 100, sd = 15)")
mtext(result,3)
axis(1, at=seq(40, 160, 20), pos=0)

mean=100; sd=15
lb=120; ub=1000

x <- seq(-4,4,length=100)*sd + mean
hx <- dnorm(x,mean,sd)

plot(x, hx, type="n", xlab="IQ Values", ylab="",
  main="", axes=FALSE)

i <- x >= lb & x <= ub
lines(x, hx)
polygon(c(lb,x[i],ub), c(0,hx[i],0), col="red")

area <- pnorm(ub, mean, sd) - pnorm(lb, mean, sd)
result <- paste("P(",lb,"< IQ"," | mu = 100, sd = 15)")
mtext(result,3)
axis(1, at=seq(40, 160, 20), pos=0)

```

We can determine the probability of obtaining $p(y_1, y_2, ..., y_m)$ | $w)$ if the observations are independent.

Think about taking multiple random draws from the population described by the parameter set *w*. An analog here is thinking about coin flipping, the probability of realizing specific outcomes from multiple trials is determined by multiplication (e.g. probability of observing two "tails" flips is the product of the independent single observations).

So, the notation below is *Pi* (product) notation. This is used in mathematics to indicate repeated multiplication.

$p(y_1, y_2, ..., y_m)$ | $w) = p(y_1$| $w) \times p(y_2$| $w) \times ...p(y_n$| $w)$

$p(y_1, y_2, ..., y_m)$ | $w) = \prod_{i = 1}^{n}{p(y_i | w)}$ 

### The likelihood function

Now we will discuss the likelihood function that is derived by considering that the data are fixed and that the shape of the (parent) distribution is random.

In this approach we are evaluating the inverse problem of the PDF. Specifically, given the observed data and a model of interest, we are interested in finding the unique parameter vector *w*, among all the possible combinations of parameters that is most likely to have produced the data. 

We have already observed the data and now want to define the likelihood function by reversing the roles (i.e. what is random what is fixed) of the data vector *y* and the parameter vector *w*.

So, we focus on $L(w|y)$. This represents the likelihood of the parameter *w* given the observed data *y*; and as such is a function of *w*.

So, the data are fixed, and we modify the parameter vector *w* (in the case of IQ, $\mu$ and $\sigma$)

Assume we have some vector of observed data *y*, these are sampled from some population, for now, assume we take a single value.

```{r echo= F, eval = F}

mu.val <- 32
sig.val <- 2.5

rand.draws <- (34)

likhd <- c()
#best.guess.mu <- seq(mu.val - mu.val*0.5, mu.val + mu.val*0.5)
best.guess.mu <- mu.val
best.guess.sig <- sig.val
for (k in 1:length(best.guess.mu)) {
prob <- dnorm(rand.draws,best.guess.mu[k],best.guess.sig) 
# likhd[k] <- (sum(log(prob))) }
likhd[k] <- (sum((prob))) }

likhd
```

What is the likelihood that $\mu = 100$ and $\sigma = 15$ given our sampled IQ is $y = 120$?

```{r echo= F}

mu.val <- 100
sig.val <- 15

rand.draws <- seq(50,150)

likhd <- c()
best.guess.mu <- mu.val
best.guess.sig <- sig.val
for (k in 1:length(rand.draws)) {
prob <- dnorm(rand.draws[k],best.guess.mu,best.guess.sig) 
likhd[k] <- (sum((prob))) }

plot(rand.draws, likhd, type = "l",
     main = paste("L(mu = 100, sigma = 15 | y = 120)"))
segments(x0 = 120, y0 = 0, y1 = likhd[which(rand.draws == 120)])
segments(x0 = -100, x1 = 120, y0 = likhd[which(rand.draws == 120)], y1 = likhd[which(rand.draws == 120)])

```

What if we have a vector of observations $n = 5$, with mean values centered on 120? So, we want to find the distribution parameters that maximize the likelihood. 
We still think that the normal distribution is the most appropriate distribution as a candidate distribution.

$y = (100, 110, 120, 130, 140)$

```{r echo= F}

mu.val <- 100
sig.val <- 15

rand.draws <- c(100, 110, 120, 130, 140)

likhd <- n.likhd <- c()
best.guess.mu <- seq(90,150,by = 1)
best.guess.sig <- sig.val
for (k in 1:length(best.guess.mu)) {
prob <- dnorm(rand.draws,best.guess.mu[k],best.guess.sig) 
likhd[k] <- (sum(log(prob))) 
n.likhd[k] = (sum((prob))) 
}

plot(best.guess.mu,-likhd, ylab = "-Log (Likelihood)",
     main = paste("L(mu = 100, sigma = 15 | y = (100, 110, 120, 130, 140))"))
abline(v=120)
```
