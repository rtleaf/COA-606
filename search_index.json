[["index.html", "1 Biometry for the Coastal Sciences (COA 606) 1.1 Course Description and Objectives 1.2 At the conclusion of this course: 1.3 Course Materials - Online and in the Caylor Library 1.4 Course Scheduling 1.5 Course Workload Statement 1.6 Course Evaluation 1.7 Course Evaluation - Participation and Attendence 1.8 Homework policy 1.9 Grading scale 1.10 Exam Bank 1.11 Academic Support Resources 1.12 Mental Well-Being Statement 1.13 Nondiscrimination Statement 1.14 Confidentiality and Mandatory Reporting 1.15 Academic Integrity", " 1 Biometry for the Coastal Sciences (COA 606) Date: 21 September 2023 Instructor: Dr. Robert Leaf Office: GCRL Oceanography 119 Office Hours: I have an open door policy and welcome your questions and concerns. Stop in to see me anytime or make an appointment and lets meet to discuss your needs. Unfortunately, sometimes I have deadlines and travel and will not be able to accommodate “drop in” students every time but will make every reasonable effort to accomodate your schedule. Email: robert.leaf@usm.edu Course Meeting Day and Time: MW, 4:00 to 5:15 PM Repository of Readings: https://tinyurl.com/mrx7hdxe Repository of Videos and Transcripts: https://tinyurl.com/bde2ejnc Repository of HW: https://tinyurl.com/3nvwe38h In-class assignment: https://rtleaf.github.io/coa-606-in-class 1.1 Course Description and Objectives This course addresses basic approaches to experimental design, statistical analysis, and presentation of quantitative information. 1.2 At the conclusion of this course: By the end of this course, you will be able to: Define what comprises the field of statistical analysis. What is meant by random samples, random sampling, and understanding how these aspects of sampling are critical for description and inference. Explain the differences and similarities among variables, statistics, and parameters. Define a cumulative density function and a probability density function and describe several common distributional families (e.g., normal, binomial, chi-square). Understand and be familiar with the applications of: Frequency distributions Power analysis Summary statistics as a tool for describing data Means comparison techniques for testing hypotheses ANOVA comparison techniques for testing hypothesis Linear regression techniques for describing relationships between random variables. Make appropriate decisions as a part of a statistical data analysis. 1.3 Course Materials - Online and in the Caylor Library Experimental Design and Data Analysis for Biologists by G. Quinn and M. Keough (2002) Biostatistical analysis 4th edition, by Jerrold Zar, Prentice Hall (1999), ISBN-10:0131008463, ISBN-13:978-0131008465 1.4 Course Scheduling Day Activity Reading Monday, August 21, 2023 Introduction Syllabus, QK 1.1, 1.2 Wednesday, August 23, 2023 Univariate Data Zar Ch 1, Manikandan 2011, QK 1.3, 1.5 Monday, August 28, 2023 Best Practices and 4, 5, and 6 R Intro Broman and Wu 2018 Wednesday, August 30, 2023 In class assignment Monday, September 4, 2023 Labor Day Holiday Wednesday, September 6, 2023 Sampling Zar Ch 2, Banerjee and Chaudhury 2010, QK 2.1, QK 7.1, 7.2 Monday, September 11, 2023 Measures of Central Tendency Zar Ch 3 Wednesday, September 13, 2023 Measures of Dispersion Zar Ch 4 Monday, September 18, 2023 In class assignment Wednesday, September 20, 2023 The Normal Distribution Zar Ch 6, QK 2.2 Monday, September 25, 2023 In class assignment Wednesday, September 27, 2023 Exam 01 Monday, October 2, 2023 Confidence intervals and 13.NHST Zar Ch 7, QK 2.3, QK 3.1 Wednesday, October 4, 2023 One- and two-tailed Tests and 15. Statistical Power Zar Ch 8, Benjamin et al., Pernet 2015, Steidl et al. 1997, Trafimow, QK 3.2, 3.4 Monday, October 9, 2023 In class assignment Wednesday, October 11, 2023 Bivariate Relationships Zar Ch 19, Aggarwal and Ranganathan 2016, QK 5.1 Monday, October 16, 2023 In class assignment Wednesday, October 18, 2023 Exam 02 Monday, October 23, 2023 Fall Break Wednesday, October 25, 2023 Linear Regression Zar Ch 17, 18, Smith 2018, QK 5.2, 5.3 Monday, October 30, 2023 Multiple Linear Regression Zar Ch 20, QK 6.1 Wednesday, November 1, 2023 In class assignment Monday, November 6, 2023 Comparing Two Means using t-tests Wednesday, November 8, 2023 ANOVA Zar Ch 10. 11, Morley 1983, QK 8.1, 8.3 Monday, November 13, 2023 In class assignment Wednesday, November 15, 2023 Exam 03 Monday, November 20, 2023 Two-Way and Multiway Independent ANOVA Zar Ch 12, 13, 14, QK 4.3, 9.2 Wednesday, November 22, 2023 In class assignment Monday, November 27, 2023 Thanksgiving Break Wednesday, November 29, 2023 Goodness of Fit and Contingency Tables Zar 22, 23 Monday, December 4, 2023 In class assignment Monday to Thursday, December 5 to 8, 2023 Final Exam Week 1.5 Course Workload Statement The expectation of the University of Southern Mississippi is that students should spend approximately 2 to 3 hours outside of class each week for every hour in class working on reading, assignments, studying, and other work for the course. Time management is thus critical for student success. All students should assess their personal circumstances and talk with their advisors about the appropriate number of credit hours to take each term. Resources for academic support can be found at https://www.usm.edu/success. 1.6 Course Evaluation Percentage Letter Grade 93-100 A 90-92 A- 86-89 B+ 83-85 B 80-82 B- 76-79 C+ 73-75 C 70-72 C- 66-69 D+ 63-65 D 60-62 D- &lt; 60 F 1.7 Course Evaluation - Participation and Attendence Attendance is not required, nor is participation. If the students have other obligations, they do not need to let me know, and I will never question your attendance. No questions are asked about missed classes, and nothing we do in class, other than exams, are graded. 1.8 Homework policy There is no due date for the HW, you may turn it in anytime. Graded HW will be returned withing 5 working days of its receipt. Prepare your HW as an organized and clear hard copy, make it easy for me to give you full credit. However, no work will be accepted after 4 PM Monday, December 4, 2023. 1.9 Grading scale Evaluation type Number Points per item Total points HW 3 20 60 Mid-term exams 2 25 50 Final exam 1 50 50 1.10 Exam Bank The bank of exams and study material can be found here: https://tinyurl.com/3d8akhts 1.11 Academic Support Resources If a student knows or believes that they have a disability which is covered by the Americans with Disabilities Act (ADA) and makes them eligible to receive classroom accommodations, they should contact the Office for Disability Accommodations (ODA) for information regarding the registration process. Disabilities covered by the ADA may include but are not limited to ADHD, learning disabilities, psychiatric disabilities, physical disabilities, chronic health disorders, temporary illnesses or injuries and pregnancies. Students should contact ODA if they are not certain whether their documented medical condition qualifies for ODA services. Students are only required to disclose their disability to the Office for Disability Accommodations. All information submitted to ODA by the student is held with strict confidentiality. 1.12 Mental Well-Being Statement I recognize that students sometimes experience challenges that make learning difficult. If you find that life stressors such as anxiety, depression, relationship problems, difficulty concentrating, alcohol or drug problems, or other stressful experiences are interfering with your academic or personal success, consider contacting Student Counseling Services on campus at 601-266-4829. More information is also available at https://www.usm.edu/student-counseling- services. All students are eligible for free, confidential individual or group counseling services. In the event of emergency, please call 911 or contact the counselor on call at 601-606-HELP (4357). 1.13 Nondiscrimination Statement The University of Southern Mississippi offers to all persons equal access to educational, programmatic and employment opportunities without regard to age, sex, sexual orientation, disability, pregnancy, gender identity, genetic information, religion, race, color, national origin, and/or veteran status pursuant to applicable state and federal law. 1.14 Confidentiality and Mandatory Reporting As an instructor, one of my responsibilities is to help create and maintain a safe learning environment. I have a mandatory reporting responsibility related to my role as a faculty member. I am required to share information regarding sexual misconduct or information about a crime that may have occurred on the USM campus with certain University officials responsible for the investigation and remediation of sexual misconduct. The information will remain private and will only be shared with those officials necessary to resolve the matter. If you would like to speak in confidence, resources available to students include Confidential Advisors with the Shafer Center for Crisis Intervention, the Counseling Center, and Student Health Services. More information on these resources and University Policies is available at https://www.usm.edu/sexual-misconduct. 1.15 Academic Integrity All students at the University of Southern Mississippi are expected to demonstrate the highest levels of academic integrity. Forms of academic dishonesty include cheating (including copying from others work), plagiarism (representing another persons words or ideas as your own; failure to properly cite the source of your information, argument, or concepts), falsification of documents, disclosure of or use of test material or other assignment content to another student, submission of the same paper or other assignment to more than one class without the explicit approval of all faculty members involved, unauthorized academic collaboration with others, conspiracy to engage in academic misconduct. "],["univariate-data.html", "2 Univariate Data 2.1 Variables and Frequency Distributions 2.2 Categorical Variable Types 2.3 Continuous Variables", " 2 Univariate Data Univariate is a describes a type of data which consists of observations of only a single characteristic. 2.1 Variables and Frequency Distributions Zar 4th ed. (1.1, 1.3, 1.4) The properties and characteristics of an object that can assume two or more different values are called variables. These are the values that comprise our data. Examples: Heights of graduate students Foot size of toddlers at 10 days old Taxomonic diversity (number of species) in a habitat We need to understand the characteristics of variables prior to analysis because different types of measurements will require different methods of analysis. The next step after data collection is to organize the data into a meaningful form so that similarities and contrasts can be seen easily. One of the common methods for organizing univariate data is to construct frequency distributions either in a table or as a figure. The frequency distribution is an organized representation of the number of observations in each category on the scale of measurement. These allow researchers to have a glance at the entire data conveniently. Frequency distributions allow the experimenter to understand if there are observations that are high or low and also whether observations are concentrated in one area or spread out across the entire scale. 2.2 Categorical Variable Types 2.2.1 Binary variables There are only two categories, e.g. dead or alive, present or absent, positive or negative (e.g. for a disease), the value of some quantity of interest is zero or positive, or the value of some quantity of interest exceeds some threshold value. 2.2.2 Nominal variables There are more than two categories, e.g. Whether the subject is an omnivore, vegetarian, vegan, or carnivore. The subject’s taxonomic group (e.g. genera, species, phyla), 2.2.3 Ordinal variable Similar to a nominal variable but the categories are ordered. Whether people got a fail, a pass, a merit or a distinction in their exam. Intensity of infection (e.g. none, mild, moderate, severe) 2.2.4 R Code ## Provide code in R (using base R functionality) to create a data frame with three colums, ## composed of binary variables, nominal variables, and ordinal variables (e.g. “low ## income”,”middle income”,”high income”). One for each column in a data frame. ## ``` ## # Create binary variable ## # A binary variable is created and stored as a vector with values 1 and 0 ## binary_var &lt;- c(1, 0, 0, 1, 1) ## ## # Create nominal variable ## # A nominal variable is created and stored as a factor with three levels &#39;low income&#39;, &#39;middle income&#39;, and &#39;high income&#39; ## nominal_var &lt;- factor(c(&#39;low income&#39;, &#39;middle income&#39;, &#39;middle income&#39;, &#39;high income&#39;, &#39;low income&#39;), levels = c(&#39;low income&#39;, &#39;middle income&#39;, &#39;high income&#39;)) ## ## # Create ordinal variable ## # An ordinal variable is created and stored as an ordered factor with three levels &#39;Low&#39;, &#39;Medium&#39;, and &#39;High&#39; ## ordinal_var &lt;- ordered(c(&#39;Low&#39;, &#39;Medium&#39;, &#39;Low&#39;, &#39;High&#39;, &#39;Medium&#39;), levels = c(&#39;Low&#39;, &#39;Medium&#39;, &#39;High&#39;)) ## ## # Create data frame ## # A data frame is created by combining the binary, nominal, and ordinal variables using the &#39;data.frame()&#39; function ## df &lt;- data.frame(binary_var, nominal_var, ordinal_var) ## ## # View the data frame ## # The contents of the data frame are displayed in the console by calling the variable name &#39;df&#39; ## df ## ``` 2.2.5 Frequency Distributions of a Categorical Variable A tally of how frequently occurring a value is among categories. 2.2.6 R Code ## Provide code in R (using base R functionality) to plot ordinal, binary, and nominal ## variables using a barplot ## ```R ## # create example data ## ordinal_var &lt;- c(3, 2, 4, 1, 5) # create a vector of values for an ordinal variable ## binary_var &lt;- c(TRUE, FALSE, TRUE, TRUE, FALSE) # create a vector of values for a binary variable ## nominal_var &lt;- c(&#39;Red&#39;, &#39;Green&#39;, &#39;Blue&#39;, &#39;Green&#39;, &#39;Red&#39;) # create a vector of values for a nominal variable ## ## # create barplot for ordinal variable ## barplot(ordinal_var, names.arg = c(&#39;Item 1&#39;, &#39;Item 2&#39;, &#39;Item 3&#39;, &#39;Item 4&#39;, &#39;Item 5&#39;), # create a barplot of the ordinal variable with names for each bar ## main = &#39;Ordinal Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Value&#39;) # add a title and axis labels to the plot ## ## # create barplot for binary variable ## binary_table &lt;- table(binary_var) # create a frequency table of the binary variable ## barplot(binary_table, names.arg = c(&#39;FALSE&#39;, &#39;TRUE&#39;), # create a barplot of the frequency table with names for each bar ## main = &#39;Binary Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Count&#39;)```R ## # create barplot for nominal variable ## nominal_table &lt;- table(nominal_var) # create a frequency table of the nominal variable ## barplot(nominal_table, main = &#39;Nominal Variable&#39;, xlab = &#39;Item&#39;, ylab = &#39;Count&#39;) # create a barplot of the frequency table and set a title and axis labels ## ``` ## ## Overall, this R code creates three barplots to display different types of variables - ordinal, binary, and nominal. The `barplot` function is used to create each barplot, with customization options such as `names.arg`, `main`, `xlab`, and `ylab` used as needed. The `table` function is used to create frequency tables of the binary and nominal variables to feed into the `barplot` function. 2.3 Continuous Variables Equal intervals on the variable represent equal differences in the property being measured, e.g. the difference between 6 and 8 is equivalent to the difference between 13 and 15, Density or frequency of organisms in a transect or at a sampling station, Body Mass Index or some measure of condition of an organism. 2.3.1 Frequency Distribution of a Continuous Variable A question often asked: What interval to choose? Your knowledge of the domain will guide this. As a first approach, the width of the class can be determined by dividing the range of observations by the number of classes. The following are some guidelines regarding class widths: 1.) It is advisable to have equal class widths. Unequal class widths should be used only when large gaps exist in data. 2.) The class intervals should be mutually exclusive and nonoverlapping. 3.) Open-ended classes at the lower and upper side (e.g., &lt; 10, &gt; 100) should be avoided, why? Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data Phosphorous concentration Frequency 8.1 to 8.2 2 8.2 to 8.3 6 8.3 to 8.4 8 8.4 to 8.5 11 8.5 to 8.6 17 8.6 to 8.7 17 8.7 to 8.8 24 8.8 to 8.9 18 8.9 to 9.0 13 9.0 to 9.1 10 9.1 to 9.2 4 Total frequency = 130 = n 2.3.2 Cumulative Distribution of a Continuous Variable Determination of the Amount of Phosphorous in Leaves: A Frequency Table of Continuous Data Cumulative Frequency Phosphorous concentration Frequency Starting with Low Values Starting with High Values 8.15 to 8.25 2 2 130 8.25 to 8.35 6 8 128 8.35 to 8.45 8 16 122 8.45 to 8.55 11 27 114 8.55 to 8.65 17 44 103 8.65 to 8.75 17 61 86 8.75 to 8.85 24 85 69 8.85 to 8.95 18 103 45 8.95 to 9.05 13 116 27 9.05 to 9.15 10 126 14 9.15 to 9.25 4 130 4 Total frequency = 130 = n 2.3.3 R Code ## Provide code in R (using base R functionality) to plot a frequency histogram using ## simulated data from a continuous distribution. ## ``` ## # Set seed for reproducibility ## set.seed(123) ## ## # Generate simulated data from a normal distribution ## data &lt;- rnorm(1000, mean = 10, sd = 2) ## ## # Plot frequency histogram ## hist(data, freq = TRUE, breaks = 20, xlim = c(0, 20), ## main = &#39;Frequency Histogram of Simulated Data&#39;, # Add main title to plot ## xlab = &#39;Data Values&#39;, ylab = &#39;Frequency&#39;) # Add axis labels to plot ## ``` ## In this example, we set the seed to ensure the same results every time the code is run. We then simulate 1000 data points from a normal distribution with a mean of 10 and standard deviation of 2. Finally, we use the `hist()` function with the `freq = TRUE` argument to plot a frequency histogram with 20 breaks and set the x-axis limits to 0 and 20. The main title and axis labels are also added for clarity. 2.3.4 Box and Whisker Plot of a Continuous Variable This graph, first described by Tukey, can also be used to illustrate the distribution of data. There is a vertical or horizontal rectangle (box), the ends of which correspond to the upper and lower quartiles (75th and 25th percentile, respectively). Hence the middle 50% of observations are represented by the box. The length of the box indicates the variability of the data. The line inside the box denotes the median (sometimes marked as a plus sign). The position of the median indicates whether the data are skewed or not. If the median is closer to the upper quartile, then they are negatively skewed and if it is near the lower quartile, then positively skewed. 2.3.5 R Code ## Provide code in R (using base R functionality) to create a Box and Whisker Plot of a ## Continuous Variable. ## ```R ## # Create a continuous variable data ## continuous_var &lt;- rnorm(100) # Create a variable &quot;continuous_var&quot; that stores 100 random normal numbers generated by rnorm ## ## # Create a boxplot of the continuous variable ## boxplot(continuous_var, main=&#39;Boxplot of Continuous Variable&#39;, # Create a box and whisker plot of the continuous variable, with a main title of &#39;Boxplot of Continuous Variable&#39; ## ylab=&#39;Value&#39;, col=&#39;steelblue&#39;) # Set the y-axis label to &#39;Value&#39; and the color of the boxes to &#39;steelblue&#39; ## ``` 2.3.6 Violin Plots Violin plots are an alternative to box plots that solves the issues regarding displaying the underlying distribution of the observations, as these plots show a kernel density estimate of the data. same summary statistics as box plots: the white dot represents the median. the thick gray bar in the center represents the interquartile range. the thin gray line represents the rest of the distribution, except for points that are determined to be “outliers” using a method that is a function of the interquartile range. On each side of the gray line is a kernel density estimation to show the distribution shape of the data. Wider sections of the violin plot represent a higher probability that members of the population will take on the given value; the skinnier sections represent a lower probability. 2.3.7 R Code ## Provide code in R (using base R functionality) to create a violin plot in R plotting using ## the mtcars dataset using package vioplot. ## ``` ## # Load the vioplot package and the mtcars dataset ## library(vioplot) ## data(mtcars) ## ## # Create a violin plot of the mpg variable ## vioplot(mtcars$mpg, main = &#39;Violin Plot of MPG in mtcars&#39;, ylab = &#39;MPG&#39;) ## ``` ## The above code first loads the `vioplot` package and the `mtcars` dataset. Then, it creates a violin plot of the `mpg` variable using the `vioplot()` function. The `main` argument adds a title to the plot and the `ylab` argument labels the y-axis as &#39;MPG&#39;. "],["best-practices-for-data-management.html", "3 Best Practices For Data Management 3.1 Consistency and conventions 3.2 RStudio and R", " 3 Best Practices For Data Management Broman and Woo (2018) highlight the challenges and best practices for using spreadsheets. The design principles that they outline are useful for your work in general and specialized statistical programs (i.e. Python, R, and many others). Spreadsheets (Google Sheets, MS Excel) continue to to be a primary way for data storage, (some) analysis, and (some) visualization even for those using specialized statistical programs. Highly recommended is a workflow in which raw original data is never modified. Instead, it is imported and modified as a part of the workflow. Recommended workflow of data analysis (From Wickham and Grolemnund “R for Data Science”, published January 2023, 2nd edition). 3.1 Consistency and conventions Organization is critical for reproducible research and archiving. A common scenario in your scientific practice is to have multiple time-sensitive projects occurring simultaneously, organization is critical. Consistency is critical for: Recording categorical variables (a type of variable we will learn about in this class), Use a consistent fixed code for any missing values (e.g. ‘NA’ or an unfilled/empty cell). Use consistent variable (column) names. Use a consistent data layout in multiple files (same column names). This allows data to be merged in a seamless way. Use a consistent format for all dates and times (e.g. YMD, DMY). Use consistent phrases in your notes. Notes are data. So, treating these as variables that have a binary, nominal, or ordinal value will allow you to later evaluate these quantitatively. Be careful about extra spaces within cells. Again, most (all?) software will read these as character codes that must be post-processed for analysis. One variable is recorded in each cell (remember, in our scheme, a comment is a variables). Strive for a rectangular data layout. Avoid font and cell colors as annotation. Use .csv or some other file back up. Following the approaches above will allow you to better manage your projects (including work done in this class!). 3.2 RStudio and R Is a developer environment for coding, data management, and version control. Multi-pane structure Script editor Console Environment and history Files, plots, packages and help 3.2.1 R installation Open an internet browser and go to www.r-project.org. Click the “download R” link in the middle of the page under “Getting Started.” Select a CRAN location (a mirror site) and click the corresponding link. Click on the “Download R for (Mac) OS X” link at the top of the page. Click on the file containing the latest version of R under “Files.” Save the .pkg file, double-click it to open, and follow the installation instructions. Now that R is installed, you need to download and install RStudio. 3.2.2 RStudio installation Go to www.rstudio.com and click on the “Download RStudio” button. Click on “Download RStudio Desktop.” Click on the version recommended for your system, or the latest Mac version, save the .dmg file on your computer, double-click it to open, and then drag and drop it to your applications folder. 3.2.3 Script Editor Allows the user to create multi-line code. It is here that you will develop your code and send it to the console. You will save your scripts as appropriately titled .r files. To create a new R script you can either go to File -&gt; New -&gt; R Script, or click on the icon with the “+” sign and select “R Script”, or simply press Ctrl+Shift+N. Make sure to save the script. 3.2.4 Console The console is where you can type commands and see output. &gt; is the R Prompt Symbol: You should see the R prompt symbol in your console. If you don’t see the prompt, you cannot execute code. + is the R Prompt Symbol meaning you have unfinished code from the previous line. This often occurs if you have an open (unmatched) parentheses or a multiline input. Pressing the ESC will return the &gt; symbol to your console. 3.2.5 Files tab The files tab shows the directory structure and allows GUI manipulation of directories. 3.2.6 Plots tab The plots tab will show all your figures and it is possible to scroll through multiple plots windows. 3.2.7 Packages tab Provides a list of installed packages and a mechanism to load new ones. Here, you can install packages here or using the command line (in the console). # In the console type: install.packages(&quot;package name in quotes&quot;) # Example: install.packages(&quot;dplyr&quot;) For some packages you will see that “dependencies” are installed when the desired package is installed. These are the packages that the installed package needs in order to run some or all of the functions. 3.2.8 Help tab This tab will be automatically selected whenever you run help code in the Console. 3.2.9 History tab The history tab keeps a record of all previous commands. This can be useful when testing and running processes. To use archived code here, select all and click on the “To Source” icon, a window on the left will open with the list of commands. 3.2.10 Environment tab Displays data, their classes, and dimensions in your workspace. "],["working-in-r.html", "4 Working in R 4.1 Arithmetic operators 4.2 Arithmetic operators practice 4.3 Missing values (NA) 4.4 Dealing with missing values 4.5 Dealing with problematic values", " 4 Working in R 4.1 Arithmetic operators Symbol Operation + addition - subtraction * multiplication / division ^ exponentiation PEMDAS applies when writing R-code # Arithmetic operators in action: 5 + 5 / 2 3 * 2^2 (3*2)^2 # Arithmetic operators using objects: z &lt;- 5 w &lt;- c(3,7,9,2) s &lt;- w[3] z + s 4.2 Arithmetic operators practice Using what you know about parenthesis and PEMDAS, in one line of code do each of the following: Number Excercise 1. Assign the variable x to be a vector containing the values 5,5,6,2 2. Assign the variable y to be a vector containing the values 3,3,1,7 3. Add x and y 4. Substract y from x 5. Assign d as y divided by x 3. Multiply z by s then add five (z and s from last practice) 4. Add 5 to z then multilply by s 5. Take z to the fifth power and then add 2 6. Divide s by three, then add 33, then take that sum to the 0.5 power 4.3 Missing values (NA) # is.na tests for missing values dat.1 &lt;- c(-1,NA,1,1,-1) dat.1 + 2 dat.1 + rep(2, length(dat.1)) 4.4 Dealing with missing values In R, missing values are represented by “NA”. Undefined values (like dividing by zero) are represented by “NaN”, not a number. Often missing values are represented with numbers: -1, 99, -9999, etc. This is obviously a problem and should be avoided. You will likely need to use indexing prior using arithmetic operations to replace these values. The “is.na” function and the “na.rm”” argument: Sometimes we do not know whether there are missing values in our data. We can use the is.na function to test for missing values: # is.na tests for missing values dat.1 &lt;- c(-1,NA,1,1,-1) is.na(dat.1) which(is.na(dat.1)) We can use the logical na.rm argument to remove missing values from our data prior to executing the funciton: dat.1 &lt;- c(-1,NA,1,1,-1) mean(dat.1) mean(dat.1, na.rm = T) 4.5 Dealing with problematic values Sometimes we code no data as -1 and that can really screw things up. R does not know that -1 means “no data”. However we can replace the -1 with NA. There are many ways to do this, but here is a one way. # In this example, -1 is coded as a missing data field. Think back to our logical arguments and subsetting exercises. dat.2 &lt;- c(2,-1,3,4,5) dat.2 == -1 dat.2[dat.2 == -1] dat.2[dat.2 == -1] &lt;- NA dat.1 &lt;- c(2,NA,3,4,5) # is.na tests for missing values is.na(dat.1) # returns the element(s) number in the vector that is NA which(is.na(dat.1)) "],["data-classes-in-r.html", "5 Data Classes in R 5.1 Numeric 5.2 Integer 5.3 Character 5.4 Logical 5.5 Factor 5.6 List 5.7 Vectors 5.8 Matrix 5.9 DataFrame", " 5 Data Classes in R There are various kinds of R-objects or data structures: Vectors Lists Matrices Arrays Factors Data Frames Let’s first understand some of the basic datatypes on which the R-objects are built like Numeric, Integer, Character, Factor, and Logical. 5.1 Numeric num &lt;- 1.2 print(num) class(num) 5.2 Integer Integer: Numbers that do not contain decimal values have a data type as an integer. However, to create an integer data type, you explicitly use as.integer() and pass the variable as an argument. int &lt;- as.integer(2.2) print(int) class(int) 5.3 Character As the name suggests, it can be a letter or a combination of letters enclosed by quotes is considered as a character data type by R. char &lt;- &quot;datacamp&quot; print(char) class(char) char &lt;- &quot;12345&quot; class(char) 5.4 Logical Logical: A variable that can have a value of True and False like a boolean is called a logical variable. log_true &lt;- TRUE print(log_true) class(log_true) log_false &lt;- FALSE class(log_false) 5.5 Factor Factor: They are a data type that is used to refer to a qualitative relationship like colors, good &amp; bad, course or movie ratings, etc. They are useful in statistical modeling. fac &lt;- factor(c(&quot;good&quot;, &quot;bad&quot;, &quot;ugly&quot;,&quot;good&quot;, &quot;bad&quot;, &quot;ugly&quot;)) print(fac) class(fac) levels(fac) nlevels(fac) class(levels(fac)) 5.6 List Unlike vectors, a list can contain elements of various data types and is often known as an ordered collection of values. It can contain vectors, functions, matrices, and even another list inside it (nested-list). lis1 &lt;- seq(1,5) # Integer Vector lis2 &lt;- factor(1:5) # Factor Vector lis3 &lt;- letters[1:5] combined_list &lt;- list(lis1, lis2, lis3) combined_list[[1]] combined_list[[2]] combined_list[[3]] combined_list[[3]][5] flat_list &lt;- unlist(combined_list) class(flat_list) 5.7 Vectors Vectors are an object which is used to store multiple information or values of the same data type. A vector can not have a combination of both integer and character. For example, if you want to store 100 students’ total marks, instead of creating 100 different variables for each student, you would create a vector of length 100, which will store all the student marks in it. marks &lt;- c(88,65,90,40,65) marks[4] marks[1] marks[6] 5.7.1 Slicing vectors marks[seq(1,4)] marks[c(1,2,4)] char_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) class(char_vector) length(char_vector) char_vector[1:3] char_vector[-3] char_num_vec &lt;- c(1,2, &quot;a&quot;) char_vector &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) class(char_vector) length(char_vector) char_vector[1:3] char_vector[-3] vec &lt;- seq(1,1024) vec &lt;- c(1:1024) 5.8 Matrix Similar to a vector, a matrix is used to store information about the same data type. However, unlike vectors, matrices are capable of holding two-dimensional information inside it. M &lt;- matrix(vector, nrow=r, ncol=c, byrow=FALSE, dimnames=list(char_vector_rownames, char_vector_colnames)) # byrow=TRUE signifies that the matrix should be filled by rows. # byrow=FALSE indicates that the matrix should be filled by columns (the default). M &lt;- matrix(seq(1,6), nrow = 2, ncol = 3, byrow = TRUE) M &lt;- matrix(seq(1,6), nrow = 2, ncol = 3, byrow = F) 5.9 DataFrame Unlike a matrix, Data frames are a more generalized form of a matrix. It contains data in a tabular fashion. The data in the data frame can be spread across various columns, having different data types. dataset &lt;- data.frame( Person = c(&quot;Aditya&quot;, &quot;Ayush&quot;,&quot;Akshay&quot;), Age = c(26, 26, 27), Weight = c(81,85, 90), Height = c(6,5.8,6.2), Salary = c(50000, 80000, 100000)) class(dataset) nrow(dataset) ncol(dataset) df1 &lt;- rbind(dataset, dataset) df2 &lt;- cbind(dataset, dataset) head(df1,3) str(dataset) summary(dataset) "],["data-input-and-output-io.html", "6 Data input and output (IO) 6.1 Some considerations: 6.2 Reading and Writing Files 6.3 Small data 6.4 Practice 6.5 Large(r) data", " 6 Data input and output (IO) 6.1 Some considerations: Keep the names of local files downloaded from the internet or copied onto your computer unchanged. This will help you trace the provenance of the data in the future. R’s native file format .RData can be accessed using load and save. 6.2 Reading and Writing Files There are many methods to read and write files in R programming. Your command of these is critical because all scientific work begins with data, and most data is found inside files and databases. Dealing with input is probably the first step of implementing any significant project. 6.3 Small data For very small datasets, is may be preferred to enter the data by hand. c is a common function used for combine: x &lt;- c(3,7,11,19) y &lt;- c(1,1,1,1) c(x,y) y &lt;- c(x,y,5) There are a suite of functions to enter data in the console. The sequence function seq: y &lt;- seq(from = 1, to = 10) y &lt;- seq(from = 1, to = 10, by = 2.5) y &lt;- seq(from = 1, to = 10, length.out = 22) Use the function rep (repeat): y &lt;- rep(x = 5, times = 4) x.value &lt;- c(2,3) rep(x.value, times = 3) 6.4 Practice Number Excercise 1. How many different ways can you create a vector labled q containing two 3’s and four 5’s? Try some! 2. Assign a vector of four elements: 3,7,9 and 2 to w. 3. Assign the third element of w to s, where s is equal to 6. 4. What is the length of a sequence that starts at 1.1, ends at 9.2, and has increments of 0.894? 5. What is the 3rd value of the sequence you created? # Create a data frame using the &quot;data.frame&quot; function site.name &lt;- c(rep(&quot;Site.01&quot;,3),rep(&quot;Site.02&quot;,3)) density &lt;- rep(x = 2.3, times = length(site.name)) abundance &lt;- seq(from = 14.5, to = 19.8, length.out = length(site.name)) sampled. &lt;- c(F, T, F, F, T, F) y.data.frame &lt;- data.frame(site.name, density, abundance, sampled.) y.data.frame class(y.data.frame) 6.5 Large(r) data 6.5.1 Read csv Most scientific work will involve data larger than can be entered by hand. In this case we will use a suite of commands and different packages to get the data into our environment. read.csv(&quot;./Data/co2.csv&quot;) 6.5.2 Read xlsx MS Excel files are widely used install.packages(&#39;readxl&#39;) require(&#39;readxl&#39;) read_xlsx(&quot;./Data/Codes.xlsx&quot;, sheet = 1) 6.5.3 Download data from public repository Website can be found here: &quot;https://www.stats.govt.nz/large-datasets/csv-files-for-download/&quot; The data are accessed by this URL: url &lt;- &quot;https://www.stats.govt.nz/assets/Uploads/Annual-enterprise-survey/Annual-enterprise-survey-2020-financial-year-provisional/Download-data/annual-enterprise-survey-2020-financial-year-provisional-csv.csv&quot; You can see that the url links to a .csv file. This is a file in an online directory. Access the directory in a web browser: https://www.stats.govt.nz/ destfile &lt;- &quot;./Data/output.csv&quot; download.file(url, destfile) site: “bookdown::bookdown_site” output: bookdown::gitbook: lib_dir: “book_assets” bookdown::pdf_book: keep_tex: yes "],["sampling.html", "7 Sampling 7.1 Replication 7.2 Simple Random Sampling 7.3 Some Terminology of Sampling 7.4 Independence of samples 7.5 Why do we want to collect representative samples? 7.6 Sampling Design Considerations 7.7 What is Pseudoreplication? 7.8 Examples of Pseudoreplication 7.9 Sample allocation 7.10 Systematic Sampling", " 7 Sampling Statistical analyses are performed on samples that are taken from large populations (we consider most populations to be infinitely large and thus sampling is necessary). The goal is to sample some elements in a population that are representative of the whole population. This saves resources because it is not desirable or feasible to sample every element in a population. Sampling every element in a population is called a census. The most challenging aspect of a sampling program is to ensure that the sample taken from the population is a random one. 7.1 Replication Replication means having replicate observations at a spatial and temporal scale that matches the application of the experimental treatments. Replicates are essential because biological systems are inherently variable and this is particularly so for ecological systems. 7.2 Simple Random Sampling To accomplish this, the most robust approach is to use simple random sample (termed SRS). From QK “You can’t really go wrong with simple random sampling.” But… “The downside of simple random sampling is that it may be less efficient than other sampling designs, especially when there is identified heterogeneity in the population or we wish to estimate parameters at a range of spatial or temporal scales.” In SRS, each unit in the population is identified, and each unit has an equal chance of being in the sample. This is the only way to obtain a statistically valid and representative sample. Ensuring that data are collected in a random fashion allows statistics to be calculated. Non-random data collection disallows this. Only statistically valid observations can be used for analysis. Consider Exit polling vs. Twitter polls to estimate proportion voting for a candidate. Why does one provide a better estimate than another? BTW, this is the primary issue with current political polling - the quest to get a representative sample (https://fivethirtyeight.com/features/is-the-polling-industry-in-stasis-or-in-crisis/) 7.3 Some Terminology of Sampling a population is the entire collection of people or things you are interested in (all cannot be measured, or even should be); a sample is a subset of some of the units in the population; a statistic or summary statistic is a value that results from measuring all the units in the sample (e.g. mean, mode, range); For example, to find out the average age of all motor vehicles in the state in 2022: Population = all motor vehicles in the state in 2022 Sample = 300 randomly selected motor vehicles Statistic = the average age of the 300 motor vehicles in the sample Statistics derived from samples are used to estimate population parameters. In the above example, we would say that the sample statistic (sample mean) is our best estimate of the population mean. We did not perform a census, so we don’t know what the mean of the population is, but instead use our sample statistic. Another example, how to select a sample of 25 people who live in your college dorm, and ensure a random sample you might: Make a list of all the 250 people who live in the dorm. Assign each person a unique number, between 1 and 250. Then refer to a table of random numbers. Starting at any point in the table, read across or down and note every number that falls between 1 and 250. Use the numbers you have found to pull the names from the list that correspond to the 25 numbers you found. These 25 people are your sample. This is called the table of random numbers method. 7.4 Independence of samples Independence is also a critical aspect of sampling. Independence implies that sampling (measuring) of one element in the population will not impact or predict the value of another element. 7.5 Why do we want to collect representative samples? We use statistical analysis to describe (calculate) the values of sample statistics among or between populations. For example, is the mean value different between two populations or to test whether a statistic (like the mean) is different from some hypothesized value. 7.5.1 R Code ## Provide code in R (using base R functionality) to sample a simulated distribution using ## simple random sampling. ## ```R ## # Set the seed for reproducibility ## set.seed(123) ## ## # Generate a simulated population of numbers ## population &lt;- rnorm(1000, mean = 50, sd = 10) # Generating random data with 1000 samples which follows normal distribution with mean of 50 and standard deviation of 10. ## ## # Sample 100 numbers from the population using simple random sampling ## sampled_data &lt;- sample(population, size = 100, replace = FALSE) # Selecting 100 random samples, without replacement, from the generated population. This sample is stored in variable &quot;sampled_data&quot;. ## ## # View the first 5 numbers in the sampled data ## head(sampled_data) # Displays the first five numbers in the sampled_data dataset. ## ``` ## ## Explanation: ## ## - The `set.seed()` function is used to set the seed for reproducibility of the random numbers generated. ## - The `rnorm()` function is used to simulate a population of 1000 numbers following the standard normal distribution with a mean of 50 and standard deviation of 10. ## - The `sample()` function is used to randomly select 100 numbers from the population without replacement (i.e., `replace = FALSE`). The sampled data is stored in the variable `sampledata`. ## - Finally, `head()` is used to view the first 5 numbers in the sampled data. 7.6 Sampling Design Considerations Hurlburt (1984) Now that we know what constitutes a valid sample, lets look at some ways (and some ways not) to collect (and analyze) samples. Hurlburt’s paper has been instrumental in promoting an understanding of pseudoreplication and the term has become widely used. Scientists have become more aware of the need for close concordance of design, analysis, and interpretation of experiments. “No one would now dream of testing the response to a treatment by comparing two plots, one treated and the other control.” Fisher and Wishart (1930). Hurlburt evaluated the frequency of pseudoreplication as a fatal flaw of experiments (Hurlbert, 1984. Ecological Monographs 54:187‐211). The ‘fatal flaw’ implies that the conclusions of the work are not supported because the sampling was done improperly. In the paper, Hurlburt Evaluated 176 studies from 1960 to 1984. He found that 27% overall or 48% of those making statistical inferences used “pseudoreplicated” approaches. The term pseudoreplication refers to “the use of inferential statistics to test for treatment effects with data from experiments where either treatments are not replicated (though samples may be) or replicates are not statistically independent.“ The context of his paper was ecological field experiments, but pseudoreplication can occur in other contexts as well. 7.7 What is Pseudoreplication? Replication refers to having more than one experimental (or observational) unit with the same treatment. Each unit with the same treatment is called a replicate. Most models for statistical inference require true replication. True replicates are often confused with repeated measures or with pseudoreplicates. 7.8 Examples of Pseudoreplication Suppose a blood-pressure lowering drug is administered to a patient, then the patient’s blood pressure is measured twice. This is a repeated measure, not a replication. It can give information about the uncertainty in the measurement process, but not about the variability in the effect of the drug. On the other hand, if the drug were administered to two patients, and each patient’s blood pressure was measured once, we can say the treatment has been replicated, and the replication may give some information about the variability in the effect of the drug. A researcher is studying the effect on plant growth of different concentrations of CO2 in the air. He needs to grow the plants in a growth chamber so that the CO2 concentration can be controlled. He has access to only two growth chambers, but each one will hold five plants. However, since the five plants in each chamber share whatever conditions are in that chamber besides the CO2 concentration, and in fact may also influence each other, they are not independent replicates but are pseudoreplicates. The growth chambers are the experimental units; the treatments are applied to the growth chambers, not to the plant independently. Two fifth-grade math curricula are being studied. Two schools have agreed to participate in the study. One is randomly chosen to use curriculum A, the other to use curriculum B. At the end of the school year, the fifth-grade students in each school are tested and the results are used to do a statistical analysis comparing the two curricula. There is no true replication in this study; the students are pseudo-replicates. The schools are the experimental units; they, not the students, are randomly assigned to treatment. Within each school, the test results (and the learning) of the students in the experiment are not independent; they are influenced by the teacher and other school-specific factors (e.g., previous teachers and learning, socioeconomic background of the school, etc.). 7.9 Sample allocation Hurlbert’s other primary concern with proper experimental design is to ensure independence. He shows that with randomization methods do not always giving good interspersion of treatments (spatial and temporal). Hence his focus on random interspersion of samples from a population. Examples of allocation and randomization approaches. 7.10 Systematic Sampling SS is a probability sampling method where members of population selected at regular interval (k). The benefits of SS are that is imitates the randomization benefits of simple random sampling but can be easier to conduct. To employ this we will take a full list of the population and sample every \\(k^{th}\\) object. To determine k divide estimated population size by sample size. 7.10.1 Stratified Random Sampling The way in which was have selected sample units thus far has required us to know little about the population of interest in advance of selecting the sample. This approach is ideal only if the characteristic of interest is distributed homogeneously across the population. In a stratified sample, researchers divide a population into subpopulations called strata (the plural of stratum) based on specific characteristics (e.g., habitat, niche, taxanomic status, location, etc.). Every member of the population studied should be in exactly one stratum. For example, if we have information that we know to be associated with the heterogeneity in the population, we can use that ancillary information to guide alternative strategies for selecting samples that will yield estimates with higher precision that a simple random sample for the same amount of effort. The first of these designs is stratified random sampling. A stratified random sample is one obtained by dividing the population elements into mutually exclusive, non-overlapping groups of sample units called strata, then selecting a simple random sample from within each stratum (stratum is singular for strata). Stratifying involves classifying sampling units of the population into relatively homogeneous groups before (usually) selecting sample units. Strata are based on information other than the characteristic being measured that is known to or thought to vary with the characteristic of interest. Stratified Sampling Scheme. Because virtually all ecological systems are heterogeneous, stratifying is used commonly as a way to increase precision in ecological studies. Common strata in ecological studies include elevation, aspect, or other geographic features for studying plant communities and vegetation communities or soils for studying some animal communities. When choosing among several potential strata, seek strata that best minimize variation in the characteristic of interest within strata and that maximize variation among strata. How it is implemented: * Divide the entire population into non-overlapping strata * Select a simple random sample from within each strata L = number of strata \\(n_i\\) = number of sample units within stratum i n = number of sample units in the population Estimates from stratified random samples are simply the weighted average or the sum of estimates from a series of simple random samples, each generated within a unique stratum. \\(\\bar{y} = \\sum_{h=1}^{L}W_{h}\\bar{y}_h\\), where there are h = 1 to L strata, \\(W_h\\) is the proportion of total units in stratum h (often estimated from the proportion of total area in stratum h) and \\(\\bar{y}_h\\) is the sample mean for stratum h. 7.10.2 Allocating Sampling Effort among Strata Using stratified random sampling requires that we decide how to divide a fixed amount of sampling effort among the different strata; that process is called allocation. When deciding where to allocate sampling effort, the question becomes how best to allocate effort among strata so that the sampling process will provide the most efficient balance of effort, cost, and estimate precision. Uniform Allocation (equal number for all strata) Allocation Proportional to Size Allocation Proportional to Variation (developed from pilot study) "],["central-limit-theorem.html", "8 Central Limit Theorem", " 8 Central Limit Theorem The central limit theorem (CLT) states that the distribution of sample means approximates a normal distribution as the sample size becomes larger regardless of the population distribution shape. The sample theory is the study of relationships existing between a population and samples drawn from population. Consider all possible samples of size n that can be drawn from the population. For each sample, we can compute statistic like mean or a standard deviation, etc that will vary from sample to sample. This way we obtain a distribution called as the sampling distribution of a statistic. If the statistic is sample mean , then the distribution is called the sampling distribution of mean. If we take many sets of samples from a population, and calculated the mean of these, we could plot the frequency distribution of the sample means. This distribution is the ‘sampling distribution’ It: 1.) The sampling distribution from a normal distribution is normally-distributed. 2.) As sample size increases (to infinity), the sampling distribution, from any distribution, will approach a normal distribution. 3.) The expected value (the mean) of the sample distribution will be the mean of the population distribution. 8.0.1 Example of the CLT (Die Rolling) A fair die can be modeled with a discrete random variable with outcome 1 through 6, each with the equal probability of 1/6. DieOutcome &lt;- sample(1:6,10000, replace= TRUE) hist(DieOutcome, col =&quot;light blue&quot;) abline(v=3.5, col = &quot;red&quot;,lty=1) We will take samples of size 10 , from the above 10000 observation of outcome of die roll, take the arithmetic mean and try to plot the mean of sample. we will do this procedure k times (in this case k= 10000 ) x10 &lt;- c() k =10000 for ( i in 1:k) { x10[i] = mean(sample(1:6,10, replace = TRUE))} hist(x10, col =&quot;pink&quot;, main=&quot;Sample size =10&quot;,xlab =&quot;Outcome of die roll&quot;) abline(v = mean(x10), col = &quot;Red&quot;) abline(v = 3.5, col = &quot;blue&quot;) By theory , we know as the sample increases, we get better bell shaped curve. As the n apporaches infinity , we get a normal distribution. Lets do this by increasing the sample size to 30, 100 and 1000. x30 &lt;- c() x100 &lt;- c() x1000 &lt;- c() k =10000 for ( i in 1:k){ x30[i] = mean(sample(1:6,30, replace = TRUE)) x100[i] = mean(sample(1:6,100, replace = TRUE)) x1000[i] = mean(sample(1:6,1000, replace = TRUE)) } par(mfrow=c(1,3)) hist(x30, col =&quot;green&quot;,main=&quot;n=30&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x30), col = &quot;blue&quot;) hist(x100, col =&quot;light blue&quot;, main=&quot;n=100&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x100), col = &quot;red&quot;) hist(x1000, col =&quot;orange&quot;,main=&quot;n=1000&quot;,xlab =&quot;die roll&quot;) abline(v = mean(x1000), col = &quot;red&quot;) 8.0.2 Example of the CLT (Coin Flipping) Flipping a fair coin many times the probability of getting a given number of heads in a series of flips should follow a normal curve, with mean equal to half the total number of flips in each series. Here 1 represent heads and 0 tails. x &lt;- c() k =10000 for ( i in 1:k) { x[i] = mean(sample(0:1,100, replace = TRUE))} hist(x, col =&quot;light green&quot;, main=&quot;Sample size = 100&quot;,xlab =&quot;flipping coin &quot;) abline(v = mean(x), col = &quot;red&quot;) "],["measures-of-central-tendency.html", "9 Measures of Central Tendency 9.1 Population Mean 9.2 Sample Mean 9.3 Mean from the frequency distribution 9.4 Skew 9.5 Maximum Likelihood", " 9 Measures of Central Tendency Now we will start to calculate summary statistics. Zar 4th edition 3.1 to 3.5 The mode is the most frequently occurring value in the population or sample. The median is the 50th percentile in the ordered data. The mean is the most efficient, unbiased and consistent estimator of \\(\\mu\\). \\(\\mu\\) is the mean of the characteristic for the population and the one we would expect to find most frequently. Thus it is the best measure of the ‘central tendency’. The center of the distribution where we get maximum frequency. An efficient estimator is one that estimates the quantity of interest in some “best possible” manner. “unbiased” reflects the accuracy of the estimator - how close is it to the population’s value. 9.1 Population Mean \\(\\mu = \\frac{\\sum_{i = 1}^N{X_i}}{N}\\) 9.2 Sample Mean \\(\\bar{X} = \\frac{\\sum_{i = 1}^n{X_i}}{n}\\) 9.3 Mean from the frequency distribution \\(\\bar{X} = \\frac{\\sum_{i = 1}^k{f_iX_i}}{n}\\) Here, k is the number of bins. 9.3.1 R Code ## Provide code in R (using base R functionality) to report the mean, median, and mode of a ## sample. ## ```R ## # Sample data ## # Create a vector &#39;sample&#39; containing some numeric values ## sample &lt;- c(1, 3, 2, 5, 6, 3, 4, 2, 3, 2) ## ## # Mean ## # Calculate the mean of the sample vector using &#39;mean&#39; function and store the result in &#39;mean_sample&#39; variable ## mean_sample &lt;- mean(sample) ## ## # Print the mean value to the console using &#39;cat&#39; function ## cat(&#39;The mean of the sample is:&#39;, mean_sample, &#39;\\n&#39;) ## ## # Median ## # Calculate the median of the sample vector using &#39;median&#39; function and store the result in &#39;median_sample&#39; variable ## median_sample &lt;- median(sample) ## ## # Print the median value to the console using &#39;cat&#39; function ## cat(&#39;The median of the sample is:&#39;, median_sample, &#39;\\n&#39;) ## ## # Mode ## # Calculate the mode of the sample vector ## # Get the frequency of each value in the &#39;sample&#39; vector using &#39;table&#39; function ## # Sort the frequency table in decreasing order and select the first value using &#39;names&#39; function ## mode_sample &lt;- names(sort(table(sample), decreasing = TRUE))[1] ## ## # Print the mode value to the console using &#39;cat&#39; function ## cat(&#39;The mode of the sample is:&#39;, mode_sample) ## ## ``` ## ## This code will output: ## ## ``` ## The mean of the sample is: 3.2 ## The median of the sample is: 3 ## The mode of the sample is: 2 ## ``` ## ## Note that for the mode calculation, we first use `table` to get the frequency of each value in the sample, then we sort them in decreasing order and select the first value using `names`. This assumes that the mode is unique and there is at least one value that appears more than once in the sample. 9.4 Skew A left-skewed distribution has a long left tail. A right-skewed distribution has a long right tail. 9.4.1 R Code ## Provide code in R (using base R functionality) to create and plot skewed distributions ## (posititve and negative). ## ``` ## # Generate skewed data using the rlnorm() function with a seed of 123 ## set.seed(123) ## x &lt;- rlnorm(1000) ## ## # Plot skewed distribution using the hist() function with 30 bins, a blue color, and a main title ## hist(x, breaks = 30, col = &#39;steelblue&#39;, main = &#39;Positive Skewed Distribution&#39;) ## ``` ## ## ``` ## # Generate negatively skewed data using the negative value of rlnorm() with a seed of 123 ## set.seed(123) ## x &lt;- -rlnorm(1000) ## ## # Plot negatively skewed distribution using the hist() function with 30 bins, a blue color, and a main title ## hist(x, breaks = 30, col = &#39;steelblue&#39;, main = &#39;Negative Skewed Distribution&#39;) ## ``` ## ## The use of `rlnorm()` for generating skewed data is highlighted, and how negative values can create a negatively skewed distribution is demonstrated. Additionally, the `hist()` function is used to create a histogram of the generated data with specific parameters such as number of breaks, color and main title. 9.5 Maximum Likelihood The primary source for the following lecture material are primarily derived from I.J. Myung’s paper “Tutorial on maximum likelihood estimation”. Journal of Mathematical Psychology 47 (2003) 90-100. The interested reader is directed to review this paper. In this lecture we will understand an alternative to parameter estimation using least-squares estimation, maximum likelihood estimation (MLE). MLE is a preferred method of parameter estimation in statistics and is a general parameter estimation approach, in particular in non-linear modeling and/or with non-normally distributed data. Because of the prevelance of non-normally distributed data in the natural sciences (e.g. data from counting, categorical data, skewed ratio data) an examination and familiarization of this approach is useful. 9.5.1 General Exprimental Approach Because many phenomenon of interest are not directly observable, we formulate hypotheses to test and these hypothesis are stated in terms of probability using statistical models. The goal of statistical modeling is to understand underlying processes by testing the viability (e.g. quality, robustness) of the model. Our method: Specify a statistical model Collect data Evaluate how well the model fits the data by: Parameter estimation Evaluating goodness of fit 9.5.2 Two approaches to parameter estimation: LSE (least-squares estimation), for normally-distributed data MLE, a general approach for parameter estimation 9.5.3 The probability density function The goal of data analysis is to identify the population that is most likely to have generated the sample - i.e. we will estimate the parameters of the candidate model that will produce these observations. The data vector \\(y = (y_1, y_2, ..., y_m)\\) is a random sample from a population distributed in some unknown way. Populations are identified using a probability distribution and unique values of the parameters - As the parameter changes in value, different probability distributions are generated. Let \\(f(y|w)\\) denote the probability density function (PDF) that specifies the probability of observing data vector y given the parameter w. The parameter \\(w = (w_1, ..., w_k)\\) is a vector defined on a multi-dimensional parameter space. For example, if the PDF is normal, \\(w = (\\mu, \\sigma)\\) For this lecture we are primary concerned with the estimation of \\(\\mu\\). We do this by determining \\(\\bar{X}\\). Note, I present MLE because it is a very general way to determine parameter values: If the PDF is a t distribution, \\(w = (d.f.)\\) Different distributions are defined using different parameters (and different mathematical formulations), so w is distribution-specific. If we have specified a distribution that has a certain set of parameters, for example: We can use a given probability distribution and parameter set to determine the probability of obtaining a value in a population. Example: Children’s IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What is the probability of a randomly selected child having an IQ between 80 and 120? In this case the area under the curve is 0.818 or 81.8% of the integral of the distribution from \\(-\\infty\\) to \\(+\\infty\\). So, if we take 100 random draws from the population of children’s IQs, we will get values of IQ &gt; 80 and &lt; 120, 81 to 82 times… Let’s examine the statement: \\(p(80 &lt; IQ &lt; 120\\) | \\(\\mu = 100, \\sigma = 15) = 0.818\\). We are stating that the probability of randomly selecting an IQ observation the variable characteristics given the parameter values is 0.818. If we are interested in finding probabilities of students with different variable characteristics (different IQ values), then we will change the left side of the equation. For example: \\(p(IQ &lt; 65\\) | \\(\\mu = 100, \\sigma = 15)\\) or \\(p(IQ &gt; 120\\) | \\(\\mu = 100, \\sigma = 15)\\) The right side of the equation does not change because it describes the fixed shape of the distribution of the population that “creates” the observations. So when we are investigating probability of an event we are quantifying the integral of the curve for the given parameter set bounded by the left side of the equation. We change the left side to derive new probability values. We can determine the probability of obtaining \\(p(y_1, y_2, ..., y_m)\\) | \\(w)\\) if the observations are independent. Think about taking multiple random draws from the population described by the parameter set w. An analog here is thinking about coin flipping, the probability of realizing specific outcomes from multiple trials is determined by multiplication (e.g. probability of observing two “tails” flips is the product of the independent single observations). So, the notation below is Pi (product) notation. This is used in mathematics to indicate repeated multiplication. \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = p(y_1\\)| \\(w) \\times p(y_2\\)| \\(w) \\times ...p(y_n\\)| \\(w)\\) \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = \\prod_{i = 1}^{n}{p(y_i | w)}\\) 9.5.4 The likelihood function Now we will discuss the likelihood function that is derived by considering that the data are fixed and that the shape of the (parent) distribution is random. In this approach we are evaluating the inverse problem of the PDF. Specifically, given the observed data and a model of interest, we are interested in finding the unique parameter vector w, among all the possible combinations of parameters that is most likely to have produced the data. We have already observed the data and now want to define the likelihood function by reversing the roles (i.e. what is random what is fixed) of the data vector y and the parameter vector w. So, we focus on \\(L(w|y)\\). This represents the likelihood of the parameter w given the observed data y; and as such is a function of w. So, the data are fixed, and we modify the parameter vector w (in the case of IQ, \\(\\mu\\) and \\(\\sigma\\)) Assume we have some vector of observed data y, these are sampled from some population, for now, assume we take a single value. What is the likelihood that \\(\\mu = 100\\) and \\(\\sigma = 15\\) given our sampled IQ is \\(y = 120\\)? What if we have a vector of observations \\(n = 5\\), with mean values centered on 120? So, we want to find the distribution parameters that maximize the likelihood. We still think that the normal distribution is the most appropriate distribution as a candidate distribution. \\(y = (100, 110, 120, 130, 140)\\) "],["measures-of-dispersion.html", "10 Measures of Dispersion 10.1 Range 10.2 The Interquartile Range 10.3 Other quantiles 10.4 Error and Deviations from Expectations 10.5 Quantifying Error 10.6 Sum of Squared Errors 10.7 Variance 10.8 Standard Deviation 10.9 Summary of Variance Estimates 10.10 Coefficient of Variation", " 10 Measures of Dispersion 10.1 Range The smallest score subtracted from the largest score in the observation, e.g.  Number of contacts of n = 11 randomly selected social media users. 22, 40, 53, 57, 93, 98, 103, 108, 116, 121, 252 You can see that these observations are ordered (from smallest to largest) Range = 252 - 22 = 230 10.1.1 R Code ## Provide code in R (using base R functionality) to report the range, minimum, and maximum ## of a set of values. ## ```R ## # create a vector of values ## values &lt;- c(5, 2, 8, 3, 11, 6) ## ## # calculate the range ## range &lt;- max(values) - min(values) ## ## # calculate the minimum value ## minimum &lt;- min(values) ## ## # calculate the maximum value ## maximum &lt;- max(values) ## ## # print the results ## cat(&#39;Range:&#39;, range, &#39;\\n&#39;) # print the range value ## cat(&#39;Minimum:&#39;, minimum, &#39;\\n&#39;) # print the minimum value ## cat(&#39;Maximum:&#39;, maximum, &#39;\\n&#39;) # print the maximum value ## ``` ## ## The comments describe each line of code, explaining what the code does. The `cat()` function is used to print the results to the console. You can modify the values of the `values` vector to get the range, minimum, and maximum of any set of numbers. 10.2 The Interquartile Range Identify the values that split the sorted data into four equal parts. First or lower quartile (the range values of the first 25% of values in ordered sequence) Second quartile (the range values of the first 25 to 50% of values in ordered sequence) Third quartile (the range values of the first 50 to 75% of values in ordered sequence) Fourth quartile (the range values of the first 75 to 100% of values in ordered sequence) 10.2.1 R Code ## Provide code in R (using base R functionality) to report values of the interquartile range ## from a simulated set of values. ## ``` ## # Generate a simulated set of values ## set.seed(123) # set the random seed for reproducibility ## values &lt;- rnorm(100) # generate 100 normally distributed values ## ## # Calculate the interquartile range ## q25 &lt;- quantile(values, 0.25) # calculate the 25th quartile of the values ## q75 &lt;- quantile(values, 0.75) # calculate the 75th quartile of the values ## iqr &lt;- q75 - q25 # calculate the interquartile range ## ## # Report the interquartile range ## cat(&#39;The interquartile range is:&#39;, iqr) # print the interquartile range to the console ## ``` 10.3 Other quantiles In an ordered list of observations, any quantile can be determined. Let’s examine the quantiles of interest for the ‘air time’ from flights departing NYC’s airports in 2013 with destination of the Atlanta aiport “ATL” with Delta Airlines. carrier flight tailnum origin dest air_time distance hour minute DL 461 N668DN LGA ATL 116 762 6 0 DL 1743 N3739P JFK ATL 128 760 6 10 DL 575 N326NB EWR ATL 120 746 6 15 DL 1547 N6703D LGA ATL 126 762 7 0 DL 2047 N935DL LGA ATL 126 762 7 59 Let’s look at the quantiles of air time for these flights. ## 0% 5% 10% 20% 30% 40% 50% 60% 70% 80% 90% 95% 100% ## 65 98 101 104 106 109 111 114 117 120 125 129 176 10.3.1 R Code ## Provide code in R (using base R functionality) to report values of the quantiles from a ## simulated set of values. ## Sure, here are the inline comments to the provided R code: ## ## ```R ## # Simulated set of values ## set.seed(123) # Setting the seed for R&#39;s random number generator for reproducibility ## x &lt;- rnorm(100) # Generating a set of 100 random values from a normal distribution ## ## # Calling quantile function to report the quantiles ## quantile(x) # Using the `quantile()` function to report the 25th, 50th (median), and 75th quantiles in R ## ``` ## ## This will output the following: ## ``` ## 0% 25% 50% 75% 100% ## -2.345697 -0.727719 -0.013617 0.659230 2.415835 ## ``` ## ## ## ```R ## # Reporting 10th, 50th, and 90th percentiles ## quantile(x, c(0.1, 0.5, 0.9)) # Using the `quantile()` function to report the 10th, 50th (median), and 90th percentiles ## ``` ## ## This will output the following: ## ``` ## 10% 50%``` ## 90% ## -1.2305730 1.3307126 ## ``` ## ## I hope this helps clarify the use of `quantile()` function in R! Let me know if you have any further questions. 10.4 Error and Deviations from Expectations Error is the difference between a value obtained from a data collection process and the ‘true’ value for the population. We can evaluate error for a single data point or we can evaluate the total error of our sample relative to our expectation of what the population value might be. 10.5 Quantifying Error A deviation is the difference between the mean (expected) and the observed data (the outcome of the sample). The deviation of observed and expected value is called: the residual, error, or residual error. The expected value is the one that we would encounter most frequently. The expected value, in the context of the normal distribution, is the mean value. When the normal fitting models: \\(deviation = X_i - \\bar{X}\\) Should we use the Total Error as an estimate of uncertainty? We could sum \\(i^{th}\\) error terms from 1 to n. i Score Score - mean(Score) 1 -0.28 -0.795 2 -0.14 -0.655 3 1.04 0.525 4 0.65 0.135 5 0.26 -0.255 6 1.56 1.045 \\(\\Sigma(X_i - \\bar{X}) = 0\\) 10.6 Sum of Squared Errors We could add the deviations to find out the total error, but the deviations ‘cancel out’ (some are positive and others negative) Therefore, we square each deviation. If we add these squared deviations we get the sum of squared errors (SS). i Score Deviation Squared Deviation 1 -0.28 -0.795 0.632 2 -0.14 -0.655 0.429 3 1.04 0.525 0.276 3 0.65 0.135 0.018 4 0.26 -0.255 0.065 5 1.56 1.045 1.092 \\(SS = \\Sigma(X_i - \\bar{X})^2 = 2.512\\) The sum of squares is a good measure of overall variability, but is dependent on the number of scores. 10.6.1 R Code ## Provide code in R (using base R functionality) to calculated the Sum of Squared Errors ## from a simulated set of values. ## ```r ## # Simulate a set of values ## set.seed(123) # set the seed for reproducibility ## values &lt;- rnorm(10) # generate 10 random numbers from a normal distribution ## ## # Calculate the mean of the values ## mean_values &lt;- mean(values) # find the average of the generated values ## ## # Calculate the Sum of Squared Errors ## sse &lt;- sum((values - mean_values)^2) # calculate the sum of squared errors from the mean ## ## # Print the results ## cat(&#39;The Sum of Squared Errors is:&#39;, sse) # print the calculated value of Sum of Squared Errors ## ``` 10.7 Variance In statistics, when we talk about population, we mean the entire universe of possible values of a stochastic (random) variable. Population variance: \\(\\sigma^2 = \\frac{SS}{N} = \\frac{\\Sigma^n_{i=1}(X_i-\\mu)^2}{N}\\) Most of the time, we don’t sample the entire population because it is too complex or simply not feasible. Think, for instance, at a problem when you want to analyze the heights of the oak trees in a forest. You can, of course, measure every single tree of the forest and so have collected statistics about the entire forest, but this could be very expensive and would take a very long time. So, we don’t generally know \\(\\mu\\). We calculate the average variability (average variability of each sample). So, obtain a sample of, let’s say, 20 trees and try to relate sample statistics and population statistics. Sample variance: \\(s^2 = \\frac{SS}{n-1} = \\frac{\\Sigma(X_i-\\bar{X})^2}{n-1}\\) Population variance: \\(\\sigma^2 = \\frac{SS}{N} = \\frac{\\Sigma_{i=1}^N(X_i-\\mu)^2}{N}\\) Why n-1 instead of N? When we compute the difference between each value and the mean of those values (observed - expected), we don’t know the true mean of the population; all you know is the mean of your sample. In general, we do not know the population mean, the sample mean is the mean of the data and should be close to the true population mean. The value computed in the sample variance will be an underestimate of the population sample mean and cannot be larger; we have likely not sampled the full range of the values in the population. Therefore, the sample variance will be an underestimate of the population variance. To make up for this, divide by n-1 rather than N. This is called Bessel’s correction. 10.7.1 R Code ## Provide code in R (using base R functionality) to calculated the variance from a simulated ## set of values. ## ``` ## # Simulate data ## # set the seed of the random number generator to ensure reproducibility ## set.seed(123) ## # generate 100 values from a standard normal distribution and store them in the vector x ## x &lt;- rnorm(100) ## ## # Calculate variance ## # use the var() function to calculate the variance of vector x and store the result in the variable variance_x ## variance_x &lt;- var(x) ## ## # Print variance ## # print the value of variance_x to the console ## print(variance_x) ## ``` 10.8 Standard Deviation The variance has one problem: it is measured in units2 (The original units, like the numbers are squared.). This isn’t a very meaningful metric so we take the square root value. This is the sample standard deviation (sd or s): \\(sd = s = \\sqrt\\frac{\\Sigma^n_{i=1}(X_i-\\bar{X})^2}{n-1}\\) \\(\\sigma = \\sqrt\\frac{\\Sigma^N_{i=1}(X_i-\\mu)^2}{N}\\) 10.8.1 R Code ## Provide code in R (using base R functionality) to calculated the standard deviation from a ## simulated set of values. ## ```R ## # Generate a simulated set of values ## set.seed(123) # set the seed for reproducibility ## simulated_data &lt;- rnorm(n = 100, mean = 10, sd = 2) # create a vector of 100 values drawn from a normal distribution with mean 10 and standard deviation 2 ## ## # Calculate the standard deviation ## sd(simulated_data) # calculate the standard deviation of the simulated data ## ``` 10.9 Summary of Variance Estimates The sum of squares, variance, and standard deviation represent the same thing: The fit of the mean to the data, how well the mean represents the observed data The variability in the data when modeled using the mean 10.10 Coefficient of Variation The coefficient of variation (CV) is a measure of the dispersion (measured by standard deviation) of data points in a data series around the mean. The coefficient of variation represents the ratio of the standard deviation to the mean, and it is a useful statistic for comparing the degree of variation from one data series to another, even if the means are drastically different from one another. \\(CV = \\frac{s}{\\bar{X}}\\) 10.10.1 R Code ## Provide code in R (using base R functionality) to calculated the coefficient of variation ## from a simulated set of values. ## ``` ## # set the seed to ensure reproducibility ## set.seed(123) ## ## # generate a simulated set of 100 values from a normal distribution with mean=20 and sd=5 ## sim_values &lt;- rnorm(100, mean=20, sd=5) ## ## # calculate the coefficient of variation by dividing the standard deviation of the values by the mean of the values ## cv &lt;- sd(sim_values) / mean(sim_values) ## ## # print the result ## cv ## ``` ## ## This code generates a simulated set of 100 values based on a normal distribution with a mean of 20 and a standard deviation of 5, calculates the coefficient of variation for the generated values, and prints the result. By setting the seed, we ensure that the same set of values will be generated each time the code is run. You can adjust the parameters of the `rnorm()` function to generate different sets of values based on your needs. "],["the-normal-distribution.html", "11 The Normal Distribution 11.1 The Normal Probability Density Function 11.2 Z-scores 11.3 Properties of Z-scores 11.4 Areas under the Normal Curve for different quantile values 11.5 Checking for normality 11.6 Maximum Likelihood", " 11 The Normal Distribution The normal distribution is probably the most common distribution in all of probability and statistics. 11.1 The Normal Probability Density Function The probability density function for the normal distribution is defined as: \\(y_i = \\frac{1}{\\sigma\\sqrt2\\pi}e^{-(X_i-\\mu)^2/2\\sigma^2}\\) We can think of the model in this way (mathematical approach): \\(f(X) = \\frac{1}{\\sigma\\sqrt2\\pi}e^{-(X_i-\\mu)^2/2\\sigma^2}\\) Where the parameters (the symbols ) represent the mean, \\(\\mu\\) and the standard deviation, \\(\\sigma\\). What are some of the general characteristics of this model? Can you describe its shape? What are the parameters of the model? These are the quantities we will estimate in the fitting process. What are the variables used in the model? These are the observations. Below, two distributions are plotted from the ‘Standard Normal Distribution’, in this formulation: \\({\\sigma=1}\\) and \\({\\mu=0}\\). The normal distribution is an example of a continuous univariate probability distribution with infinite support. By infinite support, we mean that we can calculate values of the probability density function for all outcomes between \\(-\\infty\\) and \\(+\\infty\\). For clarification, the density value on the y-axis is not the resulting probability of obtaining the sampled value. Well, how do we get the probability from a probability density function? We need to integrate the density function given the value of the parameters. So from our example distribution with mean = 0 and standard deviation = 1, we can find the probability that an observed value will bebetween 0 and 1 by finding the area shown in the image below. \\(\\int_0^1f(x;\\mu,\\sigma)dx = P(0 &lt; X &lt; 1)\\) We can read this as “the integral of the probability density function between 0 and 1 (on the left-hand side) is equal to the probability that the outcome of the random variable is between zero and 1 (on the right-hand side)”. We can cover all possible values if we evaluate the density from \\(-\\infty\\) to \\(+\\infty\\). Therefore the following has to be true for the function to be a probability density function: \\(\\int_{-\\infty}^{\\infty}f(x)dx = 1\\). One last thing here: The probability of the random variable being equal to a specific outcome is 0, because the integral over x values of x to x is equal to zero. The definition of the definite integral: \\(\\int_{a}^{b}f(x)dx = \\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{f(x_i)\\Delta{x}}\\), where \\(x_i = a + i\\Delta{x}\\) and \\(\\Delta{x} = \\frac{b-a}{n}\\). If \\(a - b = 0\\), then \\(\\Delta{x} = 0\\). So the integral is zero: \\(\\int_{a}^{b}f(x)dx = \\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{0}\\), \\(\\underset{\\rm n \\rightarrow\\infty}{lim}\\sum_{i = 1}^n{0} = \\underset{\\rm n \\rightarrow\\infty}{lim}{0}\\), \\(\\underset{\\rm n \\rightarrow\\infty}{lim}{0} = 0\\). 11.2 Z-scores Z-scores are a way to center and scale the observations to understand how many standard deviations each value is away from the mean. Z-score values are measures of the ‘distance’ in standard deviations of a single value. \\(Z_i = \\frac{X_i-\\mu}{\\sigma}\\) Standardizing a score with respect to the other scores in the group. Expresses a score in terms of how many standard deviations it is away from the mean. Converts a distribution to a z-score distribution. Z-scores have mean = 0 and standard deviation = 1. 11.2.1 Centering with the mean 11.2.2 Scaling by the standard deviation 11.3 Properties of Z-scores A Z-scores, measure that quantifies how many standard deviations an observation is away from the mean of a distribution. Because of the scaling and centering we can quickly (and easily) convert any distribution of normally distributed random variables into Z-scores using the standard normal distribution (\\(\\mu = 0\\) and \\(\\sigma = 1\\)). The quantile value of: 1.96 is the maximum 2.5% of the standard normal distribution. -1.96 is the minimum 2.5% of the standard normal distribution. Thus, 95% of z-scores lie between \\(-1.96 \\le x_i \\le 1.96\\). 99% of z-scores lie between \\(-2.58 \\le x_i \\le 2.58\\). 99.9% of them lie between \\(-3.29 \\le x_i \\le 3.29\\). Let’s look at a Z Table to reinforce our understanding (Table B2 in Zar 4th edition). 11.3.1 R Code ## Provide code in R (using base R functionality) to illustrate and show the calculations for ## the Z-score. ## ```R ## # Sample data ## my_data &lt;- c(10, 12, 15, 18, 20) ## ## # Mean and Standard deviation ## my_mean &lt;- mean(my_data) # calculate mean of my_data ## my_sd &lt;- sd(my_data) # calculate standard deviation of my_data ## ## # Calculate the Z-score for each data point ## my_z_scores &lt;- (my_data - my_mean) / my_sd # calculate z-score of each data point ## ## # Print the Z-scores ## print(my_z_scores) # print calculated Z-scores ## ``` 11.4 Areas under the Normal Curve for different quantile values 11.4.1 R Code ## Provide code in R (using base R functionality) to determine the integral (area under the ## curve) from normal distributions using the function pnorm. ## ```R ## # area under the standard normal curve from -Inf to x ## pnorm(x) ## ## # area under the standard normal curve from a to b ## pnorm(b) - pnorm(a) ## ## # To find the area under a normal curve with a given mean &#39;m&#39; and standard deviation &#39;s&#39;, we first standardize the values using &#39;z = (x - m) / s&#39;, then use the &#39;pnorm()&#39; function: ## ## # area under a normal curve with mean m and std dev s from -Inf to x ## pnorm(x, mean = m, sd = s) ## ## # area under a normal curve with mean m and std dev s from a to b ## pnorm(b, mean = m, sd = s) - pnorm(a, mean = m, sd = s) ## ``` ## ## The inline comments provide brief explanations of what each line of code does. 11.4.2 R Code ## Provide code in R (using base R functionality) to determine the quantile for a given area ## under the curve from a normal distribution using the qnorm function. ## ```R ## area &lt;- 0.95 # Define the desired area under the curve as 0.95 ## quantile &lt;- qnorm(area) # Calculate the quantile using qnorm function ## quantile # Print the result ## ``` ## ## In this code, we have first defined the desired area under the curve as `0.95`. We have then used the `qnorm()` function to calculate the corresponding quantile value and stored it in the `quantile` variable. Finally, we have printed the result using the `print()` function. ## ## Note that the `qnorm()` function returns the quantile value corresponding to the desired area in a standard normal distribution (i.e., with a mean of 0 and standard deviation of 1). If you want to find the quantile value for a normal distribution with a different mean and/or standard deviation, you can use the `qnorm()` function with appropriate arguments (see the documentation of `qnorm()` for details). 11.5 Checking for normality Because many statistical tests we will be considering in this course are useful only if data are normally distributed we will need a way to assess if this is true. Are the observations normally distributed? 11.5.1 Qualitative Measures 11.5.2 QQ plot The QQ plot (Quantile Quantile) plot is a scatter plot that compares two sets of data. Here we will compare the observations (real-world data) to a theoretical data set that we would expect to see if the data came from a normal distribution with the sample \\(\\bar{X}\\) and \\(s\\), the reference data. If the distribution of the data is the same, the result will be a straight line. Each data value of the data is plotted along this reference line using the scale parameter. So, the question is, do these data come from a population of normally distributed values. First, rank your data and assign a probability to each value of the original data (empirical probability). obs rank.obs probability theoretical.quantiles observed.quantiles -2.215 1 0.005 -2.576 -2.587 -1.989 2 0.015 -2.170 -2.336 -1.805 3 0.025 -1.960 -2.131 -1.524 4 0.035 -1.812 -1.818 -1.471 5 0.045 -1.695 -1.759 -1.377 6 0.055 -1.598 -1.654 We build on the rank order of the data points to calculate the corresponding probabilty values. \\(p = \\frac{rank-\\frac{1}{2}}{n}\\). We then determine the theoretical quantiles, the theoretical standard normal quantiles for the calculated probability values. The quantiles from the observed data are Z scores calculated from the observed data \\(X_i\\), \\(\\bar{X}\\), and \\(s\\). Now lets look at the simulated data drawn from a non-normal distribution: Question, do these data come from a normal distribution? 11.5.3 R Code ## Provide code in R (using base R functionality) to make a quantile-quantile plot to perform ## a qualitative test of nomality from a simulated set of values. ## ```R ## # Generate a simulated set of data ## set.seed(123) # Set the seed for reproducibility ## sim_data &lt;- rnorm(1000) # Generate 1000 normally distributed random numbers ## ## # Create a Q-Q plot of the simulated data ## qqnorm(sim_data) # Plot the simulated data on the Q-Q plot ## qqline(sim_data) # Add a reference line to the Q-Q plot ## ``` ## ## This code snippet generates a simulated set of data consisting of 1000 normally distributed random numbers using the `rnorm()` function. We set the seed to ensure that the same set of random numbers is generated every time the code is run. ## ## We then create a Q-Q plot of the simulated data using the `qqnorm()` function. The `qqline()` function is used to add a reference line to the plot. ## ## The Q-Q plot allows us to compare the simulated data to a normal distribution; if the points on the plot fall roughly along the reference line, then the simulated data is assumed to be normally distributed. 11.6 Maximum Likelihood The primary source for the following lecture material are primarily derived from I.J. Myung’s paper “Tutorial on maximum likelihood estimation”. Journal of Mathematical Psychology 47 (2003) 90-100. The interested reader is directed to review this paper. In this lecture we will understand an alternative to parameter estimation using least-squares estimation, maximum likelihood estimation (MLE). MLE is a preferred method of parameter estimation in statistics and is a general parameter estimation approach, in particular in non-linear modeling and/or with non-normally distributed data. Because of the prevelance of non-normally distributed data in the natural sciences (e.g. data from counting, categorical data, skewed ratio data) an examination and familiarization of this approach is useful. 11.6.1 General Exprimental Approach Because many phenomenon of interest are not directly observable, we formulate hypotheses to test and these hypothesis are stated in terms of probability using statistical models. The goal of statistical modeling is to understand underlying processes by testing the viability (e.g. quality, robustness) of the model. Our method: Specify a statistical model Collect data Evaluate how well the model fits the data by: Parameter estimation Evaluating goodness of fit 11.6.2 Two approaches to parameter estimation: LSE (least-squares estimation), for normally-distributed data MLE, a general approach for parameter estimation 11.6.3 The probability density function The goal of data analysis is to identify the population that is most likely to have generated the sample - i.e. we will estimate the parameters of the candidate model that will produce these observations. The data vector \\(y = (y_1, y_2, ..., y_m)\\) is a random sample from a population distributed in some unknown way. Populations are identified using a probability distribution and unique values of the parameters - As the parameter changes in value, different probability distributions are generated. Let \\(f(y|w)\\) denote the probability density function (PDF) that specifies the probability of observing data vector y given the parameter w. The parameter \\(w = (w_1, ..., w_k)\\) is a vector defined on a multi-dimensional parameter space. For example, if the PDF is normal, \\(w = (\\mu, \\sigma)\\) If the PDF is a t distribution, \\(w = (d.f.)\\) Different distributions are defined using different parameters (and different mathematical formulations), so w is distribution-specific. If we have specified a distribution that has a certain set of parameters, for example: We can use a given probability distribution and parameter set to determine the probability of obtaining a value in a population. Example: Children’s IQ scores are normally distributed with a mean of 100 and a standard deviation of 15. What is the probability of a randomly selected child having an IQ between 80 and 120? In this case the area under the curve is 0.818 or 81.8% of the integral of the distribution from \\(-\\infty\\) to \\(+\\infty\\). So, if we take 100 random draws from the population of children’s IQs, we will get values of IQ &gt; 80 and &lt; 120, 81 to 82 times… Let’s examine the statement: \\(p(80 &lt; IQ &lt; 120\\) | \\(\\mu = 100, \\sigma = 15) = 0.818\\). We are stating that the probability of randomly selecting an IQ observation the variable characteristics given the parameter values is 0.818. If we are interested in finding probabilities of students with different variable characteristics (different IQ values), then we will change the left side of the equation. For example: \\(p(IQ &lt; 65\\) | \\(\\mu = 100, \\sigma = 15)\\) or \\(p(IQ &gt; 120\\) | \\(\\mu = 100, \\sigma = 15)\\) The right side of the equation does not change because it describes the fixed shape of the distribution of the population that “creates” the observations. So when we are investigating probability of an event we are quantifying the integral of the curve for the given parameter set bounded by the left side of the equation. We change the left side to derive new probability values. We can determine the probability of obtaining \\(p(y_1, y_2, ..., y_m)\\) | \\(w)\\) if the observations are independent. Think about taking multiple random draws from the population described by the parameter set w. An analog here is thinking about coin flipping, the probability of realizing specific outcomes from multiple trials is determined by multiplication (e.g. probability of observing two “tails” flips is the product of the independent single observations). So, the notation below is Pi (product) notation. This is used in mathematics to indicate repeated multiplication. \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = p(y_1\\)| \\(w) \\times p(y_2\\)| \\(w) \\times ...p(y_n\\)| \\(w)\\) \\(p(y_1, y_2, ..., y_m)\\) | \\(w) = \\prod_{i = 1}^{n}{p(y_i | w)}\\) 11.6.4 The likelihood function Now we will discuss the likelihood function that is derived by considering that the data are fixed and that the shape of the (parent) distribution is random. In this approach we are evaluating the inverse problem of the PDF. Specifically, given the observed data and a model of interest, we are interested in finding the unique parameter vector w, among all the possible combinations of parameters that is most likely to have produced the data. We have already observed the data and now want to define the likelihood function by reversing the roles (i.e. what is random what is fixed) of the data vector y and the parameter vector w. So, we focus on \\(L(w|y)\\). This represents the likelihood of the parameter w given the observed data y; and as such is a function of w. So, the data are fixed, and we modify the parameter vector w (in the case of IQ, \\(\\mu\\) and \\(\\sigma\\)) Assume we have some vector of observed data y, these are sampled from some population, for now, assume we take a single value. What is the likelihood that \\(\\mu = 100\\) and \\(\\sigma = 15\\) given our sampled IQ is \\(y = 120\\)? What if we have a vector of observations \\(n = 5\\), with mean values centered on 120? So, we want to find the distribution parameters that maximize the likelihood. We still think that the normal distribution is the most appropriate distribution as a candidate distribution. \\(y = (100, 110, 120, 130, 140)\\) "],["confidence-interval.html", "12 Confidence Interval 12.1 Example using Z-score 12.2 Confidence Intervals of the Mean from samples - using the t-distribution 12.3 Contrast standard deviation and standard error: 12.4 Calculate Confidence Interval", " 12 Confidence Interval Confidence intervals are statements about probability. For example, a confidence interval of a parameter with a 95% confidence level means that you are confident that 95 out of 100 times the estimate of the parameter will fall between the upper and lower values specified by the confidence interval. How do we determine confidence intervals? The first step is to determine a confidence level. The confidence level is 1 - \\(\\alpha\\). We will never know the magnitude of the true error of our esimated value. Instead, what we can estimate is the combined effect of sampling error and process error (the precision of \\(\\mu\\)). so, we attach a probability that the error is a certain size. Now, we want to solve for the statistic. \\(\\bar{X} = \\mu \\pm Error\\) Thought experiment: If we take an infinite number of samples from the population, we will get an estimate of the error term, \\(\\sigma\\). We cannot take an infinite number of samples (for obvious reasons). So, we need to estimate the value of the error that occurs from estimating the mean. This is called the standard error of the mean \\(S_{\\overline{X}}\\). 12.1 Example using Z-score Let’s first assume we know the population level parameters, \\(\\mu\\) and \\(\\sigma\\). We can use the Z-score. What is the Z value (bounds of negative and positive) in which 95% of the values are under the curve? Let’s look at a Z Table \\(\\mu = \\bar{X} \\pm 1.96\\sigma\\) (95% Certainty) \\(\\mu = \\bar{X} \\pm ? \\sigma\\) (99% Certainty) The above presumes that we have knowledge of the population parameters. We do not sample populations and infinite number of times. 12.2 Confidence Intervals of the Mean from samples - using the t-distribution In this example we will use the t-distribution. The t-distribution, also known as Student’s t-distribution, is a probability distribution that is widely used in statistical inference and hypothesis testing when the sample size is small or when the population standard deviation is unknown. Here are the key characteristics of the t-distribution: Shape: The t-distribution is symmetric and bell-shaped, similar to the normal distribution. However, it has heavier tails, which means it has more probability in the tails compared to the normal distribution. Degrees of freedom: The shape of the t-distribution is determined by its degrees of freedom (df). The degrees of freedom represent the sample size minus one (df = n - 1), where n is the number of observations in the sample. As the degrees of freedom increase, the t-distribution approaches the shape of the standard normal distribution. When the sample size is large (typically considered as n &gt; 30), the t-distribution is nearly identical to the standard normal distribution. Mean and variance: The mean of the t-distribution is 0, just like the standard normal distribution. However, the variance of the t-distribution depends on the degrees of freedom. For a t-distribution with df degrees of freedom, the variance is df / (df - 2) if df &gt; 2. If the degrees of freedom are less than or equal to 2, the variance is undefined. Tails: The t-distribution has thicker tails compared to the normal distribution. This means that extreme values are more likely to occur in the tails of the distribution, making it more robust to outliers and deviations from normality. Overall, the t-distribution is a valuable statistical tool when dealing with small sample sizes or uncertain population parameters, providing a way to make inferences and draw conclusions from limited data. Let’s look at a t Table ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. ## Scale for linetype is already present. ## Adding another scale for linetype, which will replace the existing scale. To determine the sampling interval from populations with unknown \\(\\sigma\\) we will use our best estimate of \\(\\sigma\\), which is \\(s\\) (sd). \\(SE = \\sigma_{\\bar{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) 12.3 Contrast standard deviation and standard error: Standard error and standard deviation are two related but distinct measures used in statistics. Standard Deviation: Standard deviation (SD) is a measure of the dispersion or variability of a set of data points. It quantifies how spread out the values are from the mean. A larger standard deviation indicates greater variability, while a smaller standard deviation indicates less variability. It is calculated as the square root of the variance. Standard Error: Standard error (SE) is a measure of the variability of a sample statistic. It quantifies how much the sample statistic (e.g., mean, proportion, regression coefficient) varies from sample to sample. The standard error provides an estimate of the precision or accuracy of the sample statistic as an estimate of the population parameter. In other words, it represents the average amount that the sample statistic differs from the true population parameter. The key difference between standard deviation and standard error lies in the populations they describe: Standard deviation characterizes the variability within a single sample or the entire population. It tells us how the individual data points are dispersed around the mean. For example, if you have a sample of test scores, the standard deviation would indicate how much the scores deviate from the mean score. Standard error, on the other hand, pertains to the sampling distribution of a statistic. It reflects the variability in the estimates of a parameter across different samples. For example, if you take multiple random samples from a population and calculate the mean of each sample, the standard error would measure the spread or variability of those sample means. In summary, the standard deviation quantifies the variability within a dataset, while the standard error quantifies the variability of a sample statistic (such as the mean) across different samples. The standard error provides information about the precision or reliability of the sample statistic as an estimate of the population parameter. 12.4 Calculate Confidence Interval Here, because we have incomplete knowledge of the population, we will use the t distribution to model the population variability. The distribution is flatter (has larger tails). What does this mean to our estimate of the confidence interval? To model the confidence interval we assume that the univariate distribution is t distributed. \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (95% Certainty, with n = 10?) \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (99% Certainty, with n = 10?) \\(CI = \\bar{X} \\pm t_{\\alpha, df}{s_{\\bar{X}}}\\) (99% Certainty, with n = 50?) Here is some code to document the calculation. # Sample data data &lt;- c(5.1, 4.9, 4.7, 4.8, 5.0, 4.9, 5.1, 4.8, 5.0, 4.9) # Confidence level confidence_level &lt;- 0.95 # Sample mean sample_mean &lt;- mean(data) # Sample standard deviation sample_sd &lt;- sd(data) # Sample size sample_size &lt;- length(data) # Degrees of freedom df &lt;- sample_size - 1 # Critical value for the t-distribution critical_value &lt;- qt((1 - confidence_level) / 2, df) # Margin of error margin_of_error &lt;- critical_value * (sample_sd / sqrt(sample_size)) # Confidence interval confidence_interval &lt;- c(sample_mean - margin_of_error, sample_mean + margin_of_error) # Print the results cat(&quot;Sample Mean:&quot;, sample_mean, &quot;\\n&quot;) cat(&quot;Sample Standard Deviation:&quot;, sample_sd, &quot;\\n&quot;) cat(&quot;Sample Size:&quot;, sample_size, &quot;\\n&quot;) cat(&quot;Confidence Level:&quot;, confidence_level, &quot;\\n&quot;) cat(&quot;Degrees of Freedom:&quot;, df, &quot;\\n&quot;) cat(&quot;Critical Value:&quot;, critical_value, &quot;\\n&quot;) cat(&quot;Margin of Error:&quot;, margin_of_error, &quot;\\n&quot;) cat(&quot;Confidence Interval:&quot;, confidence_interval, &quot;\\n&quot;) "],["null-hypothesis-significance-testing-nhst.html", "13 Null Hypothesis Significance Testing (NHST) 13.1 State testable hypothesis 13.2 Example of testing significance using Z-scores. 13.3 Example of testing significance to detect normality.", " 13 Null Hypothesis Significance Testing (NHST) Zar 6.4 NHST is a method of statistical inference by which an experimental factor is tested against a hypothesis of no effect (the null hypothesis) or no relationship based on a given observation. The first step in doing any statistical test is to state a testable hypothesis: \\(H_0\\) Null hypothesis \\(H_A\\) Alternative hypothesis Declare \\(\\alpha\\) level (this determines your quantile value) This is the level of significance that is a reference point to determine whether the results that you have is different from the null-hypothesis of no effect. Fisher recommended using \\(\\alpha = 0.05\\) to judge whether an effect is significant or not. If you recall, this alpha level is roughly two standard deviations away from the mean for the normal distribution. From Fisher: ‘The value for which p = 0.05, or 1 in 20, is 1.96 or nearly 2; it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not’. How small the level of significance is, is left to researchers (domain knowledge). The experimental procedure: Collect Data State falsifiable null hypothesis Conduct statistical test Compare the test statistic to the critical value (determined by \\(\\alpha\\)). This provides some measure of objectivity. State the resulting probability (the p-value), less than or greater than \\(\\alpha\\). The p-value is the probability of obtaining test results at least as extreme as the results actually observed if the null hypothesis is true. 13.1 State testable hypothesis These are a set of mutually exclusive and exhaustive outcomes The test statistic will support one or the other outcomes \\(H_0: \\mu = 0\\), \\(H_A:\\mu \\ne 0\\) \\(H_0: \\mu = 3.5 cm\\), \\(H_A:\\mu \\ne 3.5 cm\\) \\(H_0: \\mu = 10.5 kg\\), \\(H_A:\\mu \\ne 10.5 kg\\) 13.2 Example of testing significance using Z-scores. Let’s examine a simple statistical test. We will test the hypothesis: Is the mean fuel consumption of a population of buses equal to 20 mpg? What is the null hypothesis? We need information about the population (remember we are using Z-score so we know the population-level parameters \\(\\mu\\) and \\(\\sigma\\)). Determine the associated probability that the mean is 20 mpg given: \\(\\sigma\\) = 0.3, \\(\\mu\\) = 19.1 \\(Z_{calc} = \\frac{\\overline{X} - \\mu}{\\sigma}\\) What is the probability that we would get \\(Z_{calc}\\)? Let’s compare this to the test value of \\(Z\\), \\(Z_{test}\\). \\(Z_{test}\\) is the quantile of \\(Z\\) that results from a given \\(\\alpha\\). Given our declared \\(\\alpha\\), how does the resulting probability compare? Remember, \\(\\alpha\\) is defined prior to statistical testing If our \\(Z_{calc}\\) &gt; \\(Z_{test}\\), then we are in a ‘region of rejection’, i.e. rejection of the null hypothesis. Now we have a way to objectively reject or accept the null hypothesis. 13.2.1 R Code ## Provide code in R (using base R functionality) to determine how many standard deviations ## away from the mean a given data point is and if this is statistically significant. ## ``` ## # create data vector ## data &lt;- c(5, 7, 9, 11, 13, 15) ## ## # calculate mean and standard deviation of data ## mean_data &lt;- mean(data) # calculates the mean of the data vector ## sd_data &lt;- sd(data) # calculates the standard deviation of the data vector ## ## # calculate the number of standard deviations a given data point is away from the mean ## given_data_point &lt;- 12 # defines the value to calculate the z-score for ## num_sd_away &lt;- (given_data_point - mean_data) / sd_data # calculates the z-score for the given data point ## ## # determine if the number of standard deviations is statistically significant ## if (abs(num_sd_away) &gt;= 1.96) { # checks if the absolute value of z-score is greater than or equal to 1.96 ## cat(&#39;The given data point is statistically significant.\\n&#39;) # if true, prints that the given data point is statistically significant ## } else { ## cat(&#39;The given data point is not statistically significant.\\n&#39;) # if false, prints that the given data point is not statistically significant ## } ## ``` 13.3 Example of testing significance to detect normality. There are a variety of quantitative ways to assess goodness of fit (GOF) of normality. These are statistical tests and we will evaulate them using models and data. 13.3.1 Kolmogorov-Smirnoff Test This is a commonly used statistical test to see if your data is normally distributed. The Kolmogorov-Smirnov Goodness of Fit Test (K-S test) compares the distribution of observations (your univariate data) with those from a specified candidate distribution to understand if they come from the same distribution (in this case normal). We will test to see if our observations (data) are close to the expectations (those values we would expect if the data came from a normal distribution). In this test (and all statistical tests) we will determine if the difference in observed and expected values indicate that the distribution is similar or different. If the difference between observation and expectation is small, then we would likely say that the data are distributed normally. If the difference in observed and expected values is large then we say that the data must come from some other distribution, and the data are not normally distributed. The hypotheses for the test are: Null hypothesis (\\(H_0\\)): the data comes from the specified distribution. Alternate Hypothesis (\\(H_A\\)): at least one value does not match the specified distribution. \\(H_0: P = P_0\\) \\(H_A: P \\ne P_0\\) Here P is the sample density and \\(P_0\\) is a specified distribution. So, we need to evaluate two distributions, the one resulting from our sampling (s, sampled cumulative distribution \\(F_S(x)\\) and the ones we would expect from a theoretical cumulative distribution, \\(F_T(x)\\). The difference between the two distribution functions is evaluated by the test statistic D, the greatest vertical distance between \\(F_S(x)\\) and \\(F_T(x)\\). \\(D = \\underset{\\rm x}{sup}|F_S(x) - F_T(x)|\\), where \\(\\underset{\\rm x}{sup}\\) is the supremum of the set of distances. Intuitively, the statistic takes the largest absolute difference between the two distribution functions across all x values. The K-S test statistic measures the largest distance between the EDF \\(F_{data}\\) and the theoretical function \\(F_{data}\\), measured in a vertical direction (Kolmogorov as cited in Stephens 1992). The question is whether these data are normally distributed: First we will create a CDF (Cumulative Distribution Function) for the sample data (n = 30 observations), \\(F_S(x)\\)), Observations Freq. (Relative Freq) Cumulative Frequency, F_S(x) -2.2 1 (1/30) 0.033 (1/30) -2 1 (1/30) 0.067 (2/30) -1.5 1 (1/30) 0.1 (3/30) -0.8 2 (2/30) 0.167 (5/30) Now, we find the associated Z score for each value of the empirically derived Cumulative Frequency (\\(F_S(x)\\)): Observations Frequency F_S(x) Z.score.obs -2.2 1 0.033 -2.205 -2.0 1 0.067 -2.007 -1.5 1 0.100 -1.511 -0.8 2 0.167 -0.816 Now, we find the associated probability for each Z score from the empirical cumulative distribution, this is \\(F_T(x)\\). We get these from the Z table in Zar. P(Z = -2.205)? Observations Frequency F_S(x) Z.score.obs F_T(X) -2.2 1 0.033 -2.205 0.014 -2.0 1 0.067 -2.007 0.022 -1.5 1 0.100 -1.511 0.064 -0.8 2 0.167 -0.816 0.203 Finally, we will calculate the test statistic D. This if you recall is the maximum difference in the absolute value of \\(F_S(x) and F_T(x)\\). Remember: \\(D = \\underset{\\rm x}{sup}|F_S(x) - F_T(x)|\\). Observations Frequency F_S(x) Z.score.obs F_T(X) D -2.2 1 0.033 -2.205 0.014 0.019 -2.0 1 0.067 -2.007 0.022 0.045 -1.5 1 0.100 -1.511 0.064 0.036 -0.8 2 0.167 -0.816 0.203 0.036 -0.6 2 0.233 -0.618 0.263 0.030 -0.5 1 0.267 -0.519 0.295 0.028 -0.3 1 0.300 -0.320 0.367 0.067 -0.2 1 0.333 -0.221 0.404 0.071 -0.1 1 0.367 -0.122 0.443 0.076 0.0 2 0.433 -0.023 0.482 0.049 0.1 1 0.467 0.077 0.521 0.054 0.2 1 0.500 0.176 0.560 0.060 0.3 1 0.533 0.275 0.599 0.066 0.4 2 0.600 0.374 0.636 0.036 0.5 1 0.633 0.473 0.672 0.039 0.6 3 0.733 0.573 0.707 0.026 0.7 1 0.767 0.672 0.740 0.027 0.8 2 0.833 0.771 0.771 0.062 0.9 2 0.900 0.870 0.800 0.100 1.1 1 0.933 1.069 0.850 0.083 1.5 1 0.967 1.466 0.924 0.043 1.6 1 1.000 1.565 0.937 0.063 The open question then, how do we use the calculated \\(D\\) value, \\(D = 0.100\\) for evaluating our \\(n = 30\\) samples? Use the lookup table in Zar 4th edition, Appendix Table B.9 or Critical Values of D This will give us the ‘critical value’ for the test. The test value is \\(D = 0.100\\), the critical value is \\(\\alpha = 0.05\\) and \\(n = 30\\) is 0.24170. Remember, we are testing the Null hypothesis (\\(H_0\\)) that the data comes from the specified distribution. If our test value of \\(D\\) exceed the critical value we would reject the null hypothesis. However, it does not. \\(D = 0.100\\) \\(0.100 &lt; D_{critical, n = 30, \\alpha = 0.05}\\). 13.3.2 R Code ## Provide code in R (using base R functionality) to calculate the Kolmogorov-Smirnoff test ## of nomality from a simulated set of values. ## ``` ## # Generate a simulated set of values ## x &lt;- rnorm(100, mean = 5, sd = 2) # Generate 100 random numbers from a normal distribution with mean 5 and standard deviation 2. Store the values in x. ## ## # Perform the Kolmogorov-Smirnov test for normality ## ks.test(x, &#39;pnorm&#39;, mean(x), sd(x)) # Use the ks.test function to perform the Kolmogorov-Smirnov test for normality. The comparison cumulative distribution function used is pnorm, since we are testing for normality. The mean and standard deviation of x are passed as arguments to the function. ## ``` 13.3.3 Shapiro Wilk test This is one we will use extensively. The Shapiro-Wilk test is a statistical test used to assess the normality of a dataset. It was developed by Samuel Sanford Shapiro and Martin Wilk in 1965. The test determines whether a given sample comes from a population that follows a normal distribution. The null hypothesis of the Shapiro-Wilk test is that the data is normally distributed. The alternative hypothesis is that the data does not follow a normal distribution. The test calculates a test statistic, W, which is based on the observed and expected values of the ordered sample values. The test statistic is then compared to critical values from the Shapiro-Wilk table or determined using software. 13.3.4 R Code ## Provide code in R (using base R functionality) to test that a set of data is normally ## distributed using the Shapiro-Wilk test ## ``` R ## # Generate some sample data ## data &lt;- rnorm(100) # Using the rnorm() function to generate a random sample of 100 normally distributed data points and assigning it to a variable named &quot;data&quot; ## ## # Test normality using the Shapiro-Wilk test ## shapiro.test(data) # Using the shapiro.test() function to perform the Shapiro-Wilk test on the data to test the null hypothesis that the data is normally distributed ## ``` "],["one--and-two-tailed-tests.html", "14 One- and Two-Tailed Tests", " 14 One- and Two-Tailed Tests In a two-tailed test, the alternative hypothesis is typically expressed as “not equal to” or “different from” the null hypothesis. It allows for the possibility of the observed data being significantly greater or significantly smaller than what would be expected under the null hypothesis. Two-tailed tests are often used when the research question or hypothesis is not specific about the direction of the effect or when there is a genuine interest in detecting differences in both directions. In some cases we care about the direction of the difference (is the value less than or greater than some value). In statistical hypothesis testing (NHST), a one-tailed test refers to a type of hypothesis test that focuses on the possibility of a significant difference or relationship in only one direction, either greater than or less than the null hypothesis. Use one-tailed test In general, one-tailed hypotheses about a mean are: \\(H_0:\\mu\\ge\\mu_0\\) and \\(H_A:\\mu&lt;\\mu_0\\) In which case, H0 is rejected if the test statistic is in the left-hand tail of the distribution or: \\(H_0:\\mu\\le\\mu_0\\) and \\(H_A:\\mu&gt;\\mu_0\\) Contrast the region of rejection for these. "],["statistical-power.html", "15 Statistical Power 15.1 Type-1 and Type-2 Errors 15.2 Type 1 (\\(\\alpha\\)) Error 15.3 Type 2 (\\(\\beta\\)) Error 15.4 Tables of Error Types 15.5 What Influences Statistical Power?", " 15 Statistical Power Statistical power is: The probability that the test correctly rejects the null hypothesis. That is, the probability of a true positive result. Zar defines statistical power as: “The probability that the statistical test will correctly reject the null hypothesis”. The power of a hypothesis test is between 0 and 1; If the power is close to 1, the hypothesis test is very good at detecting a false null hypothesis. 15.1 Type-1 and Type-2 Errors Sometimes we: Reject the null hypothesis when it is true. Accept the alternative hypothesis when it is false. These are error that we have made and occure even when following the correct statistical procedure and doing all of the calculations correctly. How do these errors arise? 15.2 Type 1 (\\(\\alpha\\)) Error Type 1 error or alpha error - probability of rejecting \\(H_0\\) when it is true. Type 1 error rate is equal to \\(\\alpha\\). Type 1 error: “rejecting the null hypothesis when it is true.” We rejected the null hypothesis but did so erroneously.The samples available to us indicated that a difference existed when there was in fact no difference and the null hypothesis should have been accepted. Type 1 error is termed ‘\\(\\alpha\\) error’ because it is equal to \\(\\alpha\\) Now we have some criteria to choose alpha. So if your \\(\\alpha\\), or critical value is 0.10 we have a 10% probability of rejecting the null hypothesis when we should have, in fact, accepted it. 15.3 Type 2 (\\(\\beta\\)) Error Type 2 error: “accepting the null hypothesis when it is false.” Type 2 error or ‘\\(\\beta\\) error’ is equal to \\(\\beta\\). 15.4 Tables of Error Types If H0 is true If H0 is false If H0 is rejected Type I error No error If H0 is not rejected No error Type II error Two Types of Errors in Hypothesis Testing If H0 is true If H0 is false If H0 is rejected \\(\\alpha\\) \\(1-\\beta\\) (“power”) No error If H0 is not rejected No error \\(1-\\alpha\\) \\(\\beta\\) Long-term Probabilities of Outcomes in Hypothesis Testing Beta is commonly set at 0.2, but may be set by the researchers to be smaller. Consequently, power may be as low as 0.8, but may be higher. Powers lower than 0.8, while not impossible, would typically be considered too low for most areas of research. 15.5 What Influences Statistical Power? These are listed in Zar, we can take them step by step to learn why power is influenced. Significance level (or alpha) Sample Size - Power depends on sample size. Other things being equal, larger sample size yields higher power. Variance - Power also depends on variance: smaller variance yields higher power. Experimental Design - Power can sometimes be increased by adopting a different experimental design that has lower error variance. For example, stratified sampling can reduce error variance and hence increase power. Magnitude of the effect of the variable. "],["bivariate-relationships.html", "16 Bivariate Relationships 16.1 Direction of Relationships 16.2 Limitations of Covariance 16.3 The Correlation Coefficient 16.4 Non-parametric Correlation", " 16 Bivariate Relationships We often have information on two numeric characteristics for each member of a group and are interested in finding the degree of association between these characteristics. For instance, an obstetrician may decide to look up the records of women who delivered in her hospital in the previous year to find out whether there is a relationship between their family incomes and the birth weights of their babies. The relationship here means whether the two variables fluctuate together, i.e., does thebirth weight increase (or decrease) as the income increases. Parametric approaches Pearson’s correlation coefficient Nonparametric approaches Spearman’s rho Kendall’s tau 16.1 Direction of Relationships 16.1.1 Positive Relationship 16.1.2 Negative Relationship 16.1.3 No Relationship 16.1.4 Quantifying the magnitude of correlation As one variable increases, does the other increase, decrease or not change at all (stay the same)? This can be done by understood by calculating the covariance. We look at how much each score deviates from their respective mean values. If both variables deviate from the mean in a similar way, they are likely to be related (or ‘covary’). Here is the results (bivariate) of an experiment aimed at understanding the efficacy of advertising: Participant number (i) 1 2 3 4 5 Mean SD Adverts Watched (X) 5 4 4 6 8 5.4 1.67 Packets Bought (Y) 8 9 10 13 15 11 2.92 Residual error values: Remember residuals are the value of the observation - expectation. Participant number (i) 1 2 3 4 5 Mean SD Adverts Watched 5 4 4 6 8 5.4 1.67 Packets Bought 8 9 10 13 15 11 2.92 Advertiser Residual -0.4 -1.4 -1.4 0.6 2.6 Packets residual -3 -2 -1 2 4 16.1.5 Covariance and a re-examination of variance Remember the variance tells us how much scores deviate from the mean for a single variable. It is closely linked to the sum of squares, indeed we use the sum of squares to calculate the variance. Covariance is similar - it tells is by how much scores on two variables differ from their respective means. \\(s^2=\\frac{\\Sigma(X_i - \\bar{X})}{n-1}^2\\) \\(s^2=\\frac{\\Sigma(X_i - \\bar{X})(X_i - \\bar{X})}{n-1}\\) 16.1.6 Here we are examining the ‘covariance’ so the calculation changes a bit. Calculate the error (residual value) between the mean and each subject’s score for the first variable (X). Calculate the error (residual value) between the mean and their score for the second variable (y). Multiply the error values (the residual values). Add these values and you get the cross product deviations. The covariance is the mean (average) of the cross-product deviations: \\(cov(X,Y)=\\frac{\\Sigma(X_i - \\bar{X})(Y_i - \\bar{Y})}{n-1}\\) From our example, and plugging in the values: \\(cov(X,Y)=\\frac{(-0.4)(-3)+(-1.4)(-2)+(-1.4)(-1)+(0.6)(2)+(2.6)(4)}{4}\\) \\(cov(X,Y)=\\frac{1.2+2.8+1.4+1.2+10.4}{4}\\) \\(cov(X,Y)=\\frac{17}{4}\\) \\(cov(X,Y)=4.25\\) 16.2 Limitations of Covariance The magnitude of the covariance is dependent on the units of measurement. e.g. the covariance of two variables measured in miles might be 4.25, but if the same scores are converted to kilometres, the covariance is changed… To address this issue we can standarize the covariance value by standardization: Divide by the standard deviations of both variables. The standardized version of covariance is known as the correlation coefficient. It is unaffected by units of measurement. 16.3 The Correlation Coefficient \\(r=\\frac{cov_{X,Y}}{s_Xs_Y}\\) \\(r=\\frac{\\Sigma(X_i - \\bar{X})(Y_i - \\bar{Y})}{(n-1)s_Xs_Y}\\) \\(r=\\frac{cov_{XY}}{s_Xs_Y}\\) \\(r=\\frac{4.25}{1.67 * 2.92}\\) \\(r=0.87\\) Termed Pearson-product moment correlation coefficient It ranges between -1 and +1 A value of zero, indicates that there is no relationship It is a testable hypothesis Testing \\(H_0: \\rho=0\\) versus \\(H_A: \\rho\\ne0\\) The standard error of the correlation coefficient is calculated as: \\(S_r=\\sqrt\\frac{1-r^2}{n-2}\\) It is a testable hypothesis r = 0.870 n = 12 (new data set, with more samples) We will calculate the critical value: \\(t=\\frac{r}{S_r}= \\frac{0.870}{0.156}= 5.58\\) t0.05(2),10 =2.228 Testing \\(H_0: \\rho=0\\) versus \\(H_A: \\rho\\ne0\\) Coefficient of determination, r^2 By squaring the value of r you get the proportion of variance in one variable shared by the other. Square of correlation coefficient (\\(r^2\\)), known as coefficient of determination, represents the proportion of variation in one variable that is accounted for by the variation in the other variable. For example, if height and weight of a group of persons have a correlation coefficient of (\\(\\rho = 0.80\\)), one can estimate that 64% (0.80 × 0.80 = 0.64) of variation in their weights is accounted for by the variation in their heights. 16.4 Non-parametric Correlation The Spearman’s Rank Correlation Coefficient is used to discover the strength of a link between two sets of data. This worked example is taken from: https://geographyfieldwork.com/SpearmansRank.htm. I found it to be a very elegant and real world example. There is also the case of tied ranks in this example, which is useful. Hypothesis: We might expect to find that the price of a bottle of water decreases as distance from the Contemporary Art Museum (tourist area) increases. Higher property rents close to the museum should be reflected in higher prices in the shops. The hypothesis might be written like this: The price of a convenience item decreases as distance from the Contemporary Art Museum increases. Note in the column “Rank Price” there is no rank 3 or 4. In this case of the tied ranks Store Distance (m) Rank distance Price Rank price Difference between ranks (d) d x d 1 50 10 1.80 2.0 8.0 64.00 2 175 9 1.20 3.5 5.5 30.25 3 270 8 2.00 1.0 7.0 49.00 4 375 7 1.00 6.0 1.0 1.00 5 425 6 1.00 6.0 0.0 0.00 6 580 5 1.20 3.5 1.5 2.25 7 710 4 0.80 9.0 -5.0 25.00 8 790 3 0.60 10.0 -7.0 49.00 9 890 2 1.00 6.0 -4.0 16.00 10 980 1 0.85 8.0 -7.0 49.00 Next, we will calculate the coefficient (\\(R_s\\)) using the formula below. The answer will always be between 1.0 (a perfect positive correlation) and -1.0 (a perfect negative correlation). \\(R_s = 1 - \\frac{6\\Sigma{d^2}}{n^3-n}\\). Now to put all these values into the formula. Find the value of all the \\(d^2\\) values by adding up all the values in the difference² column. In our example this is 285.5. Multiplying this by 6 gives 1713. Now for the bottom line of the equation. The value n is the number of sites at which you took measurements. This, in our example is 10. Substituting these values into n³ - n we get 1000 - 10 We now have the formula: \\(R_s = 1 - \\frac{1713}{990}\\), which gives a value for R: 1 - 1.73 = -0.73 The closer (\\(R_s\\)) is to +1 or -1, the stronger the likely correlation. A perfect positive correlation is +1 and a perfect negative correlation is -1. The Rs value of -0.73 suggests a fairly strong negative relationship. Now, let’s determine if this calculated value is signifcantly different from zero. We will use the Student’s t-distribution. Similar to Pearson’s coefficient, the t-value is found by: \\(t = R_s\\sqrt{\\frac{n - 2}{1 - {R_s}^2}}\\) \\(t_{calc} = -0.73\\sqrt{\\frac{8}{1 - ({-0.73})^2}}\\) \\(t_{critical} = t_{\\alpha = 0.05, df = 9} = 2.262\\), this is Table B.3 in Zar 4th edition (Critical Values of the t distribution). So, we would reject the null hypothesis, "],["linear-regression.html", "17 Linear Regression 17.1 What is Regression? 17.2 Assumptions of Simple Linear Regression 17.3 Model for Linear Relationship is a two-parameter model 17.4 Intercepts and Gradients 17.5 The Method of Least Squares 17.6 Sum of Squares Components 17.7 Total SS (SST, SST) 17.8 Residual SS or Error SS (SSR) 17.9 Model SS or Regression SS (SSM) 17.10 The SS components are related and useful for understanding the model 17.11 Variance Ratio Test 17.12 Test \\(\\beta_1 = 0\\) 17.13 Worked Example 17.14 Regression: An Example 17.15 Output of a Simple Regression 17.16 Using the Model 17.17 Analysis of the slope coeffient 17.18 An R example 17.19 Prediction and confidence intervals 17.20 An R example - Confidence Intervals 17.21 An R example - Prediction Intervals", " 17 Linear Regression Analysis with a one independent (X) and dependent variable (Y) is termed “Simple” linear regression. 17.1 What is Regression? A way of predicting the value of one variable from another. It is a model of the relationship between two variables. The modeled relationship is linear. Therefore, we describe the relationship using the equation for a straight line. To do the fitting and the hypothesis testing, we will evaluate the following quantities: Total sum of squares (TSS or \\(SS_T\\)) Model sum of squares (Regression SS, \\(SS_M\\)) Residual sum of squares (RSS or \\(SS_R\\)) We will use these quantities to understand two things: Is \\(\\beta_1 = 0\\) (using the confidence intervals)? We would like to know if X is a good predictor of Y and this is determined by evaluating the null hypothesis, \\(\\beta_1 = 0\\). Is the model significant (using an F-test)? Specifically does the fit of model fit the data better than does the null model (more on this below). 17.2 Assumptions of Simple Linear Regression For each value of X, Y are randomly sampled and independent. For any value of X in the population there exists a normal distribution of Y values There is homogeneity of variance in the population. i.e. the variance of the normal distribution of Y values in population are equal for all of values of X. X is measured without error The exptected value of Y at the \\(i^{th}\\) value of X is \\(E(Y_i)=\\beta_0 + \\beta_1X_i\\) 17.3 Model for Linear Relationship is a two-parameter model The first parameter is \\(\\beta_1\\) Gradient (slope) of the regression line. This indicates the direction of the relationship. The second parameter is \\(\\beta_0\\) Intercept (value of Y when X = 0). Location where the regression line crosses the Y-axis (ordinate). 17.4 Intercepts and Gradients 17.5 The Method of Least Squares This is the approach we will use for parameter estimation and inference in this course. This figure shows a scatterplot of some data with a line representing the best fit model. x &lt;- c(2,3,4,6,7,8,10,11,14,15,17,18,20,21,23) y &lt;- c(5,10,7,11,20,13,15,30,27,37,35,30,32,35,40) plot(x, y, xlim = c(0,25), ylim = c(0,45), pch = 1, xlab = &quot;Size of Spider&quot;, ylab = &quot;Anxiety&quot;) segments(x0 = 0, y0 = 3, x1 = 23, y1 = 41) This figure shows the residual (observed - expected) value of the best fit model. Our task is to minimize the \\(SS\\) value. Let’s look at a some cases for alternative candidate model parameters: Our task is to minimize the \\(SS\\) value. 17.6 Sum of Squares Components Once the best fit model is derived, we can make inference about the quality of the model (relative to the null model) and the hypothesis \\(\\beta_1 = 0\\). Let’s take a close look at the three sum of squares components. 17.7 Total SS (SST, SST) Total variability (variability between scores and the mean) in the observations. TSS is the sum of the squared residuals when the most basic model is applied to the data. How good is the mean as a model to the observed data? Lets consider the mean of Y to be the null model: SST uses the differences between the observed data and the mean value of Y (the null model) \\(TSS =\\sum(Y_i - \\bar{Y})^2\\) 17.8 Residual SS or Error SS (SSR) SSR is the error that is derived between the regression model and the actual data). Difference between the observation and the model Represents the degree of inaccuracy when fitting the model to the data. Residual/error variability (variability between the regression model and the actual data). \\(RSS =\\sum(Y_i - \\hat{Y})^2\\) 17.9 Model SS or Regression SS (SSM) SSM is the difference between the prediction from the regression model and the mean (null model). This is the improvement we get from fitting the model to the data relative to the null model. \\(SS_M=\\sum(\\hat{Y}_i - \\bar{Y})^2\\) 17.10 The SS components are related and useful for understanding the model The total variabilty is partitioned between the model and the residual: \\(SS_T = SS_M + SS_R\\) We calculate the sum of squares components to evaluate the amount of systematic variance (regression/model) and the amount of unsystematic (residual) variance. If we have a good model, the total variance in the observations will be described to a great extent by the model. The proportion of improvement due to the model. \\(R^2 = SS_M/SS_T\\), percentage of variation explained by the model. 17.11 Variance Ratio Test We will evalute the model using an F test - “termed variance ratio test”. We will be evaluating the ratio of SSM and SSR. We will use the SSM and SSR to calculate quantities “mean squares” This will be done by dividing the SSM and SSR by their respective degrees of freedom (df). These terms can be expressed as averages, divided by df terms. df for SSM (The number of parameters in the model - 1) df for SSR (number of obs - number of parameters in the model) These quantities are called mean squares. Mean Squares Model; MSM = SSM/dfM Mean Squares Residual: MSR = SSR/dfR F-test is termed the “variance ratio test” \\(F=\\frac{MS_M}{MS_R}\\) 17.12 Test \\(\\beta_1 = 0\\) To test whether \\(\\beta_1 = 0\\) we will use the t-distribution. After determining the model parameters values we can estimate the standard error of the slope paramter. The approach is to determine the confidence interval of the slope (\\(\\beta_1\\)) for a given value of \\(\\alpha\\). Look at the following equation, what values will we need? \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{(\\sum{{Y_i - \\hat{Y_i}}})^2}{df}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{(\\sum{{Y_i - \\hat{Y_i}}})^2}{(n - 2)}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) Degrees of freedom: For simple linear regression (one independent and one dependent variable), the degrees of freedom (df) is equal to, n - 2. Our calculated test statistic is: \\(t = \\frac{\\beta_1}{SE_{\\beta_1}}\\) We will compare this to the critical value: \\(t_{\\alpha, 2-tailed, df}\\) 17.13 Worked Example Age (days) (X) Wing Length (cm) (Y) 3.0 1.4 4.0 1.5 5.0 2.2 6.0 2.4 8.0 3.1 9.0 3.2 10.0 3.2 11.0 3.9 12.0 4.1 14.0 4.7 15.0 4.5 16.0 5.2 17.0 5.0 We find that the TSS is = 19.65 We find that the Model SS = 19.13 Source of Variation Sum of Squares (SS) df Mean Squares (MS) Total \\(\\sum(Y_i-\\bar{Y})^2\\) Model (or Regression) \\(\\sum(\\hat{Y}_i-\\bar{Y})^2\\) The number of parameters in the model - 1 SSM/dfM Residual (or Error) \\(\\sum(Y_i-\\hat{Y_i})^2\\) The number of obs - the number of parameters in the model SSR/dfR 17.13.1 Summary of the Calculations for Model Fit We are falsifying the null hypothesis: \\(H_0\\): There is no statistically significant relationship between variables x and y. Source of Variance SS df MS Total 19.65 12 Model (or Regression) 19.13 1 19.13 Residual (or Error) 0.52 11 0.047 \\(F=\\frac{19.132214}{0.047701}=401.1\\) \\(F_{0.05(1),1.11} = 4.84\\) Therefore, reject H0 P &lt; 0.0005 17.14 Regression: An Example A record company executive is interested in predicting record sales from advertising. Data are: 200 different album releases Outcome variable: Sales (CDs and downloads) in the week after release Predictor variable: The amount (in units of $1,000) spent promoting the record before release. 17.15 Output of a Simple Regression In R: Coefficients: Estimate Std. Error t value Pr(&gt; (intercept) 134 7.53 17.799 &lt; \\(2\\times10^{-16}\\) *** Adverts 0.096 0.00963 9.979 &lt; \\(2\\times10^{-16}\\) *** Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 Residual standard error: 65.99 on 198 degrees of freedom Multiple R-squared: 0.3346, Adjusted R-squared: 0.3313 F-statistic: 99.59 on 1 and 198 df, p-value: &lt; 2.2e-16 17.16 Using the Model Expected Record Salesi = 134.14 + (0.096 x Advertising Budgeti) E(Record sales if Budget = 100) = 134.14 + (0.096 x 100) E(Record sales if Budget = 100) = 143.75 17.17 Analysis of the slope coeffient Using the fact that \\(\\beta_1\\) is approximately normally distributed in large samples, we can test the hypothesis that \\(\\beta_1 = 0\\). From our reading, we learned that the calculated value of the t-statistic is: \\(t = \\frac{\\beta_{observed} - {\\beta_{expected}}}{{SE_{\\beta}}}\\). For the hypothesis testing, we: Compute the standard error of \\(\\beta_1\\), \\(SE_{\\beta_1}\\): \\(SE_{\\beta_1}=\\frac {\\sqrt{\\frac{\\sum{{Y_i - \\hat{Y_i}}})^2}{{n - 2}}}}{\\sqrt{\\sum{(X_i - \\bar{X})^2}}}\\) Compute the t-statistic, in this case, we are comparing our observed value of \\(\\beta_1\\) to the expected value, if the null hypothesis is true. So, our expected value is \\(\\beta_1 = 0\\). And we divide this difference by \\(SE_{\\beta_1}\\). \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\). \\(t = \\frac{\\hat{\\beta_1}}{SE_{\\beta_1}}\\). This is our computed t value. We calculate the critical value of t, which is derived using the degrees of freedom. For simple linear regression (one independent and one dependent variable), the degrees of freedom (df) is equal to, n - 2. To determine the 95% CI of \\(\\beta_1\\), we can rearrange the above equation: \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\). \\(t SE_{\\beta_1} = \\hat{\\beta_1}\\). \\(\\hat{\\beta_1} = t_{\\alpha, df} SE_{\\beta_1}\\). If \\(\\alpha = 0.05\\), then we would get the 95% confidence interval of \\(\\beta_1\\). 17.18 An R example data(mtcars) plot(mtcars$mpg ~ mtcars$hp) model. &lt;- lm(mtcars$mpg ~ mtcars$hp) summary(model.)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.09886054 1.6339210 18.421246 6.642736e-18 ## mtcars$hp -0.06822828 0.0101193 -6.742389 1.787835e-07 The second column of the coefficients’ summary, reports \\(SE_{\\beta_1}\\) and \\(SE_{\\beta_0}\\). In the third column t value, we find the t statistics suitable for tests of the separate hypotheses: \\(\\beta_o = 0\\) and \\(\\beta_1 = 0\\). Furthermore, the output provides us with p-values corresponding to both tests against the two-sided hypothesis in the fourth column of the table. Given these results: \\(\\beta_1 = -0.06822828\\) \\(SE_{\\beta_1} = 0.0101193\\) \\(t = \\frac{\\hat{\\beta_1} - 0}{SE_{\\beta_1}}\\) \\(t = \\frac{ -0.068}{0.0101}\\). \\(t = -6.742\\). 95% CI of \\(\\beta_1\\): \\(t_{\\alpha=0.05, df = (n - k -1)}\\) \\(t_{\\alpha=0.05, df = (32 - 2 -1)}\\) \\(t_{\\alpha=0.05, df = (29)}\\) \\(t_{\\alpha=0.05, df = (29)} = -1.699\\) 95% CI of \\(\\beta_1\\) = \\(t_{\\alpha=0.05, df = 29}SE_{\\beta_1}\\) } \\(\\beta_1 = -0.068 \\pm 0.017\\) 17.19 Prediction and confidence intervals Sometimes (often) we will want to use the model to predict values. For example, suppose we fit a simple linear regression model using hours studied as a predictor variable and exam score as the response variable. Using this model, we might predict that a student who studies for 6 hours will receive an exam score of 91. However, because there is uncertainty around this prediction, we might create a prediction interval that says there is a 95% chance that a student who studies for 6 hours will receive an exam score between 85 and 97. This range of values is known as a 95% prediction interval and it’s often more useful to us than just knowing the predicted value. The predicted value \\(\\hat{Y_i}\\) is called a point and is useful. We would like to report the variability around this value.. Two type of intervals available: 1.) Confidence interval A confidence interval captures the uncertainty around the mean predicted values. 2.) Prediction interval The prediction interval predicts in what range a future individual observation will fall. A prediction interval will always be wider than a confidence interval for the same value. Confidence interval: \\(\\hat{y} = s_yt_{(\\alpha, df = n-2)}\\sqrt{\\frac{1}{n}+\\frac{(X^*-\\bar{X})^2}{(n-1)s^2_X}}\\) Here, \\(s_y= \\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}\\) \\(s_y\\) is called residual standard error in R regression output. Prediction intervals are similar in their calculation \\(\\hat{y} = s_yt_{(\\alpha, df = n-2)}\\sqrt{1+\\frac{1}{n}+\\frac{(X^*-\\bar{X})^2}{(n-1)s^2_X}}\\) Here, \\(s_y= \\sqrt{\\frac{\\sum(y_i-\\hat{y_i})^2}{n-2}}\\) The formula is very similar, except the variability is higher since there is an added 1 in the formula. 17.20 An R example - Confidence Intervals data(mtcars) mt &lt;- mtcars[order(mtcars$hp),] plot(mt$mpg ~ mt$hp) model. &lt;- lm(mt$mpg ~ mt$hp) new_hp &lt;- data.frame(hp = mt$hp) pred.vals &lt;- predict(model., newdata = new_hp, interval = &quot;confidence&quot;) lines(cbind(new_hp, pred.vals)[,c(1,2)]) lines(cbind(new_hp, pred.vals)[,c(1,3)], col = &quot;red&quot;) lines(cbind(new_hp, pred.vals)[,c(1,4)], col = &quot;red&quot;) 17.21 An R example - Prediction Intervals data(mtcars) mt &lt;- mtcars[order(mtcars$hp),] plot(mt$mpg ~ mt$hp) model. &lt;- lm(mt$mpg ~ mt$hp) new_hp &lt;- data.frame(hp = mt$hp) pred.vals &lt;- predict(model., newdata = new_hp, interval = &quot;prediction&quot;) lines(cbind(new_hp, pred.vals)[,c(1,2)]) lines(cbind(new_hp, pred.vals)[,c(1,3)], col = &quot;red&quot;) lines(cbind(new_hp, pred.vals)[,c(1,4)], col = &quot;red&quot;) "],["multiple-linear-regression.html", "18 Multiple Linear Regression 18.1 Multiple Predictors 18.2 Album Sales Model 18.3 Multiple Predictors Model 18.4 Regression “Plane” 18.5 Multiple Linear Regression 18.6 Model Interpretation 18.7 Test of Null Hypothesis 18.8 Significance Testing 18.9 Multicollinearity 18.10 Determination of Predictors to Include? 18.11 Model Design Considerations 1", " 18 Multiple Linear Regression Model is fit by minimizing the sum of squared differences between the line and the actual data points - method of least squares. We still use our base equation: \\(outcome=model+error\\) But this time the model it is a bit more complicated. When we add predictors, we add parameters (\\(\\beta\\) terms). So each predictor variable has its own coefficient (parameter) and the outcome is predicted from a combination of all variables multiplied by their respective coefficients (\\(\\beta\\) terms). 18.1 Multiple Predictors Y is the outcome variable, \\(\\beta_1\\) is the coefficient of X1,\\(\\beta_n\\) is the coefficient of \\(X_n\\). We seek to find combinations of \\(\\beta\\) values (parameters) that minimize the sum of squares error: \\(outcome_i=model_i+error_i\\) \\(error_i = outcome_i - model_i\\) As you know, we quantify the error the model fitting using \\(SS\\). \\(Y_i=(b_0 + b_1X_{1i}+b_2X_{2i}+\\ldots+b_nX_{ni})+\\varepsilon_i\\) \\(\\varepsilon_i=Y_i-(b_0 + b_1X_{1i}+b_2X_{2i}+\\ldots+b_nX_{ni})\\) 18.2 Album Sales Model From our previous example above of record sales data, we know that advertising accounts for 33% of the variation in album sales. Remember: \\(R^2=\\frac{SS_M}{SS_T}\\) \\(R^2=0.3313\\) Therefore a large proportion of variation remains unexplained. Lets bring a new predictor variable into the mix to see if we can increase the amount of residual variation described by the model: How many times the song was played on the radio during the week prior to its release. 18.3 Multiple Predictors Model We will incorporate a new “airplay” variable. So we have a model with three parameters and two slope coefficients. Because there are two predictors, so we can view the model in two dimensions: \\(E(Album \\:Sales_i)=\\beta_0+\\beta_1X_{advertising \\: budget_i}+\\beta_2X_{airplay_i}\\) 18.4 Regression “Plane” 18.5 Multiple Linear Regression Cities Y X1 X2 X3 X4 X5 X6 Pheonix 10 70.3 213 582 6.0 7.05 36 Little Rock 13 61.0 91 132 8.2 48.52 100 San Francisco 12 56.7 453 716 8.7 20.66 67 Denver 17 51.9 454 515 9.0 12.95 86 Hartford 56 49.1 412 158 9.0 43.37 127 Wilmington 36 54.0 80 80 9.0 40.25 114 Washington 29 57.3 434 757 9.3 38.89 111 Jacksonville 14 68.4 136 529 8.8 54.57 116 Miami 10 75.5 207 335 9.0 59.80 128 Atlanta 24 61.5 368 497 9.1 48.34 115 Chicago 110 50.6 3344 3369 10.4 34.44 122 Indianapolis 28 52.3 361 746 9.7 38.74 121 Des Moines 17 49.0 104 201 11.2 30.85 103 Wichita 8 56.6 125 277 12.7 30.58 82 Louisville 30 55.6 291 593 8.3 43.11 123 New Orleans 9 68.3 204 361 8.4 56.77 113 Baltimore 47 55.0 625 905 9.6 41.31 111 Detroit 35 49.9 1064 1513 10.1 30.96 129 Minneapolis- St. Paul 29 43.5 699 744 10.6 25.94 137 Kansas City 14 54.5 381 507 10.0 37.00 99 St. Louis 56 55.9 775 622 9.5 35.89 105 Omaha 14 51.5 181 347 10.9 30.18 98 Alburquerque 11 56.8 46 244 8.9 7.77 58 Albany 46 47.6 44 116 8.8 33.36 135 Buffalo 11 47.1 391 463 12.4 36.11 166 Cincinnati 23 54.0 462 453 7.1 39.04 132 Table of air pollution in 41 U.S. cities associatied with six environmental variables 18.6 Model Interpretation We regress SO2 content in the air on average temperature X1 and the number of manufacturing enterprises, X2 \\(\\hat{Y} = 77.231 + 1.0480X_1 + 0.02431X_2\\) A one unit change in \\(X_2\\) results in a 0.02431 increase in Y. A one unit change in \\(X_1\\) results in a 1.048 increase in Y. 18.7 Test of Null Hypothesis We can analyze the null hypothesis that all of the regression coefficients are equal to zero using an ANOVA analysis. This approach is analogous to that used in simple linear regression. 18.8 Significance Testing In general, a significant F value will be associated with the rejection of the null hypothesis for some regression coefficients. 18.9 Multicollinearity Sometimes it is possible to have a significant F without significant regression coefficients. Multicollinearity occurs when there is multicollinearity among variables. This happens when two variables are highly correlated. Result in untrustworthy coefficients - serves to increase the variance of the estimate of the \\(\\beta\\) values. Example: inclusion of one predictor results in R2 = 0.80. When you add a highly correlated variable, the variance it accounts for is already described by the first variable - it does not account for unique variance. So we only get a slight increase in our R value. We can deal with this by evaluating the variables prior to inclusion into the model. 18.10 Determination of Predictors to Include? One of the most widespread ways is to use “stepwise” methods, you specify a direction either forward or backward. Forward selection: Initial model with only the constant b0 is made Add single predictor that best predicts the outcome by selecting the one with the greatest correlation with the outcome If fit is improved, then the predictor is retained Repeat Backward selection: As above but remove coefficients one at a time. 18.11 Model Design Considerations 1 Predictor variables must be either continuous or categorical. Predictors should have non-zero variance. Predictors should not be highly correlated (avoid multicollinearity). It is desirable to employ a regression that is parsimonious - as few as necessary but as many as needed. "],["comparing-two-means-using-t-tests.html", "19 Comparing Two Means using t-tests 19.1 Small differences in mean samples values 19.2 Large differences in mean samples values 19.3 Example 19.4 We will evaluate these data using a linear model 19.5 Picture Group 19.6 Real Spider Group 19.7 Output from a Regression 19.8 The Independent t-test 19.9 Lets use the data from Zar as a worked example. 19.10 Determine test value 19.11 Determine critical value 19.12 When Assumptions are Broken", " 19 Comparing Two Means using t-tests A common situation is the comparison of the means of two populations. An example of a t-test is to evaluate the mean blood clotting times between populations that were given two drugs. \\(H_0: \\mu_B = \\mu_G\\), where \\(\\mu\\) is the mean blood clotting time (in minutes) If the samples came from two normally distributed populations and the variances of the populations are equal ,we can use a two sample t-test. These conform to the assumptions of parametric statistics. Two samples of data are collected and the sample means calculated. The calculated means might differ by either a little or a lot. 19.1 Small differences in mean samples values We expect their means to be equal or very close. In this case, it is possible (even likely) for the population means to differ by chance alone (due to sampling error). We wwould expect that large differences in the mean values would occur very infrequently. \\(H_0: \\mu_B = \\mu_G\\) Alternatively, we can pose this as the difference in the mean: \\(H_0: \\mu_B - \\mu_G = 0\\). We are interested in understanding the magnitude of difference between the sample means. We use the standard error as a gauge of the variability between sample means. 19.2 Large differences in mean samples values If the difference between the samples we have collected is large, then we can infer one of two things: 1.) There is no effect of the two blood clotting medicine and by chance we calculated two sample means that are atypical of the population from which they came. 2.) The two samples come from different populations (populations with different mean values). In this scenario, the difference between samples represents a genuine difference between the samples (and we falsify the null hypothesis). So, this is the null model we are testing; did the samples come from the same population or did the samples come from the different populations? If the observed difference between the sample means is large: The more confident we become that the second explanation is correct (i.e. that the null hypothesis should be rejected). 19.3 Example Is arachnophobia (fear of spiders) specific to real spiders or is a picture enough? 24 arachnophobic individuals Experimental Manipulation: n = 12 randomly chosen participants were exposed to a real spider. n = 12 randomly chosen participants were exposed to a picture of the same spider. We monitor the anxiety produced. 19.4 We will evaluate these data using a linear model Consider an experiment where ‘Groups’ were exposed to a “Picture of a Spider” and an “Actual Spider” The response variable is the level of Anxiety (A) \\(\\hat{A_i}=b_0+b_1G_i\\) The independent variable G has only two values “Group 1” and “Group 2” ie. The “real” and “picture” groups. ## Group Anxiety G ## 1 Picture 20.67124 0 ## 2 Picture 36.79956 0 ## 3 Picture 38.36280 0 ## 4 Picture 26.76544 0 ## 5 Picture 36.46704 0 ## 6 Picture 15.36294 0 ## 7 Picture 20.63903 0 ## 8 Picture 23.93972 0 ## 9 Picture 40.35177 0 ## 10 Picture 41.79260 0 ## 11 Picture 31.16043 0 ## 12 Picture 28.42174 0 ## 13 Real Spider 23.68506 1 ## 14 Real Spider 59.23398 1 ## 15 Real Spider 35.84786 1 ## 16 Real Spider 47.14871 1 ## 17 Real Spider 44.51467 1 ## 18 Real Spider 52.42887 1 ## 19 Real Spider 61.94535 1 ## 20 Real Spider 52.43209 1 ## 21 Real Spider 39.05075 1 ## 22 Real Spider 57.72527 1 ## 23 Real Spider 48.60876 1 ## 24 Real Spider 41.13388 1 19.5 Picture Group We can code the variable, G, as a ‘dummy’ variable. It will take on the values one or zero. We can assign one group to have a value of zero, and one group to have a value of one. In the case of the ‘picture’ group, we will assign a value of G to be zero. \\(G_{picture}=0\\) \\(\\hat{A_i}=b_0+b_1G_i\\) The expected anxiety of group “picture” is the mean of anxiety of group “picture”. \\(\\bar{A}_{Picture}=b_0+b_1G_{picture}\\) \\(\\bar{A}_{Picture}=b_0+b_1\\times0\\) \\(\\bar{A}_{Picture}=b_0\\) 19.6 Real Spider Group Again, we can code the variable, G, as a ‘dummy’ variable. It can take on the values one or zero. We have assigned a value of G to be zero for the “picture” group. So, for the “real” group, we will assign the dummy variable G to equal one. \\(G_{real}=1\\) \\(\\hat{A_{G=1}}=b_0+b_1G_i\\) \\(\\hat{A_i}=30.06+b_1\\times1\\) \\(\\hat{A_i}=30.06+b_1\\) \\(b_1 = \\hat{A_i}-30.06\\) \\(b_1\\) = Difference between means \\(b_1=\\bar{A}_{Real}-\\bar{A}_{Picture}\\) Our task is to understand if this \\(\\beta_1\\) value is significantly different from zero. 19.7 Output from a Regression Let’s see how this is analyzed in the linear regression approach using R: ## ## Call: ## lm(formula = df.$Anxiety ~ df.$Group) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.2945 -6.5733 0.6342 7.1292 14.9657 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 30.061 2.864 10.496 4.97e-10 *** ## df.$GroupReal Spider 16.918 4.050 4.177 0.000392 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 9.921 on 22 degrees of freedom ## Multiple R-squared: 0.4423, Adjusted R-squared: 0.4169 ## F-statistic: 17.45 on 1 and 22 DF, p-value: 0.0003915 ## Analysis of Variance Table ## ## Response: df.$Anxiety ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## df.$Group 1 1717.4 1717.40 17.448 0.0003915 *** ## Residuals 22 2165.5 98.43 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 19.8 The Independent t-test I am using the variable Y for the observations - this is a departure from Zar’s presentation but the mechanics are identical. \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{\\sqrt{\\frac{S^2_p}{n_1}+\\frac{S^2_p}{n_2}}}\\) The numerator is the difference between sample means. The denominator is the standard error of the difference between the sample means. This quantity is a measure of the variability of the data within the two samples. \\(S^2_p=\\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}\\) \\(S^2_p=\\frac{SS_1+SS_2}{v_1+v_2}\\) Here v1 and v2 are the degrees of freedom, v1 = n1 - 1 and v2 = n2 -1 The test value is compared to the critical value at a given \\(\\alpha\\) \\(t_{\\alpha,2,df}\\) Need to set \\(\\alpha\\) value. One or two-tailed test? v1 = n1 - 1 and v2 = n2 - 1 19.9 Lets use the data from Zar as a worked example. \\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1\\ne\\mu_2\\) \\(H_0: \\mu_1 - \\mu_2 = 0\\) \\(H_A: \\mu_1-\\mu_2\\ne0\\) Given drug B Given drug G 8.8 9.9 8.4 9.0 7.9 11.1 8.7 9.6 9.1 8.7 9.6 10.4 9.5 —— \\(n_1=6\\) \\(n_2=7\\) \\(\\nu_1=5\\) \\(\\nu_2=6\\) \\(\\bar{Y}_1=\\) 8.75 min \\(\\bar{Y}_2=\\) 9.74 min SS1 = 1.6950 min2  SS2 = 4.0171 min2 \\(S^2_p=\\frac{SS_1+SS_2}{v_1+v_2}=\\frac{1.6950+4.0171}{5+6}=\\frac{5.7121}{11}=0.5193 \\: \\mbox{min}^2\\) \\(s_{\\bar{Y_1}-\\bar{Y}_2}=\\sqrt{\\frac{S^2_p}{n_1}+\\frac{S^2_p}{n_2}}=\\sqrt{\\frac{0.5193}{6}+\\frac{0.5193}{7}}=\\sqrt{0.0866+0.0742}\\) \\(=\\sqrt{0.1608}=0.40\\mbox{min}\\) \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}\\) 19.10 Determine test value \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}=\\frac{8.75-9.74}{0.40}=\\frac{-0.99}{0.40}=-2.475\\) \\(|t|=|\\frac{\\bar{Y}_1-\\bar{Y}_2}{s_{\\bar{Y}_1-\\bar{Y}_2}}|=2.475\\) 19.11 Determine critical value \\(t_{0.05(2),v}=t_{0.05(2),11}=2.201\\) Therefore, reject H0 \\(P(|t|\\ge2.475)&lt;0.05\\) \\(\\: \\:\\) \\(P=0.031\\) 19.12 When Assumptions are Broken Non-parameteric t-test Mann-Whitney “U” Test +Do not require estimation of \\(\\mu\\) and \\(\\sigma\\). +No assumptions about distributions. Convert data to RANKS of data. Two sample rank test Rank from highest to lowest, the greatest value in either group is given a one, second given a two.. \\(U=n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\\) n1 and n2 are the number of observation sin samples 1 and 2. R1 is the sum of the ranks in sample 1 H0: Male and female students are the same height. HA: Male and female students are not the same height. \\(\\alpha=0.05\\) Height of males Height of females Ranks of male heights Ranks of female heights 193 cm 178 cm 1 6 188 173 2 8 185 168 3 10 183 165 4 11 180 163 5 12 175 7 170 9 — \\(n_1=7\\) \\(n_2=5\\) \\(R_1=31\\) \\(R_2=47\\) \\(U=n_1n_2+\\frac{n_1(n_1+1)}{2}-R_1\\) \\((7)(5)+\\frac{(7)(8)}{2}-31\\) \\(35+28-31\\) \\(32\\) \\(U^1=n_1n_2-U\\) \\((7)(5)-32\\) \\(U_{0.05(2),5,7}=30\\) Because 32&gt;30, H0 is rejected. Therefore, we conclude that height is different for male and female students. "],["analysis-of-variance-anova.html", "20 Analysis of Variance (ANOVA) 20.1 When and why to use ANOVA 20.2 What Does ANOVA Tell Us? 20.3 ANOVA as Regression 20.4 Placebo Group 20.5 Low Dose Group 20.6 High Dose Group 20.7 Output from Regression 20.8 Theory of ANOVA 20.9 ANOVA Worked Example 20.10 The Data 20.11 Step 1: Calculate SST 20.12 Degrees of Freedom 20.13 Model Sum of Squares (SSM) 20.14 Step 2: Calculate SSM 20.15 Model Degrees of Freedom 20.16 Residual Sum of Squares (SSR) 20.17 Step 3: Calculate SSR 20.18 Residual Degrees of Freedom 20.19 Double Check 20.20 Step 4: Calculate the Mean Squared Error 20.21 Step 5: Calculate the F-Ratio 20.22 Step 6: Construct a Summary Table 20.23 Multiple-Comparison Tests 20.24 Tukey Test", " 20 Analysis of Variance (ANOVA) In this module we will understand the basic principles of ANOVA. We will be analyzing one-way ANOVA, also called ‘one-factor’ ANOVA or ‘single factor Analysis of Variance’. We will be analyzing the impact to the mean value among k groups, where \\(k &gt;2\\). ANOVA is a parametric test. We will stick with Zar’s terminology (p 178, 4th edition). 20.1 When and why to use ANOVA When we want to compare means we can use a t-test but this test has limitations, we can only test the equality of two mean values. Often we would like to compare means from three or more groups. The ANOVA is an extension of regression (and hence is a general linear model). 20.1.1 Why Not Use Lots of t-tests? Inflates the Type I error rate \\(1 - (1- \\alpha)^n\\) 20.2 What Does ANOVA Tell Us? Null hypothesis: Like a t-test, one way ANOVA tests the null hypothesis that the means are the same among groups. ANOVA is an omnibus test: It test for an overall test for evaluating the equality of means among groups. It tells us that the group means are different. It doesn’t tell us which mean(s) differ. 20.3 ANOVA as Regression Let us assume that we test three different medicine levels and want to see if it impacts (results in contrast, i.e. significant differences) the illness level in pigs. We are going to test the effect of the three groups (k = 3), in this case medicine type. The three groups are ‘placebo’, ‘high’, and ‘low’. The analysis is termed a one-factor test or one-way ANOVA (only a single thing, the medicine, is different). Pigs are assigned, at random, to each of the three groups. These are the replicates of each group. \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) Here \\(high\\) and \\(low\\) are dummy variables, just like G in the t-test. Group Dummy variable 1 (High) Dummy variable 2 (Low) Placebo 0 0 Low dose medicine 0 1 High dose medicine 1 0 20.4 Placebo Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 0)+(b_2\\times 0)\\) \\(\\mbox{illness}_i=b_0\\) \\(\\bar{Y}_{placebo}=b_0\\) 20.5 Low Dose Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 1)+(b_2\\times 0)\\) \\(\\mbox{illness}_i=b_0+b_1\\) \\(\\bar{Y}_{low}=\\bar{Y}_{placebo}+b_1\\) \\(b_1=\\bar{Y}_{low}-\\bar{Y}_{placebo}\\) 20.6 High Dose Group \\(\\mbox{illness}_i=b_0+b_1\\mbox{low}_i+b_2\\mbox{high}_i+\\varepsilon_i\\) \\(\\mbox{illness}_i=b_0+(b_1\\times 0)+(b_2\\times 1)\\) \\(\\mbox{illness}_i=b_0+b_2\\) \\(\\bar{Y}_{high}= \\bar{Y}_{placebo}+b_2\\) \\(b_2=\\bar{Y}_{high}-\\bar{Y}_{placebo}\\) 20.7 Output from Regression Coefficients: Estimate Std. Error t value Pr(&gt; (intercept) 2.2000 0.6272 3.508 0.00432 ** dummy 1 2.8000 0.8869 3.157 0.00827 ** dummy 2 1.0000 0.8869 1.127 0.28158 Signif. Codes:  0 ‘’ 0.001 ’’ 0.01 ’’ 0.05’.’ 0.1 ’’ 1 residual standard error:  1.402 on 12 degrees of freedom. Multiple R-squared:  0.4604, Adjusted R-squared:  0.3704 F-statistic:  5.119 on 2 and 12 df, p-value:  0.02469 20.8 Theory of ANOVA We use SS components: First, we calculate how much variability there is between scores Total sum of squares (SST). We then calculate how much of this variability can be explained by the model we fit to the data How much variability is due to the experimental manipulation? This is evaluated using the model sum of squares (SSM) How much variability is due to individual differences in performance, residual sum of squares (SSR). We compare the amount of variability explained by the model (experiment), to the error in the model (individual differences) This ratio is called the F-ratio. If the model explains a lot more variability than it can’t explain, then the experimental manipulation has had a significant effect on the outcome. 20.9 ANOVA Worked Example Testing the effects of medicine on illness using three groups: Placebo (sugar pill) Low dose medicine High dose medicine The outcome/dependent variable (DV) was an objective measure of illness. 20.10 The Data Placebo Low Dose High Dose 3 5 7 2 2 4 1 4 5 1 2 3 4 3 6 \\(\\bar{Y}\\) 2.20 3.20 5.00 s 1.30 1.30 1.58 s2 1.70 1.70 2.50 Grand Mean = 3.467, Grand s = 1.767, Grand s2 = 3.124 20.11 Step 1: Calculate SST SST uses the differences between the observed data and the mean value of Y. Where the mean is the grand mean Placebo Low Dose High Dose 3 5 7 2 2 4 1 4 5 1 2 3 4 3 6 \\(\\bar{Y}\\) 2.20 3.20 5.00 s 1.30 1.30 1.58 s2 1.70 1.70 2.50 Grand Mean = 3.467, Grand s = 1.767, Grand s2 = 3.124 SST = sum((observed - Grand mean)2) \\(SS_T=\\sum(obs_i - \\bar{X}_{grand})\\) 20.12 Degrees of Freedom Degrees of freedom (df) are the number of values that are free to vary. In general, the df are one less than the number of values used to calculate the SS. dfTotal = N - 1 20.13 Model Sum of Squares (SSM) Difference between the model estimate and the mean (or “Grand Mean”) 20.14 Step 2: Calculate SSM \\(SS_M=\\sum n_i(\\bar{x}_i - \\bar{x}_{grand})^2\\) \\(SS_M=5(2.2-3.467)^2+5(3.2-3.467)^2+5(5.0-3.467)^2\\) \\(SS_M=5(-1.267)^2+5(-0.267)^2+5(1.533)^2\\) \\(SS_M=8.025+0.355+11.755\\) \\(SS_M=20.135\\) 20.15 Model Degrees of Freedom How many values did we use to calculate SSM? We used the 3 means. \\(df_M=k-1=3-1=2\\) 20.16 Residual Sum of Squares (SSR) 20.17 Step 3: Calculate SSR \\(SS_R=\\mbox{sum}([x_i-\\bar{x}_i]^2)\\) \\(SS_R=S^2_{group1}(n_1-1)+S^2_{group2}(n_2-1)+S^2_{group3}(n_3-1)\\) \\(SS_R=1.70(5-1)+1.70(5-1)+2.5(5-1)\\) \\(SS_R=(1.70\\times4)+(1.70\\times4)+(2.50\\times4)\\) \\(SS_R=6.8+6.8+10\\) \\(SS_R=23.60\\) 20.18 Residual Degrees of Freedom How many values did we use to calculate SSR? We used the 5 scores for each of the SS for each group. \\(df_R=df_{group1}+df_{group2}+df_{group3}\\) \\(df_R=(n_1-1)+(n_2-1)+(n_3-1)\\) \\(df_R=(5-1)+(5-1)+(5-1)\\) \\(df_R=12\\) 20.19 Double Check \\(SS_T=SS_R+SS_M\\) \\(43.74=20.14+23.60\\) \\(df_T=df_R+df_M\\) \\(14=2+12\\) 20.20 Step 4: Calculate the Mean Squared Error \\(MS_M=\\frac{SS_M}{df_M}=\\frac{20.135}{2}=10.067\\) \\(MS_R=\\frac{SS_R}{df_R}=\\frac{23.60}{12}=1.967\\) 20.21 Step 5: Calculate the F-Ratio \\(F=\\frac{MS_M}{MS_R}\\) \\(F=\\frac{MS_M}{MS_R}=\\frac{10.067}{1.967}=5.12\\) 20.22 Step 6: Construct a Summary Table Source SS df MS F Model 20.14 2 10.067 5.12* Residual 23.60 12 1.967 Total 43.74 14 20.23 Multiple-Comparison Tests The ANOVA that you examined is used to test the hypothesis that there is no difference in the sample means among k treatment levels However we cannot conclude, after doing the test, which of the mean values are different from one-another. 20.24 Tukey Test Tukey test - balanced, orthogonal designs Step 1: is to arrange and number all five sample means in order of increasing magnitude Calculate the pairwise difference in sample means. We use a t-test “analog” to calculate a q-statistic S2 is the error mean square by ANOVA computation n is the of data in each of groups B and A Remember this is a completely balanced design. \\(SE=\\sqrt{\\frac{s^2}{n}}\\) \\(q=\\frac{\\bar{Y}_B-\\bar{Y}_A}{SE}\\) Start with the largest mean, vs. the smallest mean. Then when the first largest mean has been compared with increasingly large second means, use the second largest mean. If the null hypothesis is accepted between two means then all other means within that range cannot be different. "],["two-way-independent-anova.html", "21 Two-Way Independent ANOVA 21.1 What is Two-Way Independent ANOVA? 21.2 Example of two-way ANOVA 21.3 Hypotheses Being Tested 21.4 Multiway Factorial ANOVA Hypotheses Being Tested 21.5 Hypotheses Being Tested 21.6 Benefit of Factorial Designs 21.7 A worked example of factorial design 21.8 Hypotheses Being Tested 21.9 Remeber the terminology 21.10 Step 1: Calculate SST 21.11 Step 2: Calculate SSM 21.12 Step 2a: Calculate SSA 21.13 Step 2b: Calculate SSB 21.14 Step 2c: Calculate SS(AxB) 21.15 Step 3: Calculate SSR 21.16 Degrees of Freedom and Mean Square Calculations 21.17 Interpreting Factorial ANOVA 21.18 Interpretation: Main Effect pH 21.19 Interpretation: Main Effect Sun 21.20 Interpretation: Interaction Effects 21.21 Factorial ANOVA as Regression 21.22 Interaction Effects 21.23 Post-hoc Tests 21.24 Non-Parametric Tests", " 21 Two-Way Independent ANOVA Like one-way ANOVA this is a comparison of means test. In this module we will examine multifactor ANOVA. Multi-factorial ANOVA as a linear model Hypotheses being tested Interaction effects Post-hoc tests Non-parametric versions 21.1 What is Two-Way Independent ANOVA? Two independent variables Two-way (n = 2) factors Simultaneous analysis of two factors and measurement of mean response. Terminology: One factor termed A and one factor termed B. a is the number of levels in A. b is the number of levels in B. 21.2 Example of two-way ANOVA Researchers have sought to examine the effects of various types of music on agitation levels in patients in early and middle stages of Alzheimer’s disease. Patients were selected based on their form of Alzheimer’s disease. Three forms of music were tested: easy listening, Mozart, and piano interludes. The response variable agitation level was scored. Here is the design: Group Piano Interlude Mozart Easy Listening Early Stage ALzheimer’s 21 9 29 24 12 26 22 10 30 18 5 24 20 9 26 Middle Stage Alzheimer’s 22 14 15 20 18 18 25 11 20 18 9 13 20 13 19 21.3 Hypotheses Being Tested Three factor ANOVA: HO: Mean agitation is the same for each music type. HO : Mean agitation is the same for each Alzheimer’s stage. HO : Mean agitation is the same for all levels of music, independent of Alzheimer stage. 21.4 Multiway Factorial ANOVA Hypotheses Being Tested Popcorn Oil Amt. Batch Yeild Plain Little Large 8.2 Gourmet Little Large 8.6 Plain Lots Large 10.4 Gourmet Lots Large 9.2 Plain Little Small 9.9 Gourmet Little Small 12.1 Plain Lots Small 10.6 Gourmet Lots Small 18.0 Plain Little Large 8.8 Gourmet Little Large 8.2 Plain Lots Large 8.8 Gourmet Lots Large 9.8 Plain Little Small 10.1 Gourmet Little Small 15.9 Plain Lots Small 7.4 Gourmet Lots Small 16.0 21.5 Hypotheses Being Tested Three factor ANOVA: HO: Yield is the same in all three Batch sizes HO : Yield is the same in all three Oil amounts HO : Yield is the same in all three Popcorn types HO : The mean yield is the same for all levels of batch, independent of oil amount (Batch X Oil) HO : The mean yield is the same for all levels of Oil amount, independent of popcorn type (Oil X Type) HO : The mean yield is the same for all levels of batch, independent of popcorn type (Batch X Type) HO : Differences in mean Yield among the batch, oil amount, and popcorn type are independent of the other factors + (Batch X Type X Oil) Df Sum Sq Mean Sq F value Pr(&gt;F) popcorn type 1 3.062 3.062 0.1731 0.6883 oil amount 1 0.062 0.062 0.0035 0.9541 batch size 1 52.562 52.562 2.9717 0.1230 popcorn type:oil amount 1 27.562 27.562 1.5583 0.2472 popcorn type:batch size 1 14.062 14.062 0.7951 0.3986 oil amount:batch size 1 0.063 0.063 0.0035 0.9541 popcorn type:oil amount:batch size 1 1.563 1.563 0.0883 0.7739 Residuals 8 141.500 17.687 21.6 Benefit of Factorial Designs 1.) The experimental design is an efficient design to test multiple hypotheses simultaneously. 2.) We can look at how variables interact, this process is called ‘interaction’. Show how the effects that one level of one group (or factor) might depend on the effects of level in another group. Are often more interesting than main effects. 21.7 A worked example of factorial design Testing the effects of the pH of soil and amount of sunlight exposure (full or partial) on mean plant height. We will test: 21.8 Hypotheses Being Tested HO: Mean plant height is the same in all sunlight treatments. HO : Mean plant height is the same in all pH treatments. HO : There is no significant interaction of ph and sunlight. 21.9 Remeber the terminology Factor A (pH), levels = 6, 7, 8 Factor B (Sun), levels = full, partial The response is the height of the plant at the end of the experiment. The experiment has two “levels” for the factor “B” (b = 2) and three “levels” for the factor “A” (a = 3). Thus, there are \\(ab = 3\\times2=6\\) unique combinations of sun and pH With each combination. We have r = 8 observations for each combination. r is called the number of replicates, in each cell. The total number of replicates (n) is: \\(N = abr = 3\\times2\\times8 = 48\\). The amounts \\(Y_{i,j,k}\\) is the amount of plant height for each replicate (k = 1 to 8) with sun i, (i = 1, 2) at pH j, (j = 1, 2, 3). Treatment and level-specific data for the plant height (cm): pH (Factor A, a levels) 6 6 7 7 8 8 Sun (Factor B, b levels) Full Partial Full Partial Full Partial 65 50 70 45 55 30 70 55 65 60 65 30 60 80 60 85 70 30 60 65 70 65 55 55 60 70 65 70 55 35 55 75 60 70 60 20 60 75 60 80 50 45 55 65 50 60 50 40 Total 485 535 500 535 460 285 Mean 60.625 66.875 62.50 66.875 57.50 35.625 Variance 24.55 106.70 42.86 156.70 50.00 117.41 21.10 Step 1: Calculate SST \\(Grand \\: Mean = \\bar{X} = 58.33\\) \\(SS_T=\\sum_{i=1}^{a}\\sum_{j=1}^{b}\\sum_{l=1}^{n}({X_{i,j,l} - \\bar{X}})^2\\) \\(SS_T=8966.66\\) 21.11 Step 2: Calculate SSM pH (Factor A, a levels) 6 6 7 7 8 8 Sun (Factor B, b levels) Full Partial Full Partial Full Partial Mean 60.625 66.875 62.50 66.875 57.50 35.625 Here we evaluate main effects and interaction effects. \\(SS_M=n\\sum_{i=1}^{a}\\sum_{j = 1}^{b}({\\bar{X}_{ij}}-\\bar{X})^2\\) \\(\\mbox{SS}_\\mbox{M}=8(60.625-58.33)^2+8(66.875-58.33)^2+8(62.5-58.33)^2+8(66.875-58.33)^2+8(57.5-58.33)^2+8(35.625-58.33)^2\\) \\(\\mbox{SS}_\\mbox{M}=8(2.295)^2+8(8.545)^2+8(4.17)^2+8(8.545)^2+8(-0.83)^2+8(-22.705)^2\\) \\(\\mbox{SS}_\\mbox{M}=42.1362+584.1362+139.1112+584.1362+5.5112+4124.1362\\) \\(\\mbox{SS}_\\mbox{M}=5479.17\\) 21.12 Step 2a: Calculate SSA Mean “Full Sun”: 60.21 cm SSA 65 70 55 70 65 65 60 60 70 60 70 55 60 65 55 55 60 60 60 60 50 55 50 50 Mean “Partial Sun”: 56.46 cm SSA 50 45 30 55 60 30 80 85 30 65 65 55 70 70 35 75 70 20 75 80 45 65 60 40 \\(SS_{A} = {rb}\\sum_{i=1}^a(\\overline{X}_{i} - \\overline{X})^2\\) \\(\\mbox{SS}_\\mbox{Sun}=24(60.21-58.33)^2+24(56.46-58.33)^2\\) \\(\\mbox{SS}_\\mbox{Sun}=24(1.88)^2+24(-1.87)^2\\) \\(\\mbox{SS}_\\mbox{Sun}=84.8256+83.9256\\) \\(\\mbox{SS}_\\mbox{Sun}=168.75\\) 21.13 Step 2b: Calculate SSB Factor B is pH. pH = 6 Mean \\(\\mbox{pH}_6\\) = 63.75 SSB 65 50 70 55 60 80 60 65 60 70 55 75 60 75 55 65 pH = 7 Mean \\(\\mbox{pH}_7\\) = 64.68 SSB 70 45 65 60 60 85 70 65 65 70 60 70 60 80 50 60 pH = 8 Mean \\(\\mbox{pH}_8\\) = 46.56 SSB 55 30 65 30 70 30 55 55 55 35 60 20 50 45 50 40 \\(SS_B = ra\\sum_{j=1}^b(\\overline{X}_{j} - \\overline{X})^2\\) \\(\\mbox{SS}_\\mbox{pH}=16(63.75-58.33)^2+16(64.68-58.33)^2+16(46.56-58.33)^2\\) \\(\\mbox{SS}_\\mbox{pH}=16(5.42)^2+16(6.3575)^2+16(-11.7675)^2\\) \\(\\mbox{SS}_\\mbox{pH}=470.0224+646.6849+2215.5849\\) \\(\\mbox{SS}_\\mbox{pH}=3332.292\\) 21.14 Step 2c: Calculate SS(AxB) \\(\\mbox{SS}_\\mbox{M}=\\mbox{SS}_{\\mbox{A}{\\times}\\mbox{B}} + \\mbox{SS}_\\mbox{A}+\\mbox{SS}_\\mbox{B}\\) \\(\\mbox{SS}_{\\mbox{A}{\\times}\\mbox{B}}=\\mbox{SS}_\\mbox{M}-\\mbox{SS}_\\mbox{A}-\\mbox{SS}_\\mbox{B}\\) \\(\\mbox{SS}_{\\mbox{Sun}{\\times}\\mbox{pH}}=\\mbox{SS}_\\mbox{M}-\\mbox{SS}_\\mbox{Sun}-\\mbox{SS}_\\mbox{pH}\\) \\(=5479.167-168.75-3332.292\\) \\(=1978.125\\) 21.15 Step 3: Calculate SSR The residual sum of squares is calculated in the same way as for one-way ANOVA . Represents individual differences in performance or the variance that can’t be explained by factors that were systematically manipulated. We saw in one-way ANOVA that the value is calculated by taking the squared error between each data point and its corresponding group mean. So, we use the individual variances of each group and multiply them by one less than the number of people within the group (n). We have the individual group variances: there were eight people in each group (therefore, n = 8). \\(\\mbox{SS}_\\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group \\: n}(n_n-1)\\) \\(\\mbox{SS}_\\mbox{R}=s^2_{group1}(n_1-1)+s^2_{group2}(n_2-1)+s^2_{group3}(n_3-1)+s^2_{group4}(n_4-1)+s^2_{group5}(n_5-1)+s^2_{group6}(n_6-1)\\) \\(\\mbox{SS}_\\mbox{R}=(24.55\\times7)+(106.7\\times7)+(42.86\\times7)+(156.7\\times7)+(50\\times7)+(117.41\\times7)\\) \\(\\mbox{SS}_\\mbox{R}=171.85+746.9+300+1096.9+350+821.87\\) \\(\\mbox{SS}_\\mbox{R}=3487.52\\) 21.16 Degrees of Freedom and Mean Square Calculations Source SS df MS Factor A \\(SS_A\\) a-1 \\(\\frac{SS_A}{a-1}\\) Factor B \\(SS_B\\) b-1 \\(\\frac{SS_B}{b-1}\\) Factor A x B \\(SS_{AB}\\) (a-1)(b-1) \\(\\frac{SS_{AB}}{(a-1)(b-1)}\\) Error SSE N-ab \\(\\frac{SSE}{N-ab}\\) Total SST N-1 21.17 Interpreting Factorial ANOVA Response Plant Growth Sum Sq Df F value Pr(&gt;F) Sunlight 169 1 2.0323 0.1614 pH 3332 2 20.0654 7.649e-07 Sunlight:pH 1978 2 11.9113 7.987e-05 residuals 3488 42 21.18 Interpretation: Main Effect pH There was a significant main effect of the amount of pH in the soil on the height of the plant, F(2, 42) = 20.07, p &lt; 0.001 21.19 Interpretation: Main Effect Sun There was a non-significant main effect of sunlight exposure on the plant height, F(1, 42) = 2.03, p = 0.161. 21.20 Interpretation: Interaction Effects There was a significant interaction between the amount of pH in the soil and the amount of sunlight exposure, on the height of the plant, F(2, 42) = 11.91, p &lt; .001. Non-parallel lines indicate such an interaction: For low pH full and partial sunlight, scores do not change much. At a high pH, partial sunlight scores plummet but full sunlight scores remain fairly high. So, the interaction is caused by a difference between sunlight exposure in the height of plants. 21.21 Factorial ANOVA as Regression pH 6 6 7 7 8 8 Sunlight Full Partial Full Partial Full Partial 65 50 70 45 55 30 70 55 65 60 65 30 60 80 60 85 70 30 60 65 70 65 55 55 60 70 65 70 55 35 55 75 60 70 60 20 60 75 60 80 50 45 55 65 50 60 50 40 Total 485 535 500 535 460 285 Mean 60.625 66.875 62.50 66.875 57.50 35.625 Variance 24.55 106.70 42.86 156.70 50.00 117.41 \\(outcome_i = (\\mbox{model})+\\mbox{error}_i\\) \\(plant height_i=(b_0+b_{1}\\mbox{Sunlight}_i+b_2\\mbox{pH}_i)+\\varepsilon_i\\) \\(plant height_i=(b_0+b_1A_i+b_2B_i+b_3AB_i)+\\varepsilon_i\\) \\(plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\\varepsilon_i\\) How do we code the interaction term? Multiply the variables A x B Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(plant height_i=(b_0+b_1Sunlight_i+b_2pH_i+b_3interaction_i)+\\varepsilon_i\\) \\(\\bar{Y}_{partial,6}=b_0+(b_1\\times0)+(b_2\\times0)+(b_3\\times0)\\) \\(b_0=\\bar{Y}_{partial,6}\\) \\(b_0=66.875\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{Full,6}=b_0+(b_1\\times1)+(b_2\\times0)+(b_3\\times0)\\) \\(\\bar{Y}_{Full,6}=b_0+b_1\\) \\(\\bar{Y}_{Full,6}=\\bar{Y}_{partial,6}+b_1\\) \\(b_1=\\bar{Y}_{Full,6}-\\bar{Y}_{partial,6}\\) \\(b_1=60.625-66.875\\) \\(b_1=-6.25\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{partial,4 \\: pH}=b_0+(b_1\\times0)+(b_2\\times1)+(b_3\\times0)\\) \\(\\bar{Y}_{partial,4 \\: pH}=b_0+b_2\\) \\(\\bar{Y}_{partial,4 \\: pH}=\\bar{Y}_{partial,6}+b_2\\) \\(b_2=\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6}\\) Sunlight pH Dummy (Sunlight) Dummy (pH) Interaction Mean Partial 6 0 0 0 66.875 Partial 8 0 1 0 35.625 Full 6 1 0 0 60.625 Full 8 1 1 1 57.500 \\(\\bar{Y}_{Full,4 \\: pH}=b_0+(b_1\\times1)+(b_2\\times1)+(b_3\\times1)\\) \\(\\bar{Y}_{Full,4 \\: pH}=b_0+b_1+b_2+b_3\\) \\(\\bar{Y}_{Full,4 \\: pH}=\\bar{Y}_{partial,6}+(\\bar{Y}_{Full,6}-\\bar{Y}_{partial,6})+(\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6})+b_3\\) \\(\\bar{Y}_{Full,4 \\: pH}=\\bar{Y}_{Full,6}+\\bar{Y}_{partial,4 \\: pH}-\\bar{Y}_{partial,6}+b_3\\) \\(b_3=\\bar{Y}_{partial,6}-\\bar{Y}_{Full,6}+\\bar{Y}_{Full,4 \\: pH}-\\bar{Y}_{partial,4 \\: pH}\\) \\(b_3=66.875-60.625+57.500-35.625\\) \\(b_3=28.125\\) 21.22 Interaction Effects Experiment: we are interested in oxygen consumption of two species of limpets in different concentration of seawater. Factor A is the species of limpet (levels, a) Factor B is the concentration of SW as a function of maximum salinity - 100, 75, and 50 % (levels, b) Completed anova Source of variation df SS MS Species 1 16.6380 16.638 ns salinities 2 10.3566 5.178 ns Sp X Sal 2 194.8907 97.445 ** Error 42 401.5213 9.560 Total 47 623.4066 When the two factors are identified as A and B, the interaction is identified as the A X B interaction. Variability not accounted for by A and B alone. Interaction: The effect of one factor in the presence of a particular level of another factor. There is an interaction between two factors if the effect of one factor depends on the levels of the second factor. Species Species Seawater Concentration A. scabra A. digitalis Mean 100% 10.56 7.43 9.00 75% 7.89 7.34 10.11 50% 12.17 12.33 9.76 Mean 10.21 9.03 9.62 The response to salinity differs between the two species At 75% salinity A. scabra consumes the least oxygen and A. digitalis consumes the most. Therefore a simple statement about the species response to salinity is not clear; all we can really say is: The pattern of response to changes in salinity differed in the two species. The difference among levels of one factor is not constant at all levels of the second factor “It is generally not useful to speak of an individual factor effect - even if its F is significant - if there is a significant interaction effect” - Zar 21.23 Post-hoc Tests Tukey test - balanced, orthogonal designs Step one: is to arrange and number all five sample means in order of increasing magnitude Calculate the pairwise difference in sample means. We use a t-test “analog” to calculate a q-statistic Scheffe’s test Examine multiple contrasts: ideas is to compare combinations of samples to each other instead of the comparison among individual k levels. Compare the mean outflow volume of four different rivers: 5 vs 1,2,3,4 \\(H_0:\\mu_2/3+\\mu_4/3+\\mu_3/3-\\mu_5=0\\) \\(H_0:(\\mu_2+\\mu_4+\\mu_3)/3=\\mu_5\\) \\(c_2=\\frac{1}{3}, \\: c_4=\\frac{1}{3}, \\: c_3=\\frac{1}{3}, \\: and \\: c_5=-1\\) Alternatives multiple contrasts: \\(H_0:(\\mu_1+\\mu_5)/2-(\\mu_2+\\mu_4+\\mu_3)/3=0\\) \\(H_0:\\mu_1-(\\mu_2+\\mu_4+\\mu_3)/3\\) Test Statistic: \\(s=\\frac{|\\sum c_i\\bar{Y}_i|}{SE}\\) Where \\(SE=\\sqrt{s^2(\\sum \\frac{c^2_i}{n_i})}\\) and the critical value of the test is \\(S_{\\alpha}=\\sqrt{(k-1)F_{\\alpha(1),k-1,N-k}}\\) 21.24 Non-Parametric Tests Violations of the assumptions We assume equality of variance - ANOVA is a robust test. Robust to unbalanced design. How to deal with outliers: use in analysis if they are valid data. Test of normality: Shapiro Wilks Test of equality of variance: Bartletts test. Nonparametric analysis of variance. If k &gt; 2 Kruskal-Wallis test - analysis of variance by rank Power increases with sample size. If k = 2 the Kruskal-Wallis is equivalent to the Mann-Whitney test. \\(H=\\frac{12}{N(N+1)}\\sum ^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\) If there are tied ranks H needs to be corrected using a correction factor C. \\(C=1-\\frac{\\sum t}{N^3-N}\\) \\(H_c=\\frac{H}{C}\\) \\(\\sum t=\\sum (t^3_i-t_i)\\) ti is the number of tied ranks. A limnologist obtained eight containers of water from each of four ponds. The pH of each water sample was measured. The data are arranged in ascending order within each pond. (One of the containers from pond 3 was lost, so n3 = 7, instead of 8; but the test procedure does not require equal numbers of data in each group.) The rank of each datum is shown parenthetically. H0: pH is the same in all four ponds. HA: pH is not the same in all four ponds. Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(H=\\frac{12}{N(N+1)}\\sum ^k_{i=1}\\frac{R^2_i}{n_i}-3(N+1)\\) \\(=\\frac{12}{32(32)}[\\frac{55^2}{8}+\\frac{132.5^2}{8}+\\frac{145^2}{7}+\\frac{163.5^2}{8}]-3(32)\\) \\(=11.876\\) Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(\\sum t=\\sum (t^3_i-t_i)\\) \\(\\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)\\) \\(\\sum t=168\\) Pond 1 Pond 2 Pond 3 Pond 4 7.68 (1) 7.71 (6*) 7.74 (13.5*) 7.71 (6*) 7.69 (2) 7.73 (10) 7.75 (16) 7.71 (6*) 7.70 (3.5*) 7.74 (13.5*) 7.77 (18) 7.74 (13.5) 7.70 (3.5*) 7.74 (13.5*) 7.78 (20*) 7.79 (22) 7.72 (8) 7.78 (20*) 7.80 (23.5*) 7.81 (26*) 7.73 (10*) 7.78 (20*) 7.81 (26*) 7.85 (29) 7.73 (10*) 7.80(23.5*) 7.84 (28) 7.87 (30) 7.76 (17) 7.81 (26*) 7.91 (31) n1 =8 n2 =8 n3 =7 n4 =8 R1 =55 R2 =132.5 R3 =145 R4 =163.5 *tied ranks \\(\\sum t=\\sum (t^3_i-t_i)\\) \\(\\sum t=(2^3-2)+(3^3-3)+(3^3-3)+(4^3-4)+(3^3-3)+(2^3-2)+(3^3-)\\) \\(\\sum t=168\\) \\(C=1-\\frac{\\sum t}{N^3-N}=1-\\frac{168}{31^3-31}=1-\\frac{168}{29760}=0.9944\\) \\(H_c=\\frac{H}{C}=\\frac{11.876}{0.9944}=11.943\\) \\(\\nu=k-1=3\\) \\(F=\\frac{(N-k)H_c}{(k-1)(N-1-H_c)}=\\frac{(31-4)(11.943)}{(4-1)(31-1-11.943)}=5.95\\) \\(F_{0.05(1),3,26}=2.98\\) Reject H0 "],["in-class-assignments.html", "22 In-Class Assignments 22.1 R Class 22.2 Sampling 22.3 Measures of dispersion and central tendency 22.4 Likelihood 22.5 Z-score 22.6 Normal Distribution 22.7 Probabilities and random variables: 22.8 Student’s \\(t\\) distribution 22.9 Correlation 22.10 Simple linear regression 22.11 Logistic Regression 22.12 Multiple Linear Regression 22.13 \\(t\\)-test 22.14 One-Way ANOVA 22.15 Factorial ANOVA 22.16 Chi square 22.17 Resampling Statistics 22.18 Permutation of the Slope 22.19 Multimodel Analysis and Selection 22.20 Multilevel Model", " 22 In-Class Assignments 22.1 R Class Here are some programming questions based on the material we examined in class: 22.1.1 Arithmetic Operators and Expressions Calculate the result of 5 + 5 / 2. Evaluate 3 * 2^2. Compute the value of (3*2)^2. Given z &lt;- 5 and w &lt;- c(3,7,9,2), what is the result of z + w[3]? 22.1.2 Working with Vectors Assign the variable x to be a vector containing the values 5, 5, 6, 2. Assign the variable y to be a vector containing the values 3, 3, 1, 7. Calculate the sum of vectors x and y. Subtract vector y from vector x. Assign vector d as the result of dividing vector y by vector x. 22.1.3 Exponentiation and Missing Values Calculate z^5 + 2. Compute (s / 3 + 33)^0.5. 22.1.4 Dealing with Missing Values Given dat.1 &lt;- c(-1,NA,1,1,-1), replace the missing values with zeros. Calculate the mean of dat.1 without removing missing values. Calculate the mean of dat.1 after removing missing values. 22.1.5 Data Classes and Basic Types Create a numeric variable num with the value 1.2. Convert 2.2 to an integer and assign it to the variable int. Create a character variable char with the value “datacamp”. Create a logical variable log_true with the value TRUE. 22.1.6 Factors and Lists Create a factor variable fac with the levels “good”, “bad”, “ugly”. Access the second level of the factor fac. Create a list combined_list containing vectors lis1, lis2, and lis3. Access the third element of the third vector in combined_list. 22.1.7 Matrix and DataFrame Create a 2x3 matrix M with values from 1 to 6 filled by rows. Create a 2x3 matrix M with values from 1 to 6 filled by columns. Create a data frame dataset with columns: Person, Age, Weight, Height, Salary. Calculate the number of rows in the data frame dataset. 22.1.8 Data Input and Output Read the CSV file named “co2.csv” located in the “./Data/” directory. Use the “read_xlsx” function to read the Excel sheet named “Sheet1” from the file “Codes.xlsx” in the “./Data/” directory. How many different ways can you create a vector labeled q containing two 3’s and four 5’s? Assign a vector of elements 3, 7, 9, and 2 to w. Assign the third element of w to s, where s is equal to 6. Calculate the length of a sequence that starts at 1.1, ends at 9.2, and has increments of 0.894. What is the third value of the sequence you created? Create a data frame site.name with values “Site.01” and “Site.02”. Create a data frame density with values 2.3, 2.3, 2.3, 2.3, 2.3, 2.3. Create a data frame abundance with values from 14.5 to 19.8 in equal intervals. Create a logical vector sampled. with values FALSE, TRUE, FALSE, FALSE, TRUE, FALSE. Create a data frame y.data.frame with columns site.name, density, abundance, and sampled. Calculate the number of rows in the data frame y.data.frame. Read the CSV file named “output.csv” from the “./Data/” directory. Convert the data in the Excel file “Codes.xlsx” (Sheet1) to a data frame. Download the CSV file from the given URL and save it as “output.csv” in the “./Data/” directory. Feel free to use these questions for practicing and testing your knowledge of R programming concepts. If you have any specific doubts or need further explanations, please let me know! 22.2 Sampling 22.2.1 Create a vector consisting of random draws from a normal distribution with (mean = 2, sd = 1) with at least 20 samples. Take 5 samples (without replacement) from this distribution and calculate all of the sample statistics you can think of. Now take an increasing large number of samples (without replacement), n = 8, 10,15, …20. For each iteration of random draws record the test statistics. Make a table and comment on how the CV changes as the number of sakples increases. 22.2.2 Your turn to perform a simulation In the function below I find that the taking more samples and that seems to give a more satisfactory result. This is an evaluation of how sample size influences summary statistics. In your own words, how does increasing sample size change our understanding of the estimate of \\(\\bar{X}\\) and \\(sd\\)? Add some code to calculate and plot the CV for each of the samples. What do you conclude from the simulation? Summary stats are descriptive statistics that describe the characteristics of distributions. # Here is another function to help us evaluate this: samp.eval.fun &lt;- function(samples. = seq(10,500, by = 2), mean.val = 2, sd.val = 1) { norm.dist &lt;- rnorm(n = 10000, mean = mean.val, sd = sd.val) sum.mat &lt;- matrix(NA, nrow = length(samples.), ncol = 4) for (j in 1:length(samples.)) { samp.vect &lt;- sample(x = norm.dist, size = samples.[j], replace = FALSE) sum.mat[j,1] &lt;- mean(samp.vect) sum.mat[j,2] &lt;- sd(samp.vect) sum.mat[j,3] &lt;- min(samp.vect) sum.mat[j,4] &lt;- max(samp.vect) } plot(samples., sum.mat[,1], xlab = &quot;Number of Samples&quot;, ylab = &quot;Mean of Samples&quot;, type = &quot;b&quot;) abline(h = mean.val, col = &quot;red&quot;, lwd = 2) plot(samples., sum.mat[,2], xlab = &quot;Number of Samples&quot;, ylab = &quot;SD of Samples&quot;, type = &quot;b&quot;) abline(h = sd.val, col = &quot;red&quot;, lwd = 2) } # Copy this code into your console. You will notice that in your # history window samp.eval.fun will show up as a function... # Now you can change the argument &quot;samples.&quot; samp.eval.fun() 22.3 Measures of dispersion and central tendency The mtcars dataset is part of the base R package datasets, the flights dataset is part of the nycflights13 package. You’ll need to install and load nycflights13 to access the flights dataset. 22.3.1 Measures of central tendency Mean What is the average mpg (miles per gallon) for cars in the mtcars dataset? Find the mean air_time of flights in the flights dataset. Why might the mean not always be a representative measure of central tendency? How might outliers affect our perception of the “center” of the data? Median Identify the median hp (horsepower) in the mtcars dataset. Calculate the median departure delay (dep_delay) for all flights in the flights dataset. In what scenarios would the median be a more appropriate measure of central tendency than the mean, especially in skewed distributions or when outliers are present? Mode Determine the mode of the gear column in the mtcars dataset. What is the most common departure hour (hour column) in the flights dataset? How might the mode, as the most frequent value, fail to capture the overall nature of a dataset? Why isn’t the mode as commonly used as the mean or median for numerical data? Putting it all together For the wt (weight) column in mtcars, find the mean, median, and mode. Calculate the mean, median, and mode for the distance column in the flights dataset. Using the information you obtained above, yhen might looking at all three measures (mean, median, mode) together provide a clearer picture of data distribution? How can discrepancies among these measures guide data interpretation? Making histograms or boxplot may help… Transformations and Central Tendency Calculate the mean mpg in the mtcars dataset before and after applying a logarithmic transformation. Find the median arrival delay (arr_delay) in the flights dataset for both raw and squared values. Make a histogram, which (transformed or untransformed) provides the best understanding of central tendency? Why might certain transformations be used, and how do they influence data interpretation? 22.3.2 Measures of dispersion Range What is the range of mpg (miles per gallon) in the mtcars dataset? Find the range of flight durations (arr_time - dep_time) in the flights dataset. In what situations might the range be a misleading measure of dispersion? Discuss the potential pitfalls of only using the range as a measure of data spread. Quantiles Calculate the interquartile range (IQR) for hp (horsepower) in the mtcars dataset. What are the 10th and 90th percentiles of the distance column in the flights dataset? How do quantiles provide a more nuanced understanding of data distribution compared to the range? In what situations would median (or other quantiles) be preferred over the mean, and why? Mean Deviation Compute the mean absolute deviation of wt (weight) in the mtcars dataset. What is the mean deviation of air_time in the flights dataset? Why isn’t the mean absolute deviation as commonly used in standard statistical methods as variance or standard deviation, even though it might be more intuitively understood? How might outliers impact the mean deviation compared to other measures of dispersion? Variance Find the variance of mpg in the mtcars dataset. Calculate the variance of arr_delay in the flights dataset. How do variance and standard deviation relate, and in which situations might one be preferred over the other? Why is variance, a squared measure, useful, even if it doesn’t have the same unit as the original data? Sample SS (Sum of Squares) Compute the sum of squares for the drat (rear axle ratio) column in the mtcars dataset. Find the sum of squares for dep_delay in the flights dataset. Philosophically, why do we square differences when calculating the sample sum of squares? What are the implications of this? How might the sum of squares be influenced by extreme values, and what does this tell us about the underlying data? Standard Deviation What is the standard deviation of qsec (1/4 mile time) in the mtcars dataset? Find the standard deviation of flight (flight number) in the flights dataset. Coefficient of Variation Calculate the coefficient of variation for cyl (number of cylinders) in the mtcars dataset. What is the coefficient of variation for carrier_delay in the flights dataset? What insights can the coefficient of variation provide that standard deviation cannot? Under what circumstances would it be more appropriate to use the coefficient of variation instead of the standard deviation to compare the spread of two datasets? General Which cars hav above-average hp in the mtcars dataset? For which month is the dep_delay variability the highest in the flights dataset? Which car model in the mtcars dataset has a wt closest to the median weight? Identify the airline (carrier) with the most consistent arr_delay times in the flights dataset. How does the variance of mpg for 4-cylinder cars compare to 6-cylinder cars in the mtcars dataset? Determine the day of the week in the flights dataset with the highest IQR for dep_delay. Is there a difference in the dispersion of air_time between flights destined for Los Angeles (LAX) vs. San Francisco (SFO)? Compute the range of hp for cars with an automatic transmission versus those with a manual transmission in the mtcars dataset. Find the month with the lowest mean absolute deviation of arr_delay in the flights dataset. Which origin airport in the flights dataset has the highest variability in the number of flights per day? Some questions for you to consider: How do measures of central tendency and dispersion together provide a more comprehensive understanding of a dataset? Discuss the implications of relying solely on measures of dispersion without considering the context or underlying distribution of the data. How do the assumptions underlying each measure of dispersion influence its applicability and interpretation? In what situations might measures of dispersion fail to capture the true complexity or nature of data distribution? How do transformations of data (e.g., logarithmic or square root transformations) influence measures of dispersion, and what are the philosophical considerations when choosing such transformations? 22.4 Likelihood 22.4.1 Why do we use likelihood? We know well that there is a analytic solution to this problem, the problem being parameter estimation - think of how to determine the mean and sd of a vector of numbers… From Gribble: “The point of MLE is to show that as long as you have a model of the probability of the data given a guess at the parameter(s), you can use an optimizer to find the parameter value(s) that maximize the likelihood (minimize the negative log-likelihood) of the data given the parameter(s)” 1a. What does this mean and what is the relevance for parameter estimation? 1b. Also, this may not be clear but the likelihood approach allows us to calculate confidence intervals around our parameters, how? 1c. Why is this a desirable feature? 22.4.2 We can compute the probability of the data x given parameters \\(\\mu\\) and \\(\\sigma^2\\) using the equation for the normal distribution pdf. 2a. Translate into words both the left and right sides of Gribble equation 8. How do you convert the equation to values? 22.4.3 Lets start with MS Excel to investigate this issue of parameter estimation, where you will find parameters of a model that provide the best fit to the data. For any probability distribution, the probability of observing a data point \\(Y_i\\), given a particular parameter value,p, is: Pr(\\(Y_i\\)|p). In this case we will use the normal distribution use the file “Maximum Likelihood Example.xlsx” 3a. What are the values in column A? 3b. What are the values in column B? 3c. What are the values in column C? 3d. What are the values in column D? 3e. Change the values of the estimated \\(\\mu\\) and \\(\\sigma\\)… what happens to the resulting sum logL?** 22.4.4 Lets look at some examples in R…Basically lets code the xlsx file: # We can make an R analog of the Maximum Likelihood Example.xlsx file: # Our approach is to make some data with underlying properties we understand. The data come from a normal distribution... We will change the value of our candidate values and determine how the negative log likelihood changes. # Here is some &quot;pseudo code&quot; # 1. Write code to make a normal distribution with mean.val and sd.val objects # rand.vect &lt;- ??? # 2. Make a vector that has the density values, assuming that the distribution follows: # mean.val.cand and sd.val.cand. # prob.vect &lt;- the density of the observed data at the given point? # How is this calculated in theory and in practice? # 3. Determine the likelihood - you need to determine the natural log of the elements in the vector and then...? # 4. What happens when you change the candidate values of the parameters? 22.4.5 Lets push our understanding # Get products of probabilities, find the MLE mu.val &lt;- 22 sig.val &lt;- 10 rand.draws &lt;- (rnorm(500,mean = mu.val, sd = sig.val)) hist(rand.draws) likhd &lt;- c() best.guess.mu &lt;- seq(15,25,by = 0.1) best.guess.sig &lt;- sig.val for (k in 1:length(best.guess.mu)) { prob &lt;- dnorm(rand.draws,best.guess.mu[k],best.guess.sig) likhd[k] &lt;- -(sum(log(prob))) } plot(best.guess.mu, likhd) abline(v = mu.val) 22.5 Z-score 22.5.1 Make a vector of 10 randomly drawn numbers from a normal distribution. Calculate the z-scores of each value Plot the z- scores and the vector of numbers # Here is another function to help us evaluate this: vals.zscores &lt;- function() { rnorm.vect &lt;- rnorm(10) plot(rnorm.vect, (rnorm.vect - mean(rnorm.vect))/sd(rnorm.vect), xlab = &quot;Original Vector&quot;, ylab = &quot;Z-score&quot;) abline(h = 0, col = &quot;red&quot;, lwd = 2) abline(v = mean(rnorm.vect), col = &quot;red&quot;, lwd = 2) } # Copy this code into your console. You will notice that in your # history window vals.zscores will show up as a function... vals.zscores() 22.6 Normal Distribution 22.6.1 The shape of the normal distribution What parameters control the shape of the normal distribution? Make some histograms of different normal distributions, in each, alter the parameter values in a systematic way to understand how these control the shape of the distribution. Interpret you results in words using the terms “precision” and “central tendency”. # Here is some code to help you. You will copy the code and paste it in - # I have written it as a function... mean.eval.fun &lt;- function(mean. = seq(1,5)) { n.val &lt;- 10000 sim.mat &lt;- matrix(NA, nrow = n.val, ncol = length(mean.)) par(mfrow = c(ceiling(length(mean.)/2),2)) for (j in 1:length(mean.)) { sim.mat[,j] &lt;- rnorm(n = n.val, mean = mean.[j], sd = 1) } for (j in 1:length(mean.)) { hist(sim.mat[,j], xlim = range(sim.mat), main = paste(&quot;Mean value of distribution = &quot;, mean.[j])) abline(v = mean.[j], col = &quot;red&quot;, lwd = 2) } } # Copy this code into your console. You will notice that in your # history window mean.eval.fun will show up as a function... # Now you can change the argument &quot;mean.&quot; mean.eval.fun(mean. = c(1,3,5)) mean.eval.fun(mean. = seq(10,14)) # I would recommend not plotting too many at one time... So, now we can see what happens to the distribution when we change the mean, the mean is the measure of the central tendency. Here is a function to evaluate how changing \\(s\\) impacts the distribution # Here is some code to help you. You will copy the code and paste it in - # I have written it as a function... sd.eval.fun &lt;- function(sd. = seq(1,5)) { n.val &lt;- 10000 sim.mat &lt;- matrix(NA, nrow = n.val, ncol = length(sd.)) par(mfrow = c(ceiling(length(sd.)/2),2)) for (j in 1:length(sd.)) { sim.mat[,j] &lt;- rnorm(n = n.val, mean = 0, sd = sd.[j]) } for (j in 1:length(sd.)) { hist(sim.mat[,j], xlim = range(sim.mat), main = paste(&quot;St. Dev. value of distribution = &quot;, sd.[j])) } } # Copy this code into your console. You will notice that in your # history window sd.eval.fun will show up as a function... # Now you can change the argument &quot;sd.&quot; sd.eval.fun(sd. = c(1,3,5)) sd.eval.fun(sd. = seq(10,14)) # I would recommend not plotting too many at one time... 22.6.2 Your turn Create a vector consisting of random draws from a normal distribution with (mean = 2, sd = 1) with at least 20 samples, using rnorm. Take 5 samples (without replacement) from this distribution and calculate some summary statistics. Now take an increasing large number of samples (without replacement), n = 8, 10,15, …20. For each iteration of random draws record the summary statistics. 22.6.3 Simulation In the function below I find that the taking more samples and that seems to give a more satisfactory result. Okay, so basically this is an evaluation of how sample size influences summary statistics… Summary stats are descriptive statistics that describe the characteristics of distributions. # Here is another function to help us evaluate this: samp.eval.fun &lt;- function(samples. = seq(10,500, by = 2), mean.val = 2, sd.val = 1) { norm.dist &lt;- rnorm(n = 10000, mean = mean.val, sd = sd.val) sum.mat &lt;- matrix(NA, nrow = length(samples.), ncol = 4) for (j in 1:length(samples.)) { samp.vect &lt;- sample(x = norm.dist, size = samples.[j], replace = FALSE) sum.mat[j,1] &lt;- mean(samp.vect) sum.mat[j,2] &lt;- sd(samp.vect) sum.mat[j,3] &lt;- min(samp.vect) sum.mat[j,4] &lt;- max(samp.vect) } plot(samples., sum.mat[,1], xlab = &quot;Number of Samples&quot;, ylab = &quot;Mean of Samples&quot;, type = &quot;b&quot;) abline(h = mean.val, col = &quot;red&quot;, lwd = 2) plot(samples., sum.mat[,2], xlab = &quot;Number of Samples&quot;, ylab = &quot;SD of Samples&quot;, type = &quot;b&quot;) abline(h = sd.val, col = &quot;red&quot;, lwd = 2) } # Copy this code into your console. You will notice that in your # history window samp.eval.fun will show up as a function... # Now you can change the argument &quot;samples.&quot; samp.eval.fun() 22.6.4 Calculate the summary statistics Calculate the z-scores of each value Plot the z- scores and the vector of numbers # Here is another function to help us evaluate this: vals.zscores &lt;- function() { rnorm.vect &lt;- rnorm(10) plot(rnorm.vect, (rnorm.vect - mean(rnorm.vect))/sd(rnorm.vect), xlab = &quot;Original Vector&quot;, ylab = &quot;Z-score&quot;) abline(h = 0, col = &quot;red&quot;, lwd = 2) abline(v = mean(rnorm.vect), col = &quot;red&quot;, lwd = 2) } # Copy this code into your console. You will notice that in your # history window vals.zscores will show up as a function... vals.zscores() 22.7 Probabilities and random variables: Here are some examples of useful functions to calculate the value of the cumulative distribution function at (or the probability to the left of) a given number. Given a number or a list it computes the probability that a normally distributed random number will be less than that number. This function is the “cumulative distribution function.” 22.7.1 Examine the properties of the Normal Distribution: pnorm(0) pnorm(1) pnorm(0,mean=2) pnorm(0,mean=2,sd=3) v &lt;- c(0,1,2) pnorm(v) x &lt;- seq(-20,20,by=.1) y &lt;- pnorm(x) plot(x,y) y &lt;- pnorm(x,mean=3,sd=4) plot(x,y) x &lt;- c(-2,-1,0,1,2) x pnorm(x) 22.7.2 Calculate the following probabilities: 1a. Probability that a normal random variable with mean 22 and variance 25 lies between 16.2 and 27.5 1b. is greater than 29 1c. is less than 17 1d. is less than 15 or greater than 25 22.7.3 qnorm The idea behind qnorm is that you give it a probability, and it returns the number whose cumulative distribution matches the probability. For example, if you have a normally distributed random variable with mean zero and standard deviation one, then if you give the function a probability it returns the associated Z-score - qnorm which is the inverse of pnorm. qnorm(0.5) qnorm(0.5,mean=1) qnorm(0.5,mean=1,sd=2) qnorm(0.5,mean=2,sd=2) qnorm(0.5,mean=2,sd=4) qnorm(0.25,mean=2,sd=2) qnorm(0.333) qnorm(0.333,sd=3) qnorm(0.75,mean=5,sd=2) v = c(0.1,0.3,0.75) qnorm(v) x &lt;- seq(0,1,by=.05) y &lt;- qnorm(x) plot(x,y) y &lt;- qnorm(x,mean=3,sd=2) plot(x,y) y &lt;- qnorm(x,mean=3,sd=0.1) plot(x,y) y &lt;- c(.01,.05,.1,.2,.5,.8,.95,.99) qnorm(y, mean=0, sd=1) 22.7.4 rnorm In rnorm the argument that you give it is the number of random numbers that you want, and it has optional arguments to specify the mean and standard deviation. The first sample is from \\(N(0,1)\\) distribution and the next one from \\(N(5,1)\\) distribution. z &lt;- rnorm(n = 10) z w &lt;- rnorm(1000,mean=5,sd=1) hist(w) rnorm(4) rnorm(4,mean=3) rnorm(4,mean=3,sd=3) rnorm(4,mean=3,sd=3) y &lt;- rnorm(200) hist(y) y &lt;- rnorm(200,mean=-2) hist(y) y &lt;- rnorm(200,mean=-2,sd=4) hist(y) 22.7.5 Plotting the probability density function (pdf) of a Normal distribution : x &lt;- seq(-4.5,4.5,.1) normdensity &lt;- dnorm(x,mean=0,sd=1) # plot(x,normdensity,type=&quot;l&quot;) 22.7.6 Some more simulation practice: Let’s a some simulation: the variable of interest is X with X ~ N(\\(\\mu\\) = 2, \\(\\sigma\\) = 4). We will investigate some properties of the sample mean and variance when taking a random sample of size n. In other words, we use R to generate n random draws from the distribution X. This happens in each simulation step. Perform 10 simulations and take a sample of size = 10 in each simulation step. Save the sample mean and variance in a vector. In that way, you are able to investigate the results of all simulation steps. mu &lt;-2 sigma &lt;-2 n &lt;-10 asim &lt;-10 xbar &lt;-c() xvar &lt;-c() for(i in 1:asim) { x&lt;-rnorm(n,mu,sigma) # x contains a random sample of size n of the variable X xbar[i] &lt;- mean(x) xvar[i] &lt;- var(x) } 22.7.7 Is the sample mean a good estimator for the population mean? How can you check this property using the above simulation? Increase the number of simulations to 100 and then finally to 1000. What do you notice? 22.8 Student’s \\(t\\) distribution Generate 500 samples from Student’s \\(t\\) distribution with 5 degrees of freedom and plot the historgam. Note: \\(t\\) distribution is going to be covered in some detail later, but here is some initial practice. There are four functions that can be used to generate the values associated with the tdistribution. You can get a full list of them and their options using the help command. These commands work just like the commands for the normal distribution. One difference is that the commands assume that the values are normalized to mean zero and standard deviation one, so you have to use a little algebra to use these functions in practice. The other difference is that you have to specify the number of degrees of freedom. The commands follow the same kind of naming convention, and the names of the commands are dt, pt, qt, and rt. A few examples are given below to show how to use the different commands. First we have the distribution function, dt. x &lt;- seq(-20,20,by=.5) y &lt;- dt(x,df=10) plot(x,y) y &lt;- dt(x,df=50) plot(x,y) Next we have the cumulative probability distribution function: pt(-3,df=10) pt(3,df=10) 1-pt(3,df=10) pt(3,df=20) x = c(-3,-4,-2,-1) pt((mean(x)-2)/sd(x),df=20) pt((mean(x)-2)/sd(x),df=40) Next we have the inverse cumulative probability distribution function: qt(0.05,df=10) qt(0.95,df=10) qt(0.05,df=20) qt(0.95,df=20) v &lt;- c(0.005,.025,.05) qt(v,df=253) qt(v,df=25) Finally random numbers can be generated according to the t distribution: rt(3,df=10) rt(3,df=20) hist(rt(30,df=20)) 22.9 Correlation 22.9.1 Aspects of correlation and learning objectives: Inspect data using R functionality, describe its structure, ranges, and relationships. Understand the mathematics of the calculations for variance, covariance, and correlation. Review the assumptions of parametric statistical tests Perform a statistical test. Look at the resulting power of tests. 22.9.2 We are interested in understanding the relationships of two variables that may, or may not, covary. Lets use some R functionality to examine the data. 22.9.3 For this lab we will use mtcars data set in the package ‘datasets’ version 3.5.0. Lets look at the data and use the function ‘summary’ What are the summary statistics of these variables? # Run this code in your R console require(datasets) library(help = &quot;datasets&quot;) # See all of the data sets available in R. summary(datasets::mtcars) Summary statistics describe the mean, range, variance, distribution, ect. of the univariate vectors. [, 1] mpg Miles/(US) gallon [, 2] cyl Number of cylinders [, 3] disp Displacement (cu.in.) [, 4] hp Gross horsepower [, 5] drat Rear axle ratio [, 6] wt Weight (1000 lbs) [, 7] qsec 1/4 mile time [, 8] vs Engine (0 = V-shaped, 1 = straight) [, 9] am Transmission (0 = automatic, 1 = manual) [,10] gear Number of forward gears [,11] carb Number of carburetors Lets look at the data in a figure # Run this code in your R console pairs(mtcars) pairs(mtcars[,c(1:5)]) Explore and describe the data - what variables are linearly correlated in a qualitative sense? We particularly want to focus on variables that are continuous. Why? We are going to perform parametric statistical analysis (calculation of the Pearson’s r and related statistical analysis). So, we will will need to fulfill the assumptions of parametric statistics. Data in each comparison group show a Normal distribution. The p value for parametric tests depends upon a normal sampling distribution. If the sample size is large enough and the actual sample data point value are approximately normally distributed, then the central limit theorem ensures a normally distributed sampling distribution. In regression analysis and in general linear models, it is the errors that need to be normally distributed. Data in each comparison group exhibit similar degrees of Homoscedasticity, or Homogeneity of Variance. This refers to the need for a similarity in the variance throughout the data. This means that the variable in the populations from which the samples were taken have a similar variance in these populations. In the case of regression, the variance of one variable should be the same as all the other variables. Each of these are testable hypotheses and we can set them up that way: 22.9.4 Test of normality Tests can be performed in two ways, a qualitative examination and a quantitative (statistical test). 22.9.5 Qualitative examination of normality hb &lt;- rnorm(100, mean = 15, sd = 3) hist(hb, prob = TRUE, main = &quot;Histogram of hemoglobin values&quot;, las = 1, xlab = &quot;Hemoglobin&quot;) lines(density(hb)) From the plot above it seems obvious that the data are normally distributed. hb &lt;- rnorm(100, mean = 15, sd = 3) qqnorm(hb, main = &quot;QQ plot of hemoglobin values&quot;) qqline(hb) The QQ plot below plots the sample quantile of each data point value against its theoretical quantile. A line is added for clarity. The closer the data point values follow the line, the more likely that our assumption has been met. 22.9.6 Use a statistical test to evaluate normality Here we will use the Shapiro-Wilks test The null hypothesis is that the observations are normally distributed. A p value of less than \\(\\alpha\\) indicates that the assumption for normality is NOT met. Below the hb variables is passed as argument to the shapiro.test() command, resulting in the same test statistic and p value as above. hb &lt;- rnorm(100, mean = 15, sd = 3) shapiro.test(hb) hb &lt;- runif(n = 100) shapiro.test(hb) 22.9.7 Test the homogeneity of variance The Levene test is used to test for homogeneity of variance. The null hypothesis states equality of variances. In order to conduct Levene’s test, the Companion to Applied Regression, car, package is required. The car::leveneTest() command requires the use of a data.frame object. So, both variables must be in the same dataframe. So we need to do some manipulation. test.data &lt;- rbind( data.frame(y = mtcars$mpg, group = 1), data.frame(y = mtcars$qsec, group = 2)) test.data$group &lt;- as.factor(test.data$group) car::leveneTest(test.data$y, test.data$group) 22.9.8 Lets perform the statistical investigation of correlation What is the hypothesis that we will test? What are the steps we need to do this? To determine if we will “accept” or “fail to reject” we will determine the p-value of the data given the null hypothesis. # Some code for you to explore ?cor cor(mtcars$mpg, mtcars$hp) ?cor.test cor.test(x = mtcars$mpg, y = mtcars$hp, method = &#39;pearson&#39;) cor.test(x = mtcars$mpg, y = mtcars$hp, method = &#39;spearman&#39;) 22.9.9 Correlation Simulation Function to explore the correlation structure and resulting Spearman and Pearson Correlation rejection rates for user specified number of data points and correlation values.Note bivariate correlation structure is determined using the normal distribiution. This function requires the “MASS” package for successful execution. require(mvtnorm) require(MASS) corr.power.simul &lt;- function(cor.val = 0.5, number.pts = 20) { cor.data.frame &lt;- matrix(NA, ncol = 5, nrow = 100) for (j in 1:100) { Sigma &lt;- matrix(1, nrow=2, ncol=2) Sigma[1,2] &lt;- Sigma[2,1] &lt;- cor.val rawvars &lt;- mvrnorm(n=number.pts, mu=c(0,0), Sigma=Sigma) cor.data.frame[j,1] &lt;- cor.val cor.data.frame[j,2] &lt;- cor(rawvars, method = &quot;pearson&quot;)[1,2] cor.data.frame[j,3] &lt;- cor.test(rawvars[,1], rawvars[,2], &quot;pearson&quot;, alternative = &quot;two.sided&quot;)$p.value cor.data.frame[j,4] &lt;- cor(rawvars, method = &quot;spearman&quot;)[1,2] cor.data.frame[j,5] &lt;- cor.test(rawvars[,1], rawvars[,2], &quot;spearman&quot;, alternative = &quot;two.sided&quot;)$p.value } cor.data.frame &lt;- round(cor.data.frame,4) cor.data.frame &lt;- as.data.frame(cor.data.frame) names(cor.data.frame) &lt;- c(&quot;Correlation&quot;, &quot;Pearson.Cor.&quot;,&quot;Pearson.prob.&quot;, &quot;Spearman.Cor.&quot;, &quot;Spearman.prob.&quot;) print(paste(&quot;Pearson test fails to reject (accepts) null hypothesis&quot;, length(which(cor.data.frame$Pearson.prob. &gt;= 0.05)), &quot;out of 100 trials&quot;)) print(paste(&quot;Spearman test fails to reject (accepts) null hypothesis&quot;, length(which(cor.data.frame$Spearman.prob. &gt;= 0.05)), &quot;out of 100 trials&quot;)) par(mfrow = c(3,1)) par(mar = c(5,5,3,3)) plot(rawvars[,1], rawvars[,2],xlab = &#39;x&#39;, ylab = &#39;y&#39;) plot(cor.data.frame[,2], cor.data.frame[,3], xlab = &quot;Pearson.Cor.&quot;, ylab = &quot;Pearson.prob.&quot;) abline(h = 0.05) legend(&#39;topright&#39;, legend = rbind(paste(&quot;Pearson test reject null hypothesis&quot;, 100 - length(which(cor.data.frame$Pearson.prob. &gt;= 0.05)), &quot;out of 100 trials&quot;), paste(&#39;Simulated data using r =&#39;,cor.val,&quot;from&quot;,number.pts,&quot;points&quot;))) points(cor.data.frame[which(cor.data.frame[,3] &gt; 0.05),2], cor.data.frame[which(cor.data.frame[,3] &gt; 0.05),3], col = &#39;red&#39;) plot(cor.data.frame[,4], cor.data.frame[,5], xlab = &quot;Spearman.Cor.&quot;, ylab = &quot;Spearman.prob.&quot;) abline(h = 0.05) legend(&#39;topright&#39;, legend = rbind(paste(&quot;Spearman test reject null hypothesis&quot;, 100 - length(which(cor.data.frame$Spearman.prob. &gt;= 0.05)), &quot;out of 100 trials&quot;), paste(&#39;Simulated data using r =&#39;,cor.val,&quot;from&quot;,number.pts,&quot;points&quot;))) points(cor.data.frame[which(cor.data.frame[,5] &gt; 0.05),4], cor.data.frame[which(cor.data.frame[,5] &gt; 0.05),5], col = &#39;red&#39;) # return(cor.data.frame) } corr.power.simul(cor.val = 0.75, number.pts = 10) corr.power.simul(cor.val = 0.55, number.pts = 10) 22.10 Simple linear regression In this in class assignment we will be focusing on using an analysis appropriate for a quantitative outcome and a single quantitative explanatory variable. outcome = model + error We postulate a linear relationship between the population mean of the outcome and the value of the explanatory variable. If we let Y be some outcome, and X be some explanatory variable, then we can express the structural model using a linear model. 22.10.1 Assumptions Assumptions of simple linear regression: 1a. Linear relationship of y and x 1b. It is OK to transform x or Y, and that allows many non-linear relationships to be represented on a new scale that makes the relationship linear. Generally, it is not recommended to perform extrapolation to make predictions outside of the range of x values All assumptions consistent with parametric statistics 22.10.2 Computation Example Consider a survey of daily air quality measurements in New York, May to September 1973. # Import the data require(datasets) # Lets look at the data &quot;airquality&quot; head(airquality) names(airquality) #We will reduce the size of the data airquality.2 &lt;- airquality[complete.cases(airquality),] # Lets construct a linear model. # The response variable in the regression is Ozone. What do you think is the best predictor. # Plot it: plot(airquality.2$Wind, airquality.2$Ozone) plot(airquality.2$Solar.R, airquality.2$Ozone) plot(airquality.2$Temp, airquality.2$Ozone) Choose a predictor variable that you think will best describe patterns of Ozone in NYC Use your knowledge of parametric statstics to ensure that the assumptions are met Now we can build the model: We will use the stereotyped format in R (by stereotyped I mean that ALL of the models you build in R follow this syntax). The function used for building linear models is lm(). The lm() function takes two main arguments: Formula Data. The data is typically a data.frame and the formula is a object of class formula. But the most common convention is to write out the formula directly in place of the argument as written below. linearMod &lt;- lm(Y ~ X, data=Your.Data) We will use the “lm” function to make the model and assign it to an object. Look at the anatomy of the function. lm(Ozone ~ Temp, data = airquality.2) Parameter estimates can be derived from least-squares analysis. We will be minimizing the sum of squared differences (the residuals) and we will let the computer do this for us. We will want to evaluate the patterns in the residuals (deviations). What does the runs of positive and negative residuals mean? # Run this code in your R console - we will investigate the model object lm.mod.obj &lt;- lm(Ozone ~ Temp, data = airquality.2) par(mfrow = c(3,1), oma = rep(1.5,4), mar = rep(2,4)) plot(airquality.2$Temp, airquality.2$Ozone) legend(&quot;topleft&quot;, &quot;Regression Model&quot;) abline(lm.mod.obj) summary(lm.mod.obj) # We see that B0 = -146.9955 and B1 = 2.4287 plot(airquality.2$Temp, lm.mod.obj$residuals, type = &#39;h&#39;, lwd = 2) legend(&quot;topleft&quot;, &quot;Residual deviations&quot;) abline(h = 0) hist(lm.mod.obj$residuals, main = &quot;&quot;) legend(&quot;topleft&quot;, &quot;Histogram of Residual deviations&quot;) Lets look at the ANOVA table # Run this code in your R console - we will investigate the model object anova(lm.mod.obj) coef(lm.mod.obj) 22.10.3 Failure to meet assumptions What if the relationship of y and x are not linear? Log-transforming Only the Predictor for SLR In this section, we learn how to build and use a simple linear regression model by transforming the values. This might be the first thing that you try if you find a non-linear trend in your data. Note, though, that it may be necessary to correct the non-linearity before you can assess the normality and equal variance assumptions. Also, while some assumptions may appear to hold prior to applying a transformation, they may no longer hold once a transformation is applied. In other words, using transformations is part of an iterative process where all the linear regression assumptions are re-checked after each iteration. require(&#39;openintro&#39;) hist(openintro::mammals$body_wt) hist(openintro::mammals$brain_wt) plot(openintro::mammals$body_wt, openintro::mammals$brain_wt) lm.model = lm(brain_wt ~ body_wt, data = openintro::mammals) anova(lm.model) ## Analysis of Variance Table ## ## Response: brain_wt ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## body_wt 1 46068314 46068314 411.19 &lt; 2.2e-16 *** ## Residuals 60 6722239 112037 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm.model) ## ## Call: ## lm(formula = brain_wt ~ body_wt, data = openintro::mammals) ## ## Residuals: ## Min 1Q Median 3Q Max ## -810.07 -88.52 -79.64 -13.02 2050.33 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 91.00440 43.55258 2.09 0.0409 * ## body_wt 0.96650 0.04766 20.28 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 334.7 on 60 degrees of freedom ## Multiple R-squared: 0.8727, Adjusted R-squared: 0.8705 ## F-statistic: 411.2 on 1 and 60 DF, p-value: &lt; 2.2e-16 Notice how the residual standard error is high: 334.7. Maybe a log-transformation in the values might help us to improve the model. For that, we will use the log function, which, by default, computes the natural logarithm of a given number or set of numbers. require(&#39;openintro&#39;) plot(log(openintro::mammals$body_wt), log(openintro::mammals$brain_wt)) lm.model = lm(log(brain_wt) ~ log(body_wt), data = openintro::mammals) anova(lm.model) ## Analysis of Variance Table ## ## Response: log(brain_wt) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(body_wt) 1 336.19 336.19 697.42 &lt; 2.2e-16 *** ## Residuals 60 28.92 0.48 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm.model) ## ## Call: ## lm(formula = log(brain_wt) ~ log(body_wt), data = openintro::mammals) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.71550 -0.49228 -0.06162 0.43597 1.94829 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.13479 0.09604 22.23 &lt;2e-16 *** ## log(body_wt) 0.75169 0.02846 26.41 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6943 on 60 degrees of freedom ## Multiple R-squared: 0.9208, Adjusted R-squared: 0.9195 ## F-statistic: 697.4 on 1 and 60 DF, p-value: &lt; 2.2e-16 22.11 Logistic Regression 1.) estimate a logistic regression 2.) interpret coefficients (as they affect odds and probability) 3.) understand how glm() handles categorical variables 4.) make predictions 22.11.1 We are interested in understanding the relationships of two variables Logistic regression is a method for fitting a regression curve, \\(y\\) = \\(f(x)\\), when \\(y\\) is a categorical variable. The typical use of this model is predicting \\(Y\\) given a set of predictors \\(X\\). The predictors can be continuous, categorical or a mix of both (in the case of including multiple \\(\\beta\\) values. Recall simple linear regression: \\(Y_i = \\beta_0 + \\beta_1X_i\\). We will now predict the probability that \\(y\\) will occur given the known values of \\(X\\). \\(P(Y_i) = \\frac{1}{1 + e^-(\\beta_0 + \\beta_1X_i)}\\) The categorical variable \\(y\\), in general, can assume different values. In the simplest case scenario the dependent variable \\(y\\) is binary meaning that it can assumes a value 1 or 0. 22.11.2 Logistic regression analysis Consider a study that looks at survival (of some organism) as a function of body size. In ploikiotherms we often think of survival increasing with body size, does that hold true? # First, we&#39;ll create a data set of 20 individuals of different body sizes: n &lt;- 20 bodysize &lt;- c(round(runif(n/2,0,10)), round(runif(n/2,10,30))) # sorts these values in ascending order. survive &lt;- c(round(runif(n/2,0,0.7)), round(runif(n/2,0.4,1.3))) # assign &#39;survival&#39; to these 20 individuals non-randomly dat &lt;- as.data.frame(cbind(bodysize,survive)) # saves dataframe with two columns: body size &amp; survival dat # Print the data to screen plot(x = dat$bodysize, y = dat$survive, xlab=&quot;Body size&quot;, ylab=&quot;Probability of survival&quot;) # plot with body size on x-axis and survival (0 or 1) on y-axis The basic regression analysis uses fairly simple formulas to get estimates of model parameters. These estimates can be derived from least-squares analysis. We will be minimizing the sum of squared differences (the residuals). # Now lets fit the model, run a logistic regression model (in this case, generalized linear model with logit link). see ?glm g &lt;- glm(survive~bodysize,family=binomial,dat) # draws a curve based on prediction from logistic regression model predict.y &lt;- g$fitted.values plot(x = dat$bodysize, y = dat$survive, xlab=&quot;Body size&quot;, ylab=&quot;Probability of survival&quot;) # plot with body size on x-axis and survival (0 or 1) on y-axis curve(predict(g,data.frame(bodysize=x),type=&quot;resp&quot;),add=TRUE) points(bodysize,fitted(g),pch=20) Examine the model, we are estimating the survival summary(g) Compare the above summary statistic to the summary statistic of the model below. The below model is fit to a different data set. What is your expectation? # Examine the deviance statistic # Now re-run the model with these data - examine the plot, what is the &quot;strength&quot; of the relationship body size and survival. n &lt;- 20 bodysize.2 &lt;- c(round(runif(n/2,0,22)), round(runif(n/2,10,30))) # sorts these values in ascending order. survive.2 &lt;- c(round(runif(n/2,0,0.7)), round(runif(n/2,0.3,1.3))) # assign &#39;survival&#39; to these 20 individuals non-randomly dat.2 &lt;- as.data.frame(cbind(bodysize.2, survive.2)) # saves dataframe with two columns: body size &amp; survival plot(x = dat.2$bodysize.2, y = dat.2$survive.2, xlab=&quot;Body size&quot;, ylab=&quot;Probability of survival&quot;) # plot with body size on x-axis and survival (0 or 1) on y-axis g.2 &lt;- glm(survive.2~bodysize.2,family=binomial,dat.2) predict.y.2 &lt;- g.2$fitted.values summary(g.2) Using Fields 8.3.2 make some statements about he deviance. Using Fields 8.3.6 make some statements about the odds ratio, derived from the slope parameter. 22.12 Multiple Linear Regression 22.12.1 In this in class assignment we will: 1.) Understand the assumptions of this parametric statistical approach 2.) Understand the structure and calculations of the multiple linear regression Note, you will need to install the package “plot3d” 22.12.2 The assumptions of the parameteric multiple linear regression 1.) The variables independent 2.) The variables are normally distributed We are interested in detecting whether there is a relationship of dependent variables to two or more independent variables Given a linear relationship of the dependent variable and independent variables: \\(y_i = \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + ...\\beta_nX_{ni}\\) we would like to test the null hypothesis that: \\(\\beta_1 = 0\\), \\(\\beta_2 = 0\\), and \\(\\beta_n = 0\\). Generally we are not concerned with testing the hypothesis that \\(\\beta_0 = 0\\). The reason is that we can scale the y values to \\(\\overline{y} = 0\\), this would ensure that \\(\\beta_0 = 0\\). Though not often done, this is why the statistical test of \\(\\beta_1 = 0\\) is trivial. 22.12.3 A test of the model and a test of the parameters In addition to testing the signicance of the parameters of the candidate linear model we will also want to test whether the model fit is a good one - does the model fit the data “well”. In our case, does the model fit the data better than a model that has an expectation of “no relationship” of the independent and dependent variables. 22.12.4 F-test, its not just for ANOVA To test model fit, we will examine how the value of the F statistic is calculated: # First, we&#39;ll create some data pig.dat &lt;- cbind(round(jitter(seq(20,50)),2), 3*round(jitter(seq(20,50)),2) - runif(31,-50,50)) pig.dat &lt;- as.data.frame(pig.dat) names(pig.dat) &lt;- c(&quot;diet&quot;,&quot;weight&quot;) pig.dat$weight &lt;- as.numeric(as.character(pig.dat$weight)) # Perform your analysis of the data here and convince yourself that it be analyzed # with parameteric statistics? Necessary for doing in your own work, # but we can examine the data quickly: plot(pig.dat$weight ~ pig.dat$diet, xlab = &quot;Concentration of Additive in Diet&quot;, ylab = &quot;Weight&quot;) # First, we&#39;ll look at the different components of the model plot(pig.dat$weight ~ pig.dat$diet, xlab = &quot; &quot;, ylab = &quot;Weight&quot;, pch = 20, col = &quot;orange&quot;, cex = 2) abline(h = mean(pig.dat$weight)) segments(x0 = pig.dat$diet, y0 = mean(pig.dat$weight), x1 = pig.dat$diet, y1 = pig.dat$weight, lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;SST&quot;, &quot;Diff. between observed data and mean of observed data&quot;), bty = &#39;n&#39;) plot(pig.dat$weight ~ pig.dat$diet, xlab = &quot; &quot;, ylab = &quot;Weight&quot;, pch = 20, col = &quot;orange&quot;, cex = 2) abline(lm(pig.dat$weight ~ pig.dat$diet)) segments(x0 = pig.dat$diet, y0 = pig.dat$weight, x1 = pig.dat$diet, y1 = predict(lm(pig.dat$weight ~ pig.dat$diet)), lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;SSR&quot;, &quot;Diff. between the observed data and the regression line&quot;), bty = &#39;n&#39;) plot(pig.dat$weight ~ pig.dat$diet, xlab = &quot; &quot;, ylab = &quot;Weight&quot;, pch = 20, col = &quot;orange&quot;, cex = 2) abline(lm(pig.dat$weight ~ pig.dat$diet)) abline(h = mean(pig.dat$weight)) segments(x0 = pig.dat$diet, y0 = mean(pig.dat$weight), x1 = pig.dat$diet, y1 = predict(lm(pig.dat$weight ~ pig.dat$diet)), lwd = 2) legend(&quot;topleft&quot;, legend = c(&quot;SSM&quot;, &quot;Diff. between the mean value of Y and the regression line&quot;), bty = &#39;n&#39;) We are using the relationship of the variance explained by each component to determine our model fit. \\(R^2 = \\frac{{SSM}}{SST}\\) to evaluate the quality of our regression - obviously this example is for simple linear regression but we can use the same principles as we move forward. We then used the derived properties \\(MS_M\\) and \\(MS_R\\) to determine the \\(F_{statistic}\\) So, what does it mean that the SSM &gt;&gt; SST? What does it mean that the SSM is small relative to the SST? # We can expand our data, lets make some data that is based on a feeding study # of Hogfish &quot;pigs&quot;, with two different concentrations of fatty acids, # called &quot;1&quot; and &quot;2&quot;. pig.dat &lt;- cbind(round(jitter(seq(20,50)),2), -0.3*round(jitter(seq(20,50)),2) - runif(31,-50,50)+100, 3*round(jitter(seq(20,50)),2) - runif(31,-50,50)) pig.dat &lt;- as.data.frame(pig.dat) names(pig.dat) &lt;- c(&quot;FFA.1&quot;, &quot;FFA.2&quot;,&quot;weight&quot;) pig.dat$weight &lt;- as.numeric(as.character(pig.dat$weight)) We would like to know if \\(\\beta_1\\) and \\(\\beta_2\\) are predictors of weight: \\(weight_i = \\beta_1{FFA.1_i} + \\beta_1{FFA.2_i}\\). # Lets do some preliminary examination of the data par(mfrow = c(2,1)) hist(pig.dat$FFA.1) hist(pig.dat$FFA.2) # We will want to evaluate the correlation of the independent variables... cor(pig.dat$FFA.1, pig.dat$FFA.2) # what can you conclude from this? # Lets perform the multiple linear regression fit &lt;- lm(weight ~ FFA.1 + FFA.2, data = pig.dat) fit # Other useful functions coefficients(fit) # model coefficients confint(fit, level=0.95) # CIs for model parameters # Using the analysis above - what can we say about the 95% CI and mean of the beta values? # What is being returned here? Can you write out the linear model (rounding is fine). # Lets look at the ANOVA table summary(reg.obj) # What is our interpretation of the modeled coefficients? 22.12.5 Stepwise Selection Approach Lets look at using stepwise regression on the mtcars data. We will predict mpg from the other variables. We don’t know what the best fit model is, so we can use “stepwise regression techniques” # Let&#39;s first look at the first six columns car.dat &lt;- mtcars[,1:6] head(car.dat) fit1 &lt;- step(lm(car.dat$mpg ~ car.dat$cyl + car.dat$disp + car.dat$hp + car.dat$drat + car.dat$wt)) # What is being returned? # Write and interpret the model, 7.8.3.2 # Compare the results of the nested model to that of the full model fit2 &lt;- lm(car.dat$mpg ~ car.dat$cyl + car.dat$disp + car.dat$hp + car.dat$drat + car.dat$wt) # compare models, what is going on here? Fields 7.4.8.2 anova(fit1, fit2) 22.12.6 Scaling your independent variables Gelman (a famous contemporary statistician) suggests scaling the independent variables when using linear models # Let&#39;s first look at the first six columns car.dat &lt;- mtcars[,1:6] car.dat.scale &lt;- as.data.frame(scale(car.dat)) names(car.dat.scale) &lt;- names(car.dat) par(mfrow = c(2,1)) hist(car.dat$disp) hist(car.dat.scale$disp) fit1 &lt;- lm(car.dat$mpg ~ car.dat$disp + car.dat$hp) coefficients(fit1) fit2 &lt;- lm(car.dat.scale$mpg ~ car.dat.scale$disp + car.dat.scale$hp) coefficients(fit2) # This allows coefficients on different scales to be evaluated - # they are centered and scales to zero. # So, now they are not in their original units but instead are converted to # standard deviation units. The beta coefficients can be evaluated in this way: coefficients(fit2)[2]/coefficients(fit2)[3] # A one unit change in displacement has a greater impact than a one unit change in hp? # how much different? 2.2 X... 22.13 \\(t\\)-test 1.) Understand the assumptions of the parameteric statistical approach called the \\(t\\)-test 2.) Examine a few different types of \\(t\\)-tests 3.) Evaluate data using \\(t\\)-test, GLM tools, and non-parametric approaches 5.) Understand the derivation of confidence intervals 22.13.1 The assumptions of the parameteric \\(t\\)-test approach 1.) The variables are continuous and independent. 2.) The variables are normally distributed. 3.) The variances in each group are homogeneous. 22.13.2 We are interested in understanding and describing parameters of two popuations. Fields and Zar both have comprehensive explanations of why and how we perform a t-test (Fields p. 373 to 375 is especially good). I will not review it here exhaustively, but will say that the goal of the mathematics of the calculation is to determine the amount of variance explained by the model. The proportion of variance explained by the model is the ratio of the difference in sample means and the estimate of the standard error of the difference in the sample means. This is the amount of structured error, relative to the amount of unstructured error. \\(t=\\frac{\\bar{Y}_1-\\bar{Y}_2}{\\sqrt{\\frac{S^2_p}{n_1}+\\frac{S^2_p}{n_2}}}\\) The numerator is the difference between sample means. The denominator is the standard error of the difference between the sample means. This quantity is a measure of the variability of the data within the two samples. \\(S^2_p=\\frac{(n_1-1)S^2_1+(n_2-1)S^2_2}{n_1+n_2-2}\\) \\(S^2_p=\\frac{SS_1+SS_2}{v_1+v_2}\\) Here v1 and v2 are the degrees of freedom, v1 = n1 - 1 and v2 = n2 -1 The test value is compared to the critical value at a given \\(\\alpha\\) \\(t_{\\alpha,2,df}\\) Need to set \\(\\alpha\\) value. One or two-tailed test? v1 = n1 - 1 and v2 = n2 - 1 This is a verbose way of describing the calculations, but the take home is that we would like to know if a “two mean model” where \\(\\mu_1 \\ne \\mu_2\\) is better at describing the characteristics of the sample data than is the null model, \\(\\mu_1 = \\mu_2\\). Again, remember, we are testing whether \\(\\mu_1 = \\mu_2\\) by evaluating whether the difference between them is zero: \\(\\mu_1 - \\mu_2 = 0\\). If the above is true, then a model with only a single mean (the null model) will fit the data better than a model with two means. Alternatively, if \\(\\mu_1 - \\mu_2 \\ne 0\\), then a model with two means will fit better than the null model. 22.13.3 There are a number of variants of the t-test. To compare two means we can use variants of the \\(t\\)-test. We are interested in understanding whether the population parameter \\(\\mu\\) is equal to a given value (one-sample \\(t\\)-test), equal between two populations (independent two-sample \\(t\\)-test), or equal between “repeated measures” of the same population (dependent two-sample \\(t\\)-test). All of these can be evaluated using one- and two-tailed null hypothesis testing. 22.13.4 Perform the t-test Prior to performing any of the statistical tests you will need to perform an exhaustive evaluation of the data - this is the first step in all analytical endeavors prior to performing statistical tests. I would recommend going beyond the “summary” function - which really is not that useful. Instead, you will want to plot these data using histograms, qq plots, and calculate standard deviation, mean, variance, ect. Perform statistical tests to evaluate whether parameteric statistics can be used to test the hypothesis \\(\\mu_1 = \\mu_2\\). To start, lets evaluate a the one-sample t-test. # First, we&#39;ll create some data n. &lt;- 20 set.seed(1) pop.1 &lt;- rbeta(n = n., shape1 = 2, shape2 = 3, ncp = 0) pop.2 &lt;- rbeta(n = n., shape1 = 3.5, shape2 = 3, ncp = 0) # Perform your analysis of the data here, can it be analyzed with parametric statistics? Question 1: What is the null hypothesis for the two-sample, two-tail test? # What alterations would you need to do so you can do a two sample test? # I have provided some code below. n1 &lt;- length(pop.1) # sample size of population yb1 &lt;- mean(pop.1) # mean of population s1 &lt;- var(pop.1) # variance of population n2 &lt;- length(pop.2) # sample size of population yb2 &lt;- mean(pop.2) # mean of population s2 &lt;- var(pop.2) # variance of population # t-statistic = (difference in mean)/(SE of difference) tstat.num &lt;- yb1 - yb2 # observed - expected pooled.v &lt;- ((n1-1)*s1 + (n2-1)*s2)/(n1 + n2 - 2) tstat.den &lt;- sqrt((pooled.v/n1) + (pooled.v/n2)) # SE of the difference of the mean values tstat &lt;- tstat.num/tstat.den # Determine the critical value of t for the observed df - can you figure out this code? df.val &lt;- n1 + n2 - 2 critical.t &lt;- qt(0.975, df.val) Question 2: What do we conclude from this test? Question 3: Explain how the numerator and denominator describe the value of the object tstat. Question 4: What is interesting, surprising, informative? Specifically, what is the relationship of altering the variance of samples, the differences in the means, and the alpha level? 22.13.5 Built-in t-test in R Use the functions that you wrote and compare the results to those of the built in t-test function in R Question 5: Walk through the output and describe it. # Fields page 380 has an excellent guide to the function arguments ?t.test t.test(x = pop.1, y = pop.2, var.equal = TRUE) # Student&#39;s t-test 22.13.6 Lets examine the t-test as a GLM - my favorite! # The function lm (which we know) takes as input, class data.frame pop.data &lt;- data.frame(observations = c(pop.1, pop.2), population = as.factor(c(rep(1,20),rep(2,20)))) names(pop.data) &lt;- c(&quot;observations&quot;,&quot;population&quot;) pop.data$population &lt;- as.factor(as.character(pop.data$population)) # Use the lm model mod.obj &lt;- lm(pop.data$observations ~ pop.data$population) summary(mod.obj) ## ## Call: ## lm(formula = pop.data$observations ~ pop.data$population) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.34130 -0.12144 -0.03708 0.16577 0.33966 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.47169 0.04086 11.543 5.47e-14 *** ## pop.data$population2 0.01128 0.05779 0.195 0.846 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1828 on 38 degrees of freedom ## Multiple R-squared: 0.001002, Adjusted R-squared: -0.02529 ## F-statistic: 0.03812 on 1 and 38 DF, p-value: 0.8462 Question 6: Please review and describe all aspects of the output. 22.13.7 Effect size Effect size is a statistical concept that measures the strength of the relationship between two variables on a numeric scale. For instance, if we have data on the height of men and women and we notice that, on average, men are taller than women, the difference between the height of men and the height of women is known as the effect size. The greater the effect size, the greater the height difference between men and women will be. Statistic effect size helps us in determining if the difference is real. In hypothesis testing, effect size, power, sample size, and significance level are related to each other. # The calculated t value can be translated into an index, r, that describes # the effect size. Simulate data with varying degrees of difference in the # mean, and evaluate the resulting effect size. # Use the lm model set.seed(1) t.dat = c() t.dat$p1 &lt;- rnorm(20, mean = 10, sd = 1) t.dat$p2 &lt;- rnorm(20, mean = 0, sd = 1) t.calc &lt;- t.test(t.dat$p1, t.dat$p2)[[1]] df.est &lt;- t.test(t.dat$p1, t.dat$p2)[[2]] r.val &lt;- sqrt(t.calc^2/(t.calc^2 + df.est)) Question 7: What is the relationship of the effect size estimate to \\(\\overline{X_1} - \\overline{X_2}\\) Question 8: What range of values can the effect size have? \\(r=\\sqrt{\\frac{t^2}{t^2+df}}\\) Question 9: Let’s look at the relationship of varying the population mean and effect size. Describe the pattern you see: # The calculated t value can be translated into an index, r, that describes # the effect size. Simulate data with varying degrees of difference in the # mean, and evaluate the resulting effect size. # Use the lm model r.val &lt;- t.calc &lt;- df.est &lt;- c() int.val &lt;- seq(0,10, by = 0.5) for (j in 1:length(int.val)) { set.seed(1) t.dat$p1 &lt;- rnorm(20, mean = int.val[j], sd = 1) t.dat$p2 &lt;- rnorm(20, mean = 0, sd = 1) t.calc[j] &lt;- t.test(t.dat$p1, t.dat$p2, var.equal = TRUE)[[1]] df.est[j] &lt;- t.test(t.dat$p1, t.dat$p2, var.equal = TRUE)[[2]] r.val[j] &lt;- sqrt(t.calc[j]^2/(t.calc[j]^2 + df.est[j])) } plot(t.calc, r.val, xlab = &quot;Calculated t value&quot;, ylab = &quot;r (the magnitude of the effect size)&quot;) lines(t.calc, r.val) 22.14 One-Way ANOVA 22.14.1 We will: 1.) Understand the assumptions of this parameteric statistical approach 2.) Understand the structure and calculations of the one-way ANOVA 22.14.2 The assumptions of the parameteric ANOVA approach 1.) The variables are continuous and independent 2.) The variables are normally distributed 3.) The variances in each group are equal 22.14.3 Intro: We are interested in detecting whether there is a difference in the means of two or more populations We are often interested in determining whether the means from more than two populations or groups are equal or not, and this is done using an F-test in ANOVA, the null hypothesis is: \\(\\mu_1 = \\mu_2= \\mu_3= \\mu_4...= \\mu_n\\). Examine how the value of the F statistic is calculated: What does it mean that the group MS &gt;&gt; error MS? What does it mean that the error MS is small relative to the group MS? We would like to know if a “multiple mean model” where (example, if \\(k\\) = 3, ie three groups), then one of these models: \\(\\mu_1 = \\mu_2 \\ne \\mu_3\\), \\(\\mu_1 \\ne \\mu_2= \\mu_3\\), \\(\\mu_3\\ne \\mu_1= \\mu_2\\), or \\(\\mu_1 \\ne \\mu_2\\ne \\mu_3\\) is better at describing the characteristics of the sample data than is the null model: \\(\\mu_1 = \\mu_2= \\mu_3\\). As Zar states “…ANOVA examines several sources of variation among all of the data in an experiment, by determining a sum of squares. So we need to evaluate multiple sources of error - these are reviewed in Zar and Fields. We will evaluate a design with (\\(k\\)) experimental groups. Each datum in the experiment \\(X_{ij}\\) can be denoted as the \\(j^{th}\\) observation from the experimental group \\(i\\). 22.14.4 Source of error #1 Within-group SS = \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}[(X_{ij} - \\overline{X}_i)^2]\\), where \\(\\overline{X}_i\\) is the mean of group \\(i\\) and \\(n_i\\) is the number of data points in the \\(i^{th}\\) group. The within group error is often referred to as the “error sum of squares”. Describe what we are calculating… 22.14.5 Source of error #2 Among-group SS = \\(\\sum_{i = 1}^{k}n_i(\\overline{X}_i - \\overline{X})^2\\) where \\(\\overline{X}\\) is the “grand mean”, the mean of all data. Describe what we are calculating… 22.14.6 Source of error #3 Total SS = \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}(X_{ij} - \\overline{X})^2\\). The quantity \\(\\overline{X}\\) is the “grand mean”, the mean of all data. This quantity is also called the “groups sum of squares”. It is the sum of the within-group SS and among-group SS. So, in order to do the ANOVA, the order of operations will be to: Compute the Within-group SS (\\(\\sigma_{within}^2\\)). Compute the among-group SS: (\\(\\sigma_{among}^2\\)) Produce F-statistic as the ratio of \\(F = \\frac{\\sigma_{among}^2}{\\sigma_{within}^2}\\). This is equivalent to \\(F = \\frac{{Group MS}}{Error MS}\\) 22.14.7 Alternative to the in-class assignment This in-class assignment will necessitate rudimentary R knowledege that can be obtained by completing the R tutorial. It is my hope that working through this lesson will help to reinforce the reading. R was developed to perform this type of statistical analysis. Some of you have expressed disinterest in using the R platform as a pedagogical tool. To be responsive to this you can use your class time on an alternative to the in-class assignment. I recommend working through Zar 5th edition problems by hand: Chapter 10; example 10.1, problems 10.1, 10.2, 10.6. If attempting these alternative problems are a better fit for your learning style and academic goals then I would encourage you to go this route. The most important thing is to think about what you are doing as you proceed in the in-class assignment or the alternative. 22.14.8 Perform the ANOVA Prior to performing any of the statistical tests you will need to perform an exhaustive evaluation of the data - this is the first step in all analytical endeavors prior to performing statistical tests. I would recommend going beyond the “summary” funciton - which really is not that useful. The functions “var”, “length”, “sum”, and “mean” will need to be used below. Perform statistical tests to evaluate whether parameteric statistics can be used to test the hypothesis \\(\\mu_1 = \\mu_2= \\mu_3\\). To start, lets write a script to calculate a the one-way ANOVA. # First, we&#39;ll create some data, these data are from Zar 10.1 pig.dat &lt;- rbind( cbind(rep(&quot;Feed.01&quot;, 5),c(60.8, 57, 65, 58.6, 61.7)), cbind(rep(&quot;Feed.02&quot;, 5),c(68.7,67.7,74,66.3,69.8)), cbind(rep(&quot;Feed.03&quot;, 4),c(102.6,102.1,100.2,96.5)), cbind(rep(&quot;Feed.04&quot;, 5),c(87.9,84.2,83.1,85.7,90.3))) pig.dat &lt;- as.data.frame(pig.dat) names(pig.dat) &lt;- c(&quot;diet&quot;,&quot;weight&quot;) pig.dat$weight &lt;- as.numeric(as.character(pig.dat$weight)) # Perform your analysis of the data here and convince yourself that it be analyzed # with parameteric statistics? Necessary for doing in your own work, # but we can examine the data quickly: boxplot(pig.dat$weight ~ pig.dat$diet, xlab = &quot;Diet&quot;, ylab = &quot;Weight&quot;) # Next, lets look to see how the ANOVA calculated. # This section will not make sense unless you have done the reading... # I used Zar (fifth edition), example 10.1. # Bartlett Test of Homogeneity of Variances bartlett.test(weight~diet, data=pig.dat) # We can visualize the Bartlett Test - you must install.package HH # hov is &quot;homogeneity of variances&quot; require(HH) hov(weight~diet, data=pig.dat) hovPlot(weight~diet, data=pig.dat) # This identifies the data in each of the three treatment levels... yA &lt;- pig.dat$weight[which(pig.dat$diet == &quot;Feed.01&quot;)] yB &lt;- pig.dat$weight[which(pig.dat$diet == &quot;Feed.02&quot;)] yC &lt;- pig.dat$weight[which(pig.dat$diet == &quot;Feed.03&quot;)] yD &lt;- pig.dat$weight[which(pig.dat$diet == &quot;Feed.04&quot;)] nA &lt;- length(yA) # sample size of group A meanA &lt;- mean(yA) # mean of group A shapiro.test(yA) hist(yA) qqnorm(yA) qqline(yA) nB &lt;- length(yB) # sample size of group B meanB &lt;- mean(yB) # mean of group B nC &lt;- length(yC) # sample size of sample C meanC &lt;- mean(yC) # mean of sample C nD &lt;- length(yD) # sample size of sample D meanD &lt;- mean(yD) # mean of sample D # Next determine the number of total elements &quot;N&quot; N. &lt;- sum(c(nA, nB, nC, nD)) grand.mean &lt;- mean(pig.dat$weight) So we have calculated \\(n_i\\), \\(\\overline{X}_i\\), \\(\\overline{X}\\), and \\(N\\). We can use the above quantities to calculate the different sources of error: Residual (Within-group) SS = \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}[(X_{ij} - \\overline{X}_i)^2]\\), Model (Among-group) SS = \\(\\sum_{i = 1}^{k}n_i(\\overline{X}_i - \\overline{X})^2\\), and Total SS = \\(\\sum_{i = 1}^{k}\\sum_{j = 1}^{n_i}(X_{ij} - \\overline{X})^2\\). # Residual SS (&quot;I use wg.SS to label the object &quot;within group&quot;) wg.SS &lt;- c((yA - meanA)^2, (yB - meanB)^2, (yC - meanC)^2, (yD - meanD)^2) wg.SS &lt;- sum(wg.SS) # Model SS (&quot;I use ag.SS to label the object &quot;among group&quot;) ag.SS &lt;- c(nA*(meanA - grand.mean)^2, nB*(meanB - grand.mean)^2, nC*(meanC - grand.mean)^2, nD*(meanD - grand.mean)^2) ag.SS &lt;- sum(ag.SS) # Total SS tot.SS &lt;- c(pig.dat$weight - grand.mean)^2 tot.SS &lt;- sum(tot.SS) # Check our arithmatic, does total SS equal the sum of it&#39;s components tot.SS == ag.SS + wg.SS I will let you finish the calculations: # Calculate the degrees of freedom - you will fill these in # Total df tot.df &lt;- # Residual df group.df &lt;- # Model df error.df &lt;- # Calculate the mean square model and residual - you need to fill the values in # among.group mean.sq.model &lt;- # within.group mean.sq.residual &lt;- # Calculate the F value # # Lets examine the F-distribution df1.v &lt;- 5 df2.v &lt;- 2 plot(seq(0.01,5,by = 0.01), df(seq(0.01,5,by = 0.01), df1=df1.v, df2=df2.v), ylab = &quot;f(x)&quot;, xlab = &quot;Quantile&quot;, cex = 0.2, pch = 20, main = paste(&quot;F distribution for numerator df = &quot;,df1.v, &quot; and denom. df = &quot;,df2.v, sep = &quot;&quot;)) lines(seq(0.01,5,by = 0.01), df(seq(0.01,5,by = 0.01), df1=5, df2=2)) # This distribution is striking - it is the first non-symmetric one we have seen... # Determine the critical value for the F distribution. # This should look very familiar... it is the quantile # against which we will compare our calculated F statistic. # Replace the &quot;XXX&quot; in teh equation below critical.F &lt;- qf(0.975, df.val, df.val.) What is interesting, suprising, informative (not rhetorical questions)? Understanding the components of the model is necessary for understanding the ANOVA. 22.14.9 Built-in ANOVA in R Use the functions that you wrote and compare the results to those of the built in t-test function in R lm.obj &lt;- lm(pig.dat$weight ~ pig.dat$diet) anova(lm.obj) 22.14.10 For you to do: For a data set that you create, show (using math) that MSE is equal to the pooled variance in the \\(t\\) test. This is done as easily in excel or a piece of paper as in R. 22.14.11 What are the tests we can use to determine if \\(\\mu_{k = 1} = \\mu_{k = 2}\\) from the above example? The Tukey Post-hoc test (from R-bloggers By Flavio Barros) First create some data chocolate &lt;- data.frame( Sabor = c(5, 7, 3, 4, 2, 6, 5, 3, 6, 5, 6, 0, 7, 4, 0, 7, 7, 0, 6, 6, 0, 4, 6, 1, 6, 4, 0, 7, 7, 0, 2, 4, 0, 5, 7, 4, 7, 5, 0, 4, 5, 0, 6, 6, 3 ), Tipo = factor(rep(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), 15)), Provador = factor(rep(1:15, rep(3, 15)))) Construct the linear model and examine output ajuste &lt;- lm(chocolate$Sabor ~ chocolate$Tipo) summary(ajuste) anova(ajuste) boxplot(chocolate$Sabor ~ chocolate$Tipo) The post-hoc test a1 &lt;- aov(chocolate$Sabor ~ chocolate$Tipo) posthoc &lt;- TukeyHSD(x=a1, &#39;chocolate$Tipo&#39;, conf.level=0.95) posthoc 22.15 Factorial ANOVA 22.15.1 To round out your ability to perform linear modeling in R, the following is and in class assignment to promote understanding of factorial ANOVA. 22.15.2 In this in class assignment we will: 1.) Understand the assumptions of this parameteric statistical approach. 2.) Understand the structure and calculations of the factorial ANOVA. 3.) Understand the causes and interpretation of interaction effects. 22.15.3 The assumptions of factorial ANOVA 1.) The variables independent 2.) The variables are normally distributed 3.) The variances in observations in each group are equal 22.15.4 We are interested in detecting whether there is a difference in the means of two or more populations We will extend our linear model to accomodate an increase in the number of independent factors that can be evaluated. So for example we have this experimental design, where \\(n\\) is the number of observations in each cell: In this design we have two factors (Feed and Life Stage [LS]) consisting of three feed treatments and two life stage treatments. The design is balanced - we have an equal number of represenative samples from each pairwise combination of observations. The 2-way ANOVA is probably the most popular layout in the design of experiments, both manipuative and mensurative: It utilizes every combination of factor levels hence it is called “factorial ANOVA”. The experiment has two “levels” for the factor “LS” (\\(a = 2\\)) and three “levels” for the factor “Feed” (\\(b = 3\\)). Thus, there are \\(ab\\) = 2 x 3 = 6 different combinations of Feed and LS With each combination you have \\(r = 4\\) observations. \\(r\\) is called the number of replicates, in each cell. The total number of replicates \\((N)\\) is: \\(N = abr = 24\\) The amounts \\(Y_{ijk}\\) is the amount of weight gain for each replicate \\((k = 1, 2, 3, 4)\\) with LS \\(i\\), \\((i = 1, 2)\\) at feed \\(j\\), \\((j = 1, 2, 3)\\). What do you think is the minimum number of replicates we need in each cell the table? Why? 22.15.5 What does this statistival test do? Using this design we can test three different hypotheses simultaneously: \\(\\mu_{Feed = 1} = \\mu_{Feed = 2}= \\mu_{Feed = 3}\\), \\(\\mu_{Juvenile} = \\mu_{Adult}\\), and, that there is no interaction between Feed and Life Stage treatments. Given the linear model: \\(y_i = \\beta_0 + \\beta_{Feed}X_{Feed_i} + \\beta_{Lifestage}X_{Lifestage_i} + \\beta_{Interaction}X_{Feed_i}X_{Lifestage_i}\\), we will test: \\(\\beta_{Feed} = 0\\), \\(\\beta_{Lifestage} = 0\\), and \\(\\beta_{Interaction} = 0\\). This is equivalent to testing the equality of means of the population, but now we test the whether the parameters of the linear model are equal to zero. So we are using the data to inform the model, deriving parameters, and making population-level inferences. 22.15.6 Sum of Squares Calculations To test model fit and determine linear model parameter values (and determine if they are signficantly different from zero), we will perform some calculations. Similiar to the one-way ANOVA, we will be partitioning the variance among groups: “Factor A”, “Factor B”, “interaction of Factor A and Factor B”, and “within subgroups”. These sources of variation sum to the “Total” variation. 22.15.7 ANOVA Table \\(\\underbrace{\\sum_{i=1}^a\\sum_{j=1}^b\\sum_{k=1}^r(X_{ijk} - \\overline{X})^2}_{SS_{Total}} = \\underbrace{{rb}\\sum_{i=1}^a(\\overline{X}_{i} - \\overline{X})^2}_{SS_{A}} + \\underbrace{ra\\sum_{j=1}^b(\\overline{X}_{j} - \\overline{X})^2}_{SS_{B}} + \\underbrace{\\sum_{i=1}^a\\sum_{j=1}^b(\\overline{X}_{ij} - \\overline{X}_{i}- \\overline{X}_{j} + \\overline{X})^2}_{SS_{A \\times B}} + \\underbrace{ \\sum_{i=1}^a\\sum_{j=1}^b\\sum_{k=1}^r(X_{ijk} - \\overline{X}_{ij})^2}_{SS_{within}}\\) We are using the relationship of the variance explained by each component to determine our model fit (Fields et al. 12.4). 22.15.8 If you prefer to work through the R code it is here # Lets make some observed data - each of these cells is the Feed &lt;- rep(c(rep(&quot;01&quot;,4), rep(&quot;02&quot;,4), rep(&quot;03&quot;,4)),2) Lifestage &lt;- c(rep(&quot;Juv.&quot;,12), rep(&quot;Adult&quot;,12)) weight.gain &lt;- c(4,5,6,5, 7,9,8,12, 10,12,11,9,6,6,4,4, 13,15,12,12,12,13,10,13) pig.dat &lt;- as.data.frame(cbind(Feed, Lifestage, weight.gain)) pig.dat$weight.gain &lt;- as.numeric(as.character(pig.dat$weight.gain)) # Lets do some preliminary examination of the data boxplot(pig.dat$weight.gain ~ pig.dat$Feed + pig.dat$Lifestage, xlab = &quot;Diet and Lifestage&quot;, ylab = &quot;Weight&quot;) # Or maybe you prefer this? boxplot(pig.dat$weight.gain ~ pig.dat$Lifestage + pig.dat$Feed, xlab = &quot;Diet and Lifestage&quot;, ylab = &quot;Weight&quot;) Which figure makes more sense to you, why (remember the objectives of the analysis)? 22.15.9 Construct the linear model (Fields 12.3.1) lm.obj &lt;- lm(weight.gain ~ Lifestage + Feed + Lifestage*Feed, data = pig.dat) lm.obj # Given your understanding of ANOVA as a linear model, how do you # interepret the coefficients? # Look at the last term of the linear model... how is this model different? lm.obj.main.effects.only &lt;- lm(weight.gain ~ Lifestage + Feed, data = pig.dat) lm.obj.main.effects.only 22.15.10 Evaluate the model summary(lm.obj) Make some statements about theh model fit to the data. 22.15.11 Construct the ANOVA Table anova(lm.obj) Given your understanding of partitioning the variance, how do you interpret ALL components of the “Sum Sq” and “Mean Sq”… this is the key to understanding the factorial ANOVA. See Fields 12.4, 12.9 22.15.12 Evaluate the ANOVA Table Calculate the critical F values and double check the resulting probabilities using R functionality (hint FDist {stats}). Make statements about the ability of the test to falsify each of the null hypothesis (n = 3). 22.15.13 Evaluate the interaction interaction.plot(pig.dat$Feed, pig.dat$Lifestage, pig.dat$weight.gain) Discuss the interaction term with your classmates, what do you think it tells you? See Fields Fig. 12.4, 12.8 and Fields section 12.4.2.3. 22.15.14 Perform a post hoc test TukeyHSD(aov(lm.obj), ordered = T, conf.level = 0.95) There is a lot of output here. What is being done, what is being reported? Understand this in detail by evaluating the output. Refer to Fields section 12.5.11 (Tukey HSD). 22.16 Chi square 22.16.1 In this in class assignment we will learn about using the chi square test Chi-Square test in R is a statistical method which used to determine if two categorical variables have a significant relationship between them. The two variables are selected from the same population. 22.16.2 We are interested in detecting whether there is a relationship between variables. The input data is in the form of a table that contains the count value of the variables in the observation. We use chisq.test function to perform the chi-square test of independence in the native stats package in R. For this test, the function requires the contingency table to be in the form of a matrix. Depending on the form of the data, to begin with, this can need an extra step, either combining vectors into a matrix or cross-tabulating the counts among factors in a data frame. We use read.table and as.matrix to read a table as a matrix. While using this, be careful of extra spaces at the end of lines. 22.16.3 Hypothetical Example: Effectiveness of a Drug Treatment To test the effectiveness of a drug for a certain medical condition, we will consider a hypothetical case. Suppose we have 105 patients under study and 50 of them were treated with the drug. The remaining 55 patients were kept under control samples. Thus, the health condition of all patients was checked after a week. With the following table, we can assess if their condition has improved or not. By observing this table, one can you tell if the drug had a positive effect on the patient? Here in this example, we can see that 35 out of the 50 patients showed improvement. Suppose if the drug had no effect, the 50 will split the same proportion of the patients who were not given the treatment. Here, in this case, improvement of the control case is high as about 70% of patients showed improvement, since both categorical variables which we have already defined must have only 2 levels. Also, it was sort of perceptive today that the drug treatment and health condition are dependent. 22.16.4 Worked Example Chi square test Particularly in this test, we have to check the p-values. Moreover, like all statistical tests, we assume this test as a null hypothesis and an alternate hypothesis. The main thing is, we reject the null hypothesis if the p-value that comes out in the result is less than a predetermined significance level, which is 0.05 usually, then we reject the null hypothesis. \\(H_0\\): The two variables are independent. In the case of a null hypothesis, a chi-square test is to test the two variables that are independent. data_frame &lt;- read.csv(&quot;https://goo.gl/j6lRXD&quot;) # Reading CSV table(data_frame$treatment, data_frame$improvement) chisq.test(data_frame$treatment, data_frame$improvement, correct=FALSE) Let’s look at the output: We have a chi-squared value of 5.5569. Since we get a p-Value less than the significance level of 0.05, we reject the null hypothesis and conclude that the two variables are in fact dependent. 22.16.5 Worked Example 2 Particularly for this challenge, first, find out if the ‘cyl’ and ‘carb’ variables are in ‘mtcars’ dataset and whether it is dependent or not. data(&quot;mtcars&quot;) table(mtcars$carb, mtcars$cyl) chisq.test(mtcars$carb, mtcars$cyl) 22.16.6 Use following dta to perform the chi-square test Note, you will need to instal the package “gplots” file_path &lt;- &quot;http://www.sthda.com/sthda/RDoc/data/housetasks.txt&quot; housetasks &lt;- read.delim(file_path, row.names = 1) head(housetasks) Visualization: library(&quot;gplots&quot;) # 1. convert the data as a table dt &lt;- as.table(as.matrix(housetasks)) # 2. Graph balloonplot(t(dt), main =&quot;housetasks&quot;, xlab =&quot;&quot;, ylab=&quot;&quot;, label = FALSE, show.margins = FALSE) More visualization: mosaicplot(dt, shade = TRUE, las=2, main = &quot;housetasks&quot;) chisq &lt;- chisq.test(housetasks) chisq$observed # Observed counts round(chisq$expected,2) # Expected counts round(chisq$residuals, 3) # How are these calculated and why do we want these values? # Contibution in percentage (%) contrib &lt;- 100*chisq$residuals^2/chisq$statistic round(contrib, 3) Using our analysis of residuals it can be seen that the most contributing cells to the Chi-square are Wife/Laundry (7.74%), Wife/Main_meal (4.98%), Husband/Repairs (21.9%), Jointly/Holidays (12.44%). These cells contribute about 47.06% to the total Chi-square score and thus account for most of the difference between expected and observed values. In this case, the contribution of one cell to the total Chi-square score becomes a useful way of establishing the nature of dependency. If we would like to report estimates and p-values for hypothesis texting # printing the p-value chisq$p.value # printing the mean chisq$estimate 22.16.7 Worked Example - goodness of fit testing Null hypothesis (\\(H_0\\)): There is no significant difference between the observed and the expected value. Answer to Q1: Are the colors equally common? tulip &lt;- c(81, 50, 27) res &lt;- chisq.test(tulip, p = c(1/3, 1/3, 1/3)) res # The function returns: the value of chi-square test statistic (&quot;X-squared&quot;) and a a p-value. # What do we conclude? tulip &lt;- c(81, 50, 27) res &lt;- chisq.test(tulip, p = c(1/2, 1/3, 1/6)) res # The function returns: the value of chi-square test statistic (&quot;X-squared&quot;) and a a p-value. # What do we conclude? Perform these tests by hand, can you see how the values are derived? 22.16.8 Chi-squared Distribution Here is a graph of the Chi-Squared distribution 7 degrees of freedom. plot(seq(0,30), dchisq(seq(0,30), df = 7), type = &#39;b&#39;, xlab = &quot;Probability&quot;, ylab = &quot;q&quot;) Problem: Find the 95th percentile of the Chi-Squared distribution with 7 degrees of freedom. Solution: We apply the quantile function qchisq of the Chi-Squared distribution against the decimal values 0.95. qchisq(0.95, df=7) 22.17 Resampling Statistics 22.17.1 Instructor’s note Many statistical methods, other than the classical ones presented by Zar and others that rely on the characteristics of the t-, Z-, F-, and normal distributions are not able to “handle” some statistical approaches including the non-parameteric ones presented below. Becasue R is a tool that not everyone is interested in using, and frankly I don’t know how to do these tests without some programming language, please use your class time to your benefit. If you are not interested in using R, then I suggest to 1.) Refine your project ideas and objectives, 2.) review the chapter on transformations in Zar and Fields et al., and 3.) Spend some time reviewing the statistical approaches that you would like to use in your own work. 22.17.2 Why are we interested in using resampling techniques 1.) Resampling provides clear advantages when assumptions of traditional parametric tests are not met. 2.) Resampling methods can be used for testing means, medians, ratios, or other parameters are the same, so we do not need new methods for these different applications. 3.) Resampling has advantages of conceptual simplicity (but a bit of learning curve for computation) 22.17.3 Parametric statistical approaches have some issues: 1.) Restrictive assumptions 2.) Tests can be difficult to interpret 3.) Recall that the \\(p\\)-value is defined as the probability of getting data as extreme as the observed data when the null hypothesis is true. 4.) If the data are shuffled many times in accordance with the null hypothesis being true, the number of cases with data as extreme as the observed data can be counted, and a \\(p\\)-value can be calculated. 22.17.4 Common resampling methods Three resampling methods are commonly used for different purposes: Permutation methods - use sampling without replacement to test hypotheses of “no effect”. Bootstrap methods - use sampling with replacement to establish confidence intervals. Monte Carlo methods - use repeated sampling from populations with known characteristics. 22.17.5 1. Permutation methods Approach to randomly redistribute the observations and calculate a statistic of interest. If we do this many times, say 1,000 times or 10,000 times, we generate a distribution of observed values for the statistic of interest under the null hypothesis of no difference between the two populations. We compare our observed statistic to the sampling distribution to determine how likely our observed statistic is. The benefit of this approach is that we can generate a sampling distribution for any parameter of interest and then determine the percentile of our observed parameter relative to those of the permutation samples. 22.17.6 2. Bootstrap methods Assuming we have some representative samples, we can sample, with replacement, to generate confidence intervals. We randomly sample with replacement from the observed scores to produce a new sample of the same size as our original sample. Now we can calculate the statistic of interest (e.g., median) from the new sample. With a large number of new samples, at least 10,000, we generate an empirical sampling distribution for the statistic of interest and we can determine upper and lower confidence limits for this statistic. 22.17.7 3. Monte Carlo methods With Monte Carlo techniques, we can specify several populations with known characteristics, and sample randomly from these distributions. With many replications, we generate sampling distributions for the statistics of interest. For example, we may be interested in the sensitivity of the t-test to violations of the assumption of equal variance or normality. We can generate populations that have specific characteristics, and then with multiple resampling we can generatesampling distributions for the statistics of interest. 22.17.8 Permutation methods in practice A permutation test builds - rather than assumes - sampling distribution (called the “permutation distribution”) by resampling the observed data. Specifically, we can “shuffle” or permute the observed data (e.g., by assigning different outcome values to each observation from among the set of actually observed outcomes). Permutation tests are particularly relevant in null hypothesis significance testing (NHST) approaches. In these situations, the permutation test represents our process of inference because our null hypothesis is that the two treatment groups do not impact the outcome. When we permute the outcome values we see all of the possible alternative treatment assignments we could have had. While a permutation test requires that we see all possible permutations of the data (which can become quite large), we can easily conduct “approximate permutation tests” by simply conducting a vary large number of resamples. 22.17.9 A simple permutation test: We have randomly sampled the data from a log-normal distribution with equal mean and variance. We set the seed to ensure the results are repeatable. So in this case, we are looking to find there is no significant difference between the two populations. diff.in.mean &lt;- 0.5 mean.pop.1 &lt;- 1 mean.pop.2 &lt;- abs(mean.pop.1 - diff.in.mean) set.seed(seed = 3) # Run the code below, do you see what &quot;set.seed&quot; does to the random number generator? pop.1 &lt;- rlnorm(n = 100, meanlog = mean.pop.1, sdlog = 2) pop.2 &lt;- rlnorm(n = 100, meanlog = mean.pop.2, sdlog = 2) pop.1[1] pop.2[1] The classic statistical approach here would be to use a t-test. Let’s instead apply our random permutation test. First, let’s compute the difference between the means of the two groups: obs.diff &lt;- mean(pop.1) - mean(pop.2) What we want to do is randomly shuffle our data between the two groups. If we were to sample (without replacement) once and compute the difference using the randomly shuffled version of groups, we would have the difference in population means, but only one iteration So, we will put this in a loop to get many values… perm.diff &lt;-c() for (j in 1:10000) { sample.ind &lt;- seq(1,length.out = length(c(pop.1, pop.2))) ind.1 &lt;- sample(x = seq(1,length.out = length(c(pop.1, pop.2))), size = length(pop.1), replace = F) ind.2 &lt;- which(!(sample.ind %in% ind.1)) perm.diff[j] &lt;- mean(c(pop.1,pop.2)[ind.1]) - mean(c(pop.1,pop.2)[ind.2]) } If we plot the examples, along with where our observed difference falls: par(mfrow = c(3,1)) hist(pop.1, 100, main = &quot;Histogram of population 1&quot;, xlim = c(0,100)) hist(pop.2, 100, main = &quot;Histogram of population 2&quot;, xlim = c(0,100)) hist(perm.diff,100, main = &quot;Histogram of perumuted differences in population means&quot;) abline(v = obs.diff, col = &quot;red&quot;, lwd = 2) perm.p &lt;- which(sort(perm.diff) &gt; obs.diff)[1]/10000 How does this result compare to using a parametric and alternative non-parametric test to test the equality of means of the two populations? Discuss and understand how the number of iterations you specify in the simulation impacts your assessment of the precision of your estimate. 22.17.10 A simple bootstrap example: Here, we will estimate the size of the confidence interval by applying the bootstrap and sampling many samples with replacement from the original sample, each the same size as the original sample, computing a point estimate for each, and finding the CI of this distribution of bootstrap statistics. To construct the confidence interval we need to find the point estimate (sample mean) from the original sample. mean.pop.1 &lt;- 1 set.seed(seed = 3) pop.1 &lt;- rlnorm(n = 100, meanlog = mean.pop.1, sdlog = 2) m.pop.1 &lt;- mean(pop.1) To find the standard error, we will create a large vector to store all of the samples. Note, here we are sampling with replacement. boot.samp &lt;- c() for (j in 1:1000) { boot.samp[j] &lt;- mean(sample(pop.1, size = length(pop.1), replace = T)) } Now we plot the distribution of sample means and determine the 95% CI. hist(boot.samp, 100) abline(v = sort(boot.samp)[c(25,975)], col = &quot;red&quot;, lwd = 2) abline(v = m.pop.1, col = &quot;red&quot;, lwd = 2) We are 95% confident that the mean of the population is in the interval from 6.4 to 12.2. How do these 95% CI estimates compare to those derived using the \\(t\\)-distribution? 22.17.11 Using the “boot” package to Bootstrap 95% CI for R-Squared require(boot) library(boot) # function to obtain R-Squared from the data rsq &lt;- function(formula, data, indices) { d &lt;- data[indices,] # allows boot to select sample fit &lt;- lm(formula, data=d) return(summary(fit)$r.square) } # bootstrapping with 1000 replications results &lt;- boot(data=mtcars, statistic=rsq, R=1000, formula=mpg~wt+disp) # view results results class(results) # get 95% confidence interval boot.ci(results, type=&quot;bca&quot;) Why do we choose type “bca”, how do these compare to the other varieties of CI? 22.17.12 Challenge: Write a script that will bootstrap the residuals values, such that given the linear model we can get the bootstrapped confidence intervals of the intercept and slope: \\(y_{ij} = \\alpha+\\beta\\times{x_i} + \\epsilon_{ij}\\). \\(\\alpha\\) and \\(\\beta\\) are the unknown intercept and slope and \\(\\epsilon_{ij}\\) are normally distributed errors. The residuals from the least squares fit are given by: \\(\\epsilon_{ij} =y_{ij} - \\alpha+\\beta\\times{x_i}\\). Steps: 1.) Bootstrap the residuals 2.) Produce a vector of \\(\\epsilon_{ij}^{*}\\) by taking random draws, with replacement. 3.) \\(y_{ij}^* = y_{ij} + \\epsilon_{ij}^{*}\\). 4.) Determine mean \\(\\alpha\\) and \\(\\beta\\) based on \\(y_{ij}^*\\) 5.) Repeat steps 1 to 4, \\(n\\) = 1,000 times. 22.17.13 Finally, lets take a look at doing a Monte Carlo simulation: The Monte Carlo method is a way to solve problems through simulation. In this case, we will used it to calculate the number \\(\\pi\\). You could approximate \\(\\pi\\) by simulation. The surface area of a quarter of a circle inscribe in [0,1] \\(\\times\\) [0,1] is \\(\\pi/4\\). plot(0, type = &#39;n&#39;, xlim = c(0,1), ylim = c(0,1), xlab = &quot;&quot;,ylab = &quot;&quot;) r &lt;- 1 x &lt;- seq(0,1,by = 0.01) y &lt;- sqrt(1 - x^2) lines(x,y) Let’s generate a great many points uniformly distributed in the square [0, 1] ? [0, 1]. Reminder, that the equation for a circle is \\(x^2 + y^2 = r^2\\) Compute the proportion of those falling inside the quarter circle, that should be approximately the proportion of areas of the quarter circle and the inscribing square. Clever right?! N &lt;- 1000 x &lt;- runif(N,0,1) y &lt;- runif(N,0,1) Inside &lt;- 0 for (i in 1:N) { if (x[i]^2 + y[i]^2 &lt; 1) { Inside &lt;- Inside + 1 points(x[i],y[i], pch = 20, col = &quot;green&quot;, bg = &quot;white&quot;) } if (x[i]^2 + y[i]^2 &gt; 1) { points(x[i],y[i], pch = 20, col = &quot;orange&quot;, bg = &quot;white&quot;) } } Now determine the proportion of points inside vs. outside of the circle… poportion.in &lt;- Inside/N 22.17.14 Challenge: Compute the probability under the \\(N(0,1)\\) distribution of a certain interval. So, we want to obtain the approximate integral of such function over the interval [0, 1] using simulation, so, instead of using the equation for the circle we will use: \\(f(x) = \\frac{1}{2\\pi}e^{\\frac{-x^2}{2}}\\). How does the equation above compare to what you know about the form of the normal distribution? How can we check our work? 22.18 Permutation of the Slope 22.18.1 We will examine the nonparametric approach to a test of the slope of a regression line. From 1969 - 1972, the United States Selective Service conducted draft lotteries to determine the order of call for induction into the armed forces. For this lottery, 366 blue plastic capsules containing birth days (January 1 - December 31) were placed in a large glass container. Capsules were then drawn by hand to assign order-of-call numbers to all eligible men as specified by Selective Service law. For this lab, you will be investigating whether the lotteries conducted in 1970 and 1971 were fair. The 1970 lottery applied to men born in 1951. The 1971 lottery applied to men born in 1952. The file draft70.csv contains sequential birthday number (1=Jan1, 2=Jan2, etc) and order-of-call number for each of the 366 possible birthdays selected in 1970. The file draft71.csv contains this same information for the birthdays selected in 1971. 22.18.2 Linear regression approach In order to determine whether these lotteries were fair, we are going to fit a regression line with the goal of predicting order-of-call number from sequential birthday number. That is, we are going to fit the model: \\(y = \\beta_0+\\beta_1x + \\epsilon\\), where \\(x\\) is the sequential birthday number and \\(y\\) is the order-of-call number for that birthday. We will begin by examining the data from the 1970 draft lottery, so load draft70.csv into R. draft.data &lt;- read.csv(file.choose()) Examine the data, what order-of-call number was assigned to your birthday? In a fair, random lottery, what should be the value of \\(\\beta_1\\) and \\(\\beta_0\\)? Use this answer to state appropriate null and alternative hypotheses in terms of ??1. To test the hypotheses you listed above, you will fit the regression model (using R or any program you choose). Make a plot of order-of-call number versus sequential birthday number. Does this plot appear to provide evidence against your null hypothesis? plot(callorder ~ birthday, data = draft.data, xlab = &quot;Day of Birth&quot;, ylab = &quot;Call Order&quot;) Fit the linear regression model for predicting order-of-call from sequential birthday number. Report the estimated slope of the regression line. Comment on how this compares to your null hypothesis. mod.obj &lt;- lm(callorder ~ birthday, data = draft.data) summary(mod.obj) Report your test statistic and p-value. State a conclusion in context based on this \\(p\\)-value. One of the assumptions necessary for this test statistic distribution is that the residuals must have a normal distribution. The residuals are the difference between the actual y-value and the predicted \\(y\\)-value. If this assumption is violated, the stated \\(t\\)-distribution is not valid. We can check this assumption by making a histogram of the residuals and checking whether they appear to be normally distributed. Make a histogram of the residuals. Do these residuals appear to be normally distributed? hist(residuals(mod.obj)) Is the assumption of normality violated? If so, your previously stated p-value is not correct. An alternative, we must approach this problem using nonparametric tools. We can still work with the slope of the regression line, but we will perform a permutation test to determine whether we have sufficient evidence that the slope if different from zero. 22.18.3 Permutation test of slope This permutation test will work as follows. We will estimate the slope from our observed data. Then we will shuffle the y-values (the order-of-call numbers) between the x-values (the birthday numbers) and re-estimate the slope. We will repeat this shuffling process a total of 1000 times to get an estimate of the permutation distribution for the slope. The idea is that if the slope truly is zero, then x has no effect on y, so each y could be paired with any of the x-values! The following code runs a permutation test on the slope of the regression line: x = draft.data$birthday; y = draft.data$callorder # estimate the slope for the observed data draft.lm = lm(y ~ x) teststat.obs = draft.lm$coeff[2] teststat = rep(NA, 1000) n &lt;- 5000 for(i in 1:n) { # randomly &quot;shuffle&quot; the y-values for different x-values ySHUFFLE = sample(y) # compute the F-statistic for the shuffled data SHUFFLE.lm = lm(ySHUFFLE ~ x) teststat[i] = SHUFFLE.lm$coeff[2] } # Create histogram hist(teststat,100, main = &quot;Permutation Distribution of Slopes&quot;) abline(v = teststat.obs, col = &quot;blue&quot;, lwd = 3) alpha.val &lt;- 0.05 CI &lt;- c(alpha.val/2, 1 - alpha.val/2) CI.ind &lt;- round(CI*n) CI.ind &lt;- sort(teststat)[CI.ind] abline(v = CI.ind, lwd = 2, col = &quot;red&quot;) State a conclusion (in context) about the result of this permutation test. Now you are going to repeat this process using the draft lottery data from 1971. Because the columns of data have the same names as the 1970 data set, be sure to clear your workspace. Use a permutation test to determine whether there is a relationship between order-of-call and birthday order in the 1971 draft. Define your parameter of interest, state appropriate hypotheses, and report your test statistic and p-value. Finally, state a conclusion (in context) about the result of your test. 22.19 Multimodel Analysis and Selection In this lab we will evaluate the role of AIC to help us understand how this index can assist in model selection and model averaging. We will use the mtcars data included in R base. Inspect the data using R functionality, describe its structure, ranges, relationships… We are interested in building linear models that have independent variables that can predict the mpg of a car. 2a. Using your knowledge and the data - what do you think is the “best” model? What makes it so - your answer should involve quantitative work and explananation of model output. 22.19.1 Preliminary Model Evalation lm.cand.01 &lt;- lm(mpg ~ cyl, data = mtcars) summary(lm.cand.01) lm.cand.02 &lt;- lm(mpg ~ cyl + drat, data = mtcars) summary(lm.cand.02) # Some tricks in lm lm(mpg ~ ., data = mtcars) lm(mpg ~ gear - 1, data = mtcars) Your answer to #2a may have (should have) involved evaluating some candidate models output and input. 2b. In the above box I give some more “lm” functionality using “.” and “-1”. What does this code do? Likely you will need to examine the model output in detail to make an informed answer. This question is for those interested in R programming only! Skip if that does not apply to you! How many possible models can you derive from these data to predict the mpg? The short answer is “many”. However, not all of these will be of interst. Derive some of these (i.e. create some linear models) and report the associated Pearson-product moment correlation coefficient) and plot the effect of adding predictors on the \\(R^2\\) value. r.sq.vect &lt;- c() num.pred.vect &lt;- c() r.sq.vect[1] &lt;- summary(lm.cand.01)$r.squared r.sq.vect[2] &lt;- summary(lm.cand.02)$r.squared # Specify more linear models and determine the associated r.squared: # ...r.sq.vect[3] &lt;- summary(lm.cand.03)$r.squared... now run some more # For each of the models, also determine the number of estimated parameters. num.pred.vect[1] &lt;- length(lm.cand.01$coef) num.pred.vect[2] &lt;- length(lm.cand.02$coef) # ...num.pred.vect[3] &lt;- length(lm.cand.03$coef)... now run some more # Finally, plot and understand the relationship of the r.squared value and the associated number of parameters: # plot(x = num.pred.vect, y = r.sq.vect) 22.19.2 AIC in R Lets use some built-in R functionality to evaluate AIC. Use the code below as a guide for you to evaluate the AIC value of each of the candidtate model you derived above. AIC.vect &lt;- c() AIC.vect[1] &lt;- AIC(lm.cand.01) AIC.vect[2] &lt;- AIC(lm.cand.02) # Determine values of AIC.vect for the lm.cand models you made above. # Calculate the AIC value of the candidate models you derived in #3 # and plot the AIC values as a function of the number of coefficient values: # plot(x = num.pred.vect, y = AIC.vect) Make a table: Column 1. the model formulation, “mpg ~ wt + qsec + am” (for example), Column 2. AIC value, Column 3. Delta AIC, Column 4. Relative model weight, Column 5. Provide the number of predictors used in the model Identify four candidate models that have at least on predictor in common and derive the weighted (using the weights from #4) predicted value of Y-hat with respect to the common variable. # Determine length of predicted output num.pred. &lt;- length(predict(lm.cand.01)) # Initialize a matrix to hold the values - you will fill this matrix with predictors predict.mat &lt;- matrix(NA, nrow = num.pred., ncol = 4) # Populate the model with predicted values predict.mat[,1] &lt;- predict(lm.cand.01) predict.mat[,2] &lt;- predict(lm.cand.02) #... predict.mat[,3] #... predict.mat[,4] # Use the model weights you derived in #4. Put them in a vector mod.wt &lt;- c(wt.cand.mod.01, wt.cand.mod.02, wt.cand.mod.03, wt.cand.mod.04) predict.mat[,1] &lt;- wt.cand.mod.01[1]*predict.mat[,1] pred.vect &lt;- rowSums(predict.mat) We will use the “step” function in base R, package “stats”. The R function step() can be used to perform variable selection. To perform selection we need to begin by specifying a starting model and the range of models which we want to examine in the search. How does this approach compare to the one above - PS this selection approach is a bit controversial… ?stats::step # Use the function to evaluate this model: step(lm(mpg ~ ., data = mtcars), direction = &quot;both&quot;, trace = T) 22.20 Multilevel Model 22.20.1 Instructor’s note Multilevel models, like many statistical methods, require programming. Becasue R is a tool that not everyone is interested in using, you will need to decide if this is an aspect of statistical analysis that will benefit you. If you are not interested in working on this module, then I suggest that you 1.) Refine your project ideas and objectives, 2.) review the chapter on regression in Zar and Fields et al., and 3.) Spend some time reviewing the statistical approaches that you would like to use in your own work. 22.20.2 The signature component of Multilevel Modeling Multilevel models have at their heart, group level indicators, \\(i\\). Alternative formulations with group level indicators: Varying intercept model: \\(y_{i} = \\alpha_{j[i]}+\\beta{x_i}+\\epsilon_i\\) Varying slope model: \\(y_{i} = \\alpha+\\beta_{j[i]}{x_i}+\\epsilon_i\\) Varying intercept and slope model: \\(y_{i} = \\alpha_{j[i]}+\\beta_{j[i]}{x_i}+\\epsilon_i\\) 1.) The first step in a multilevel model is to set up a regression model with varying coefficients. 2.) The second step in a multilevel model is to set up a regression model of the coefficents. 22.20.3 Group level predictor models: Multilevel models can be considered in two different ways: 1.) Generalization of lineqar regression whtere intercepts and/or slopes are allowed to vary by group. 2.) Equivalently, we can think of multilevel modeling as a regression that includes a categorical input variable representing group membership. So, the group index is a factor with \\(J\\) levels, corresponding to \\(J\\) predictors in the regression model (or 2\\(J\\)) in a varying intercept and slope model. In either case, \\(J-1\\) linear predictors are added to the model. The crucial step is that the \\(J\\) coefficients are also given a model. Multilevel models are a compromise between complete pooling and no pooling. The code below is uses the radon data from Gelman (2007) and performs the regression for the complete pooling model. srrs2 &lt;- read.delim (&quot;srrs2.dat&quot;, header=T, sep=&quot;,&quot;) mn &lt;- srrs2$state==&quot;MN&quot; radon &lt;- srrs2$activity[mn] log.radon &lt;- log(ifelse (radon==0, .1, radon)) y &lt;- log.radon x &lt;- srrs2$floor[mn] summary(lm(y~x)) The code below is uses the radon data from Gelman (2007) and performs the regression for the no pooling model. srrs2 &lt;- read.table (&quot;srrs2.dat&quot;, header=T, sep=&quot;,&quot;) mn &lt;- srrs2$state==&quot;MN&quot; radon &lt;- srrs2$activity[mn] log.radon &lt;- log(ifelse (radon==0, .1, radon)) county &lt;- srrs2$county[mn] y &lt;- log.radon x &lt;- srrs2$floor[mn] summary(lm(y~x + factor(county) - 1)) Describe the output of the summary functions for the complete and no pooling models. Both of the above models have problems. The complete pooling analysis ignores variation in average radon levels between counties. The goal of the anaysis is to identify counties with high radon homes… The no pooling model has problems as well, We are parsing the data so much that the inference made for some counties is based on very low sample sizes. Multilevel models are a compormise and considers partial pooling for unit-level predictors. The simplest multilevel model is: \\(y_{i} \\sim N( \\alpha_{j[i]}+\\beta x_i,\\sigma_y^2 )\\), for \\(i = 1, ..., n\\). "]]
